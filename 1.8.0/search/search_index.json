{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PlantSeg introduction","text":"<p>PlantSeg is a tool for 3D and 2D segmentation. The methods used are very generic and can be used for any instance segmentation workflow, but they are tuned towards cell segmentation in plant tissue. The tool is fundamentally composed of two main steps.</p> <p></p> <ul> <li> <p>Cell boundary predictions: A convolutional neural network (CNN) is utilized to perform voxel-wise boundary classification. This network is adept at filtering out diverse types and intensities of noise, homogenizing signal strength, and correcting imaging defects such as blurred or missing cell boundaries. This step ensures a high-quality boundary prediction which is crucial for accurate segmentation.</p> </li> <li> <p>Cell Segmentation as graph partitioning: The boundary predictions from the first step serve as the basis for automated segmentation. PlantSeg implements four distinct algorithms for this task, each with unique features tailored to different segmentation needs. This graph partitioning approach is particularly effective for segmenting densely packed cells.</p> </li> </ul> <p>For a detailed description of the methods employed in PlantSeg, please refer to our manuscript..</p> <p>If you find PlantSeg useful in your research, please consider citing our work:</p> <pre><code>@article{wolny2020accurate,\n  title={Accurate and versatile 3D segmentation of plant tissues at cellular resolution},\n  author={Wolny, Adrian and Cerrone, Lorenzo and Vijayan, Athul and Tofanelli, Rachele and Barro, Amaya Vilches and Louveaux, Marion and Wenzl, Christian and Strauss, S{\\\"o}ren and Wilson-S{\\'a}nchez, David and Lymbouridou, Rena and others},\n  journal={Elife},\n  volume={9},\n  pages={e57613},\n  year={2020},\n  publisher={eLife Sciences Publications Limited}\n}\n</code></pre>"},{"location":"chapters/getting_started/","title":"Quick Start","text":"<p>PlantSeg can be used in three different ways: interactively (using the Napari viewer), as a command line, or with a GUI. The following sections will guide you through the installation and usage of PlantSeg in each of these modes.</p>"},{"location":"chapters/getting_started/#interactive-plantseg-napari-viewer","title":"Interactive PlantSeg (Napari viewer)","text":"<p>PlantSeg app can be started from the terminal. First, activate the newly created conda environment with: <pre><code>mamba activate plant-seg\n</code></pre> then, start the plantseg in napari <pre><code>$ plantseg --napari\n</code></pre> A more in depth guide can be found in our documentation (GUI).</p>"},{"location":"chapters/getting_started/#pipeline-usage-command-line","title":"Pipeline Usage (command line)","text":"<p>PlantSeg can be configured using <code>YAML</code> config files.</p> <p>First, activate the newly created conda environment with: <pre><code>mamba activate plant-seg\n</code></pre> then, one can just start the pipeline with <pre><code>plantseg --config CONFIG_PATH\n</code></pre> where <code>CONFIG_PATH</code> is the path to the <code>YAML</code> configuration file. See config.yaml for a sample configuration file and our documentation (CLI) for a detailed description of the parameters.</p>"},{"location":"chapters/getting_started/#plantseg-gui","title":"PlantSeg (GUI)","text":"<p>PlantSeg app can also be started in a GUI mode, where basic user interface allows to configure and run the pipeline. First, activate the newly created conda environment with: <pre><code>mamba activate plant-seg\n</code></pre></p> <p>then, run the GUI by simply typing: <pre><code>$ plantseg --gui\n</code></pre> A more in depth guide can be found in our documentation (Classic GUI).</p>"},{"location":"chapters/getting_started/#using-liftedmulticut-segmentation","title":"Using LiftedMulticut segmentation","text":"<p>As reported in our paper, if one has a nuclei signal imaged together with the boundary signal, we could leverage the fact that one cell contains only one nucleus and use the <code>LiftedMultict</code> segmentation strategy and obtain improved segmentation. This workflow is now available in all PlantSeg interfaces.</p>"},{"location":"chapters/getting_started/contributing/","title":"Contribute to PlantSeg","text":"<p>PlantSeg is an open-source project and we welcome contributions from the community. There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into PlantSeg itself.</p>"},{"location":"chapters/getting_started/contributing/#getting-started","title":"Getting Started","text":"<p>To set up the development environment, run:</p> <pre><code>mamba env create -f environment-dev.yml\nconda activate plant-seg-dev\n</code></pre> <p>To install PlantSeg in development mode, run:</p> <pre><code>pip install -e . --no-deps\n</code></pre>"},{"location":"chapters/getting_started/contributing/#testing","title":"Testing","text":"<p>In order to run tests make sure that <code>pytest</code> is installed in your conda environment. You can run your tests simply with <code>python -m pytest</code> or <code>pytest</code>. For the latter to work you need to install <code>plantseg</code> locally in \"develop mode\" with <code>pip install -e .</code></p>"},{"location":"chapters/getting_started/installation/","title":"Installation","text":""},{"location":"chapters/getting_started/installation/#prerequisites-for-conda-package","title":"Prerequisites for Conda package","text":"<ul> <li>Linux, Windows, MacOS (not all features are available on MacOS)</li> <li>(Optional) Nvidia GPU with official Nvidia drivers installed for GPU acceleration</li> </ul>"},{"location":"chapters/getting_started/installation/#install-mamba","title":"Install Mamba","text":"<p>The easiest way to install PlantSeg is by using the conda (Anaconda) or  mamba (Miniforge) package manager. We recomend using <code>mamba</code> because it is faster and usually more consistent than <code>conda</code>.</p> LinuxWindows/MacOS <p>To download Miniforge open a terminal and type:</p> <pre><code>curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <p>Then install by typing:</p> <pre><code>bash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <p>and follow the installation instructions. Please refer to the Miniforge repo for more information, troubleshooting and uninstallation instructions. The miniforge installation file <code>Miniforge3-*.sh</code> can be deleted now. </p> <p>The first step required to use the pipeline is installing mamba. The installation can be done by downloading the installer from the Miniforge repo. There you can find the download links for the latest version of Miniforge, troubleshooting and uninstallation instructions.</p>"},{"location":"chapters/getting_started/installation/#install-plantseg-using-mamba","title":"Install PlantSeg using Mamba","text":"<p>PlantSeg can be installed directly by executing in the terminal (or PowerShell on Windows). For <code>conda</code> users, the command is identical, just replace <code>mamba</code> with <code>conda</code>.</p> <ul> <li> <p>GPU version, CUDA=12.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=12.1 pyqt plant-seg --no-channel-priority\n</code></pre> </li> <li> <p>GPU version, CUDA=11.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=11.8 pyqt plant-seg --no-channel-priority\n</code></pre> </li> <li> <p>CPU version</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch cpuonly pyqt plant-seg --no-channel-priority\n</code></pre> </li> </ul> <p>The above command will create new conda environment <code>plant-seg</code> together with all required dependencies.</p> <p>Please refer to the PyTorch website for more information on the available versions of PyTorch and the required CUDA version. The GPU version of Pytorch will also work on CPU only machines but has a much larger installation on disk.</p>"},{"location":"chapters/getting_started/installation/#optional-dependencies","title":"Optional dependencies","text":"<p>If you want to use the headless mode of PlantSeg, you need to install <code>dask[distributed]</code>:</p> <pre><code>conda activate plant-seg\nmamba install dask distributed\n</code></pre> <p>Some types of compressed tiff files require an additional package to be load correctly (e.g.: Zlib, ZSTD, LZMA, ...). To run PlantSeg on those stacks, you need to install <code>imagecodecs</code>. In the terminal:</p> <pre><code>conda activate plant-seg\npip install imagecodecs\n</code></pre> <p>Experimental support for SimpleITK watershed segmentation has been added to PlantSeg version 1.1.8. These features can be used only after installing the SimpleITK package:</p> <pre><code>conda activate plant-seg\npip install SimpleITK\n</code></pre>"},{"location":"chapters/getting_started/troubleshooting/","title":"Troubleshooting","text":"<p>This section provides solutions to common issues you might encounter while using PlantSeg. Click on a problem to jump to its specific solution.</p> <ul> <li>Font size problems in GUI</li> <li>Problems with <code>--headless</code> and <code>dask[distributed]</code></li> <li>Could not load library <code>libcudnn_ops_infer.so.8</code></li> <li>Missing configuration key errors</li> <li>Cannot import <code>lifted_problem_from_probabilities</code></li> <li>Other issues</li> </ul>"},{"location":"chapters/getting_started/troubleshooting/#font-size-problems-in-gui","title":"Font size problems in GUI","text":"<p>If you find the font size varies within either Napari or Legacy GUIs, or some buttons or texts are not visible, it might relate to your system's DPI settings or sreen resolution. To fix this, you can try to reset the resolution of your system.</p> <p>Related discussions:</p> <ul> <li><code>plantseg --gui</code>, no buttons to begin the workflow #241</li> </ul> <p>Other references:</p> <ul> <li>tkinter not recognizing screen resolution correctly</li> <li>High DPI Desktop Application Development on Windows</li> <li>SetProcessDpiAwareness function (shellscalingapi.h)</li> </ul>"},{"location":"chapters/getting_started/troubleshooting/#problems-with-headless-and-daskdistributed","title":"Problems with <code>--headless</code> and <code>dask[distributed]</code>","text":"<p>If you encounter the following error:</p> <pre><code>ImportError: dask.distributed is not installed.\n</code></pre> <p>Please install <code>dask[distributed]</code> to enable headless mode in PlantSeg. Run the following commands in your terminal:</p> <pre><code>mamba activate plant-seg\nmamba install -c pytorch -c nvidia -c conda-forge dask distributed\n</code></pre>"},{"location":"chapters/getting_started/troubleshooting/#could-not-load-library-libcudnn_ops_inferso8","title":"Could not load library <code>libcudnn_ops_infer.so.8</code>","text":"<p>If you encounter this error:</p> <pre><code>Could not load library libcudnn_ops_infer.so.8. Error: libcudnn_ops_infer.so.8: cannot open shared object file: No such file or directory\n</code></pre> <p>Resolve this by installing <code>cudnn</code> using the following command:</p> <pre><code>mamba install -c conda-forge cudnn\n</code></pre>"},{"location":"chapters/getting_started/troubleshooting/#missing-configuration-key-errors","title":"Missing configuration key errors","text":"<p>If you encounter a <code>RuntimeError</code> about a missing key, such as:</p> <pre><code>RuntimeError: key : 'crop_volume' is missing, plant-seg requires 'crop_volume' to run\n</code></pre> <p>This usually means the session configuration file is corrupted or outdated. To fix this:</p> <pre><code>rm ~/.plantseg_models/configs/config_gui_last.yaml\n</code></pre> <p>Ensure your configuration file is properly formatted and includes all required keys. Example configurations can be found in the <code>examples</code> directory of this repository.</p>"},{"location":"chapters/getting_started/troubleshooting/#cannot-import-lifted_problem_from_probabilities","title":"Cannot import <code>lifted_problem_from_probabilities</code>","text":"<p>If you receive an error related to importing from <code>elf.segmentation.features</code>, reinstall elf:</p> <pre><code>conda install -c conda-forge python-elf\n</code></pre>"},{"location":"chapters/getting_started/troubleshooting/#other-issues","title":"Other issues","text":"<p>PlantSeg is actively developed, and sometimes model or configuration files saved in <code>~/.plantseg_models</code> may become outdated. If you encounter errors related to configuration loading:</p> <ol> <li>Close the PlantSeg application.</li> <li>Delete the <code>~/.plantsep_models</code> directory.</li> <li>Restart the application and try again.</li> </ol> <p>These steps should help resolve any issues and enhance your experience with PlantSeg.</p>"},{"location":"chapters/plantseg_classic_cli/","title":"PlantSeg Classic CLI","text":""},{"location":"chapters/plantseg_classic_cli/#guide-to-custom-configuration-file","title":"Guide to Custom Configuration File","text":"<p>The configuration file defines all the operations in our pipeline together with the data to be processed. Please refer to config.yaml for a sample pipeline configuration and a detailed explanation of all parameters.</p>"},{"location":"chapters/plantseg_classic_cli/#main-keyssteps","title":"Main Keys/Steps","text":"<ul> <li><code>path</code> attribute: is used to define either the file to process or the directory containing the data.</li> <li><code>preprocessing</code> attribute: contains a simple set of possible operations one would need to run on their data before calling the neural network. This step can be skipped if data is ready for neural network processing. Detailed instructions can be found at Classic GUI (Data Processing).</li> <li><code>cnn_prediction</code> attribute: contains all parameters relevant for predicting with a neural network. Description of all pre-trained models provided with the package is described below. Detailed instructions can be found at Classic GUI (Predictions).</li> <li><code>segmentation</code> attribute: contains all parameters needed to run the partitioning algorithm (i.e., final Segmentation). Detailed instructions can be found at Classic GUI (Segmentation).</li> </ul>"},{"location":"chapters/plantseg_classic_cli/#additional-information","title":"Additional information","text":"<p>The PlantSeg-related files (models, configs) will be placed inside your home directory under <code>~/.plantseg_models</code>.</p> <p>Our pipeline uses the PyTorch library for CNN predictions. PlantSeg can be run on systems without GPU, however for maximum performance, we recommend that the application is run on a machine with a high-performance GPU for deep learning. If the <code>CUDA_VISIBLE_DEVICES</code> environment variable is not specified, the prediction task will be distributed on all available GPUs. E.g. run: <code>CUDA_VISIBLE_DEVICES=0 plantseg --config CONFIG_PATH</code> to restrict prediction to a given GPU.</p>"},{"location":"chapters/plantseg_classic_cli/#configuration-file-example","title":"configuration file example","text":"<p>This modality of using PlantSeg is particularly suited for high throughput processing and for running PlantSeg on a remote server. To use PlantSeg from command line mode, you will need to create a configuration file using a standard text editor  or using the save option of the PlantSeg GUI.</p> <p>Here is an example configuration:</p> <pre><code>path: /home/USERNAME/DATA.tiff # Contains the path to the directory or file to process\n\npreprocessing:\n  # enable/disable preprocessing\n  state: True\n  # create a new sub folder where all results will be stored\n  save_directory: \"PreProcessing\"\n  # rescaling the volume is essential for the generalization of the networks. The rescaling factor can be computed as the resolution\n  # of the volume at hand divided by the resolution of the dataset used in training. Be careful, if the difference is too large check for a different model.\n  factor: [1.0, 1.0, 1.0]\n  # the order of the spline interpolation\n  order: 2\n  # optional: perform Gaussian smoothing or median filtering on the input.\n  filter:\n    # enable/disable filtering\n    state: False\n    # Accepted values: 'gaussian'/'median'\n    type: gaussian\n    # sigma (gaussian) or disc radius (median)\n    param: 1.0\n\ncnn_prediction:\n  # enable/disable UNet prediction\n  state: True\n  # Trained model name, more info on available models and custom models in the README\n  model_name: \"generic_confocal_3D_unet\"\n  # If a CUDA capable gpu is available and corrected setup use \"cuda\", if not you can use \"cpu\" for cpu only inference (slower)\n  device: \"cpu\"\n  # how many subprocesses to use for data loading\n  num_workers: 8\n  # patch size given to the network (adapt to fit in your GPU mem)\n  patch: [32, 128, 128]\n  # stride between patches will be computed as `stride_ratio * patch`\n  # recommended values are in range `[0.5, 0.75]` to make sure the patches have enough overlap to get smooth prediction maps\n  stride_ratio: 0.75\n  # If \"True\" forces downloading networks from the online repos\n  model_update: False\n\ncnn_postprocessing:\n  # enable/disable cnn post processing\n  state: False\n  # if True convert to result to tiff\n  tiff: False\n  # rescaling factor\n  factor: [1, 1, 1]\n  # spline order for rescaling\n  order: 2\n\nsegmentation:\n  # enable/disable segmentation\n  state: True\n  # Name of the algorithm to use for inferences. Options: MultiCut, MutexWS, GASP, DtWatershed\n  name: \"MultiCut\"\n  # Segmentation specific parameters here\n  # balance under-/over-segmentation; 0 - aim for undersegmentation, 1 - aim for oversegmentation. (Not active for DtWatershed)\n  beta: 0.5\n  # directory where to save the results\n  save_directory: \"MultiCut\"\n  # enable/disable watershed\n  run_ws: True\n  # use 2D instead of 3D watershed\n  ws_2D: True\n  # probability maps threshold\n  ws_threshold: 0.5\n  # set the minimum superpixels size\n  ws_minsize: 50\n  # sigma for the gaussian smoothing of the distance transform\n  ws_sigma: 2.0\n  # sigma for the gaussian smoothing of boundary\n  ws_w_sigma: 0\n  # set the minimum segment size in the final segmentation. (Not active for DtWatershed)\n  post_minsize: 50\n\nsegmentation_postprocessing:\n  # enable/disable segmentation post processing\n  state: False\n  # if True convert to result to tiff\n  tiff: False\n  # rescaling factor\n  factor: [1, 1, 1]\n  # spline order for rescaling (keep 0 for segmentation post processing\n  order: 0\n</code></pre> <p>This configuration can be found at config.yaml.</p>"},{"location":"chapters/plantseg_classic_cli/#pipeline-usage-command-line","title":"Pipeline Usage (command line)","text":"<p>To start PlantSeg from the command line. First, activate the newly created conda environment with:</p> <pre><code>conda activate plant-seg\n</code></pre> <p>then, one can just start the pipeline with</p> <pre><code>plantseg --config CONFIG_PATH\n</code></pre> <p>where <code>CONFIG_PATH</code> is the path to a YAML configuration file.</p>"},{"location":"chapters/plantseg_classic_cli/#data-parallelism","title":"Data Parallelism","text":"<p>In the headless mode (i.e. when invoked with <code>plantseg --config CONFIG_PATH</code>) the prediction step will run on all the GPUs using DataParallel. If prediction on all available GPUs is not desirable, restrict the number of GPUs using <code>CUDA_VISIBLE_DEVICES</code>, e.g.</p> <pre><code>CUDA_VISIBLE_DEVICES=0,1 plantseg --config CONFIG_PATH\n</code></pre>"},{"location":"chapters/plantseg_classic_cli/#results","title":"Results","text":"<p>The results are stored together with the source input files inside a nested directory structure. As an example, if we want to run PlantSeg inside a directory with two stacks, we will obtain the following outputs:</p> <pre><code>/file1.tif\n/file2.tif\n/PreProcesing/\n------------&gt;/file1.h5\n------------&gt;/file1.yaml\n------------&gt;/file2.h5\n------------&gt;/file2.yaml\n------------&gt;/generic_confocal_3d_unet/\n-------------------------------------&gt;/file1_predictions.h5\n-------------------------------------&gt;/file1_predictions.yaml\n-------------------------------------&gt;/file2_predictions.h5\n-------------------------------------&gt;/file2_predictions.yaml\n-------------------------------------&gt;/GASP/\n------------------------------------------&gt;/file_1_predions_gasp_average.h5\n------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n------------------------------------------&gt;/file_2_predions_gasp_average.h5\n------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n------------------------------------------&gt;/PostProcessing/\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n</code></pre> <p>The use of this hierarchical directory structure allows PlantSeg to find the necessary files quickly and can be used to test different segmentation algorithms/parameter combinations minimizing the memory overhead on the disk. For the sake of reproducibility, every file is associated with a configuration file \".yaml\" that saves all parameters used to produce the result.</p>"},{"location":"chapters/plantseg_classic_cli/#liftedmulticut-segmentation","title":"LiftedMulticut segmentation","text":"<p>As reported in our paper, if one has a nuclei signal imaged together with the boundary signal, we could leverage the fact that one cell contains only one nucleus and use the <code>LiftedMultict</code> segmentation strategy and obtain improved segmentation. We will use the Arabidopsis thaliana lateral root as an example. The <code>LiftedMulticut</code> strategy consists of running PlantSeg two times:</p> <ol> <li> <p>Using PlantSeg to predict the nuclei probability maps using the <code>lightsheet_unet_bce_dice_nuclei_ds1x</code> network. In this case, only the pre-processing and CNN prediction steps are enabled in the config. See example config.</p> <pre><code>plantseg --config nuclei_predictions_example.yaml\n</code></pre> </li> <li> <p>Using PlantSeg to segment the input image with the <code>LiftedMulticut</code> algorithm given the nuclei probability maps from the 1st step. See example config. The notable difference is that in the <code>segmentation</code> part of the config, we set <code>name: LiftedMulticut</code> and the <code>nuclei_predictions_path</code> as the path to the directory where the nuclei pmaps were saved in step 1. Also, make sure that the <code>path</code> attribute points to the raw files containing the cell boundary staining (NOT THE NUCLEI).</p> <pre><code>plantseg --config lifted_multicut_example.yaml\n</code></pre> </li> </ol> <p>If case when the nuclei segmentation is given, one should skip step 1., add <code>is_segmentation=True</code> flag in the config and directly run step 2.</p>"},{"location":"chapters/plantseg_classic_gui/","title":"PlantSeg from GUI","text":"<p>The graphical user interface is the easiest way to configure and run PlantSeg. Currently the GUI does not allow to visualize or interact with the data. We recommend using MorphographX or Fiji in order to assert the success and quality of the pipeline results.</p> <p></p>"},{"location":"chapters/plantseg_classic_gui/#file-browser-widget","title":"File Browser Widget","text":"<p>The file browser can be used to select the input files for the pipeline. PlantSeg can run on a single file (button A) or in batch mode for all files inside a directory (button B). If a directory is selected PlantSeg will run on all compatible files inside the directory.</p>"},{"location":"chapters/plantseg_classic_gui/#main-pipeline-configurator","title":"Main Pipeline Configurator","text":"<p>The central panel of PlantSeg (C) is the core of the pipeline configuration. It can be used for customizing and tuning the pipeline accordingly to the data at hand. Detailed information for each stage can be found at: * Data-Processing * CNN-Predictions * Segmentation</p> <p>Any of the above widgets can be run singularly or in sequence (left to right). The order of execution can not be modified.</p>"},{"location":"chapters/plantseg_classic_gui/#run","title":"Run","text":"<p>The last panel has two main functions. Running the pipeline (D), once the run button is pressed the pipeline starts. The button is inactive until the process is finished. Adding a custom model (E). Custom trained model can be done by using the dedicated popup. Training a new model can be done following the instruction at pytorch-3dunet.</p>"},{"location":"chapters/plantseg_classic_gui/#results","title":"Results","text":"<p>The results are stored together with the source input files inside a nested directory structure. As example, if we want to run PlantSeg inside a directory with 2 stacks, we will obtain the following outputs: <pre><code>/file1.tif\n/file2.tif\n/PreProcesing/\n------------&gt;/file1.h5\n------------&gt;/file1.yaml\n------------&gt;/file2.h5\n------------&gt;/file2.yaml\n------------&gt;/generic_confocal_3d_unet/\n-------------------------------------&gt;/file1_predictions.h5\n-------------------------------------&gt;/file1_predictions.yaml\n-------------------------------------&gt;/file2_predictions.h5\n-------------------------------------&gt;/file2_predictions.yaml\n-------------------------------------&gt;/GASP/\n------------------------------------------&gt;/file_1_predions_gasp_average.h5\n------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n------------------------------------------&gt;/file_2_predions_gasp_average.h5\n------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n------------------------------------------&gt;/PostProcessing/\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n</code></pre> The use of this hierarchical directory structure allows PlantSeg to easily find the necessary files and can be used to test different combination of segmentation algorithms/parameters minimizing the memory overhead on the disk. For sake of reproducibility, every file is associated with a configuration file \".yaml\" that saves all parameters used to produce the result.</p>"},{"location":"chapters/plantseg_classic_gui/#start-plantseg-gui","title":"Start PlantSeg GUI","text":"<p>In order to start the PlantSeg app in GUI mode: First, activate the newly created conda environment with: <pre><code>conda activate plant-seg\n</code></pre></p> <p>then, run the GUI by simply typing: <pre><code>$ plantseg --gui\n</code></pre></p>"},{"location":"chapters/plantseg_classic_gui/cnn_predictions/","title":"CNN Predictions","text":"<p>The CNN predictions widget process the stacks at hand with a Convolutional Neural Network. The output is a boundary classification image, where every voxel gets a value between 0 (not a cell boundary) and 1 (cell boundary).</p> <p>The input image can be a raw stack \"tiff\"/\"h5\" or the output of the PreProcessing widget.</p> <ul> <li> <p>The Model Name menu shows all available models. There are two main basic models available</p> <ol> <li> <p>Generic confocalis a generic model for all confocal datasets. Some examples: </p> </li> <li> <p>Generic lightsheet this is a generic model for all lightsheet datasets.  Some examples:  </p> </li> </ol> </li> <li> <p>Due to memory constraints, usually a complete stack does not fit the GPUs memory,  therefore the Patch size can be used to optimize the performance of the pipeline.  Usually, larger patches cost more memory but can slightly improve performance.  For 2D segmentation, the Patch size relative to the z-axis has to be set to 1.</p> </li> <li> <p>To minimize the boundary effect due to the sliding windows patching, we can use different stride:</p> <ol> <li>Accurate: corresponding to a stride 50% of the patch size (yield best predictions/segmentation accuracy)</li> <li>Balanced: corresponding to a stride 75% of the patch size</li> <li>Draft: corresponding to a stride 95% of the patch size (yield fastest runtime)</li> </ol> </li> <li> <p>The Device type menu can be used to enable or not GPU acceleration. CUDA greatly accelerates the network predictions on Nvidia GPUs. At the moment, we don't support other GPUs manufacturers.</p> </li> </ul>"},{"location":"chapters/plantseg_classic_gui/data_processing/","title":"Classic Data Processing","text":"<p> PlantSeg includes essential utilities for data pre-processing and post-processing.</p>"},{"location":"chapters/plantseg_classic_gui/data_processing/#pre-processing","title":"Pre-Processing","text":"<p>The input for this widget can be either a \"raw\" image or a \"prediction\" image. Input formats allowed are tiff and h5, while output is always h5.</p> <ul> <li> <p>Save Directory can be used to define the output directory.</p> </li> <li> <p>The most critical setting is the Rescaling. It is important to rescale the image to  match the resolution of the data used for training the Neural Network. This operation can be done automatically by clicking on the GUI on Guided. Be careful to use this function only in case of data considerably different from the reference resolution. <pre><code>As an example:\n  - if your data has the voxel size of 0.3 x 0.1 x 0.1 (ZYX).\n  - and the networks was trained on 0.3 x 0.2 x 0.2 data (reference resolution).\n\nThe required voxel size can be obtained by computing the ratio between your data and the\nreference train dataset. In the example the rescaling factor = 1 x 2 x 2.\n</code></pre></p> </li> <li> <p>The Interpolation field controls the interpolation type (0 for nearest neighbors, 1 for linear spline, 2 for quadratic).</p> </li> <li> <p>The last field defines a Filter operation. Implemented there are:</p> <ol> <li>Gaussian Filtering: The parameter is a float and defines the sigma value for the gaussian smoothing. The higher, the wider is filtering kernel.</li> <li>Median Filtering: Apply median operation convolutionally over the image.  The kernel is a sphere of size defined in the parameter field.</li> </ol> </li> </ul>"},{"location":"chapters/plantseg_classic_gui/data_processing/#post-processing","title":"Post-Processing","text":"<p>A post-processing step can be performed after the CNN-Predictions and the Segmentation. The post-processing options are:  * Converting the output to the tiff file format (default is h5).</p> <ul> <li>Casting the CNN-Predictions output to data_uint8 drastically reduces the memory footprint of the output  file.</li> </ul> <p>Additionally, the post-processing will scale back your outputs to the original voxels resolutions.</p>"},{"location":"chapters/plantseg_classic_gui/segmentation/","title":"Segmentation","text":"<p>The segmentation widget allows using very powerful graph partitioning techniques to obtain a segmentation from the input stacks. The input of this widget should be the output of the CNN-predictions widget. If the boundary prediction stage fails for any reason, a raw image could be used (especially if the cell boundaries are  very sharp, and the noise is low) but usually does not yield satisfactory results.</p> <ul> <li> <p>The Algorithm menu can be used to choose the segmentation algorithm. Available choices are:</p> <ol> <li>GASP (average): is a generalization of the classical hierarchical clustering. It usually delivers very reliable and accurate segmentation. It is the default in PlantSeg.</li> <li>MutexWS: Mutex Watershed is a derivative of the standard Watershed, where we do not need seeds for the  segmentation. This algorithm performs very well in certain types of complex morphology (like )</li> <li>MultiCut: in contrast to the other algorithms is not based on a greedy agglomeration but tries to find the optimal global segmentation. This is, in practice, very hard, and it can be infeasible for huge stacks.</li> <li>DtWatershed: is our implementation of the distance transform Watershed. From the input, we extract a distance map from the boundaries. Based this distance map, seeds are placed at local minima. Then those seeds are used for computing the Watershed segmentation. To speed up the computation of GASP, MutexWS, and MultiCut, an over-segmentation  is obtained using Dt Watershed.</li> </ol> </li> <li> <p>Save Directory defines the sub-directory's name where the segmentation results will be stored.</p> </li> <li> <p>The Under/Over- segmentation factor is the most critical parameters for tuning the segmentation of GASP, MutexWS and MultiCut. A small value will steer the segmentation towards under-segmentation. While a high-value bias the segmentation towards the over-segmentation. This parameter does not affect the distance transform Watershed.</p> </li> <li> <p>If Run Watershed in 2D value is True, the superpixels are created in 2D (over the z slice). While if False makes the superpixels in the whole 3D volume. 3D superpixels are much slower and memory intensive but can improve  the segmentation accuracy.</p> </li> <li> <p>The CNN Predictions Threshold is used for the superpixels extraction and Distance Transform Watershed. It has a crucial role for the watershed seeds extraction and can be used similarly to the \"Unde/Over segmentation factor.\" to bias the final result. A high value translates to less seeds being placed (more under segmentation), while with a low value, more seeds are  placed (more over-segmentation).</p> </li> <li> <p>The input is used by the distance transform Watershed to extract the seed and find the segmentation boundaries. If Watershed Seeds Sigma and Watershed Boundary Sigma are larger than  zero, a gaussian smoothing is applied on the input before the operations above. This is mainly helpful for  the seeds computation but, in most cases, does not impact segmentation quality.</p> </li> <li> <p>The Superpixels Minimum Size applies a size filter to the initial superpixels over-segmentation. This removes Watershed often produces small segments and is usually helpful for the subsequent agglomeration.  Segments smaller than the threshold will be merged with the nearest neighbor segment.</p> </li> <li> <p>Even though GASP, MutexWS, and MultiCut are not very prone to produce small segments, the Cell Minimum Size can be used as a final size processing filter. Segments smaller than the threshold will be merged with the nearest neighbor cell.</p> </li> </ul>"},{"location":"chapters/plantseg_interactive_napari/","title":"PlantSeg Interactive - Napari","text":"<p>Documentation in Progress</p> <p>This page is under development.</p> <p>PlantSeg app can also be started using napari as a viewer. First, activate the newly created conda environment with:</p> <pre><code>conda activate plant-seg\n</code></pre> <p>then, start the plantseg in napari</p> <pre><code>plantseg --napari\n</code></pre>"},{"location":"chapters/plantseg_interactive_napari/data_processing/","title":"Data Processing","text":"<p>This section describes the data processing functionalities available in PlantSeg Interactive. This functionality are available in the <code>Data-Processing</code> tab in the PlantSeg Interactive GUI.</p>"},{"location":"chapters/plantseg_interactive_napari/data_processing/#widget-gaussian-smoothing","title":"Widget: Gaussian Smoothing","text":"<p> <p></p> <p></p>                      Apply Gaussian smoothing to an image layer.             <ul><li>Image: Image layer to apply the smoothing.</li> <li>Sigma: Define the size of the gaussian smoothing kernel. The larger the more blurred will be the output image.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/data_processing/#widget-image-rescaling","title":"Widget: Image Rescaling","text":"From factorTo layer voxel sizeTo layer shapeTo model voxel sizeTo voxel sizeTo shapeSet voxel size <p>Using the <code>From factor</code> mode, the user can rescale the image by a multiplicate factor. For example, if the image has a shape <code>(10, 10, 10)</code> and the user wants to rescale it by a factor of <code>(2, 2, 2)</code>, the new size will be <code>(20, 20, 20)</code>.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Rescaling factor: Define the scaling factor to use for resizing the input image.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To layer voxel size</code> mode, the user can rescale the image to the voxel size of a specific layer. For example, if two images are loaded in the viewer, one with a voxel size of <code>(0.1, 0.1, 0.1)um</code> and the other with a voxel size of <code>(0.1, 0.05, 0.05)um</code>, the user can rescale the first image to the voxel size of the second image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Reference layer: Rescale to same voxel size as selected layer.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To layer shape</code> mode, the user can rescale the image to the shape of a specific layer. For example, if two images are loaded in the viewer, one with a shape <code>(10, 10, 10)</code> and the other with a shape <code>(20, 20, 20)</code>, the user can rescale the first image to the shape of the second image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Reference layer: Rescale to same voxel size as selected layer.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To model voxel size</code> mode, the user can rescale the image to the voxel size of the model. For example, if the model has been trained with data at voxel size of <code>(0.1, 0.1, 0.1)um</code>, the user can rescale the image to this voxel size.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Reference model: Rescale to same voxel size as selected model.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To voxel size</code> mode, the user can rescale the image to a specific voxel size.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Out voxel size: Define the output voxel size. Units are same as imported, (if units are missing default is \"um\").</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To shape</code> mode, the user can rescale the image to a specific shape.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Out shape: Rescale to a manually selected shape.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>Set voxel size</code> mode, the user can set the voxel size of the image to a specific value. This only changes the metadata of the image and does not rescale the image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Out voxel size: Define the output voxel size. Units are same as imported, (if units are missing default is \"um\").</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/plantseg_interactive_napari/extra/","title":"Extra Seg","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/plantseg_interactive_napari/headless_batch_processing/","title":"Headless Batch Processing","text":"<p>When image(s) are exported from PlantSeg Napari, a workflow file, e.g. <code>workflow.pkl</code>, describing the processing steps is saved alongside the exported images. This workflow can be used to process more images with the same parameters in each step of the workflow, in the headless mode.</p> <p>To run the headless mode, use:</p> <pre><code>plantseg --headless PATH_TO_WORKFLOW\n</code></pre> <p>Network Prediction with 2-channel Output</p> <p>If the network used in the workflow has a 2-channel output, no other steps can be performed after the network prediction step. The only supported PlantSeg Napari workflow for headless 2-channel-output prediction is open file -&gt; network prediction -&gt; save file.</p>"},{"location":"chapters/plantseg_interactive_napari/import_export/","title":"Images Import and Export","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/plantseg_interactive_napari/proofreading/","title":"Proofreading","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/plantseg_interactive_napari/unet_gasp_workflow/","title":"Main PlantSeg Workflow","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/plantseg_interactive_napari/unet_training/","title":"UNet Training","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/plantseg_models/","title":"Official Data and Models","text":""},{"location":"chapters/plantseg_models/#datasets","title":"Datasets","text":"<p>We publicly release the datasets used for training the networks which available as part of the PlantSeg package. Please refer to our publication for more details about the datasets:</p> <ul> <li>Arabidopsis thaliana ovules dataset (raw confocal images + ground truth labels)</li> <li>Arabidopsis thaliana lateral root (raw light sheet images + ground truth labels)</li> </ul> <p>Both datasets can be downloaded from our OSF project</p>"},{"location":"chapters/plantseg_models/#pre-trained-networks","title":"Pre-trained Networks","text":"<p>The following pre-trained networks are provided with PlantSeg package out-of-the box and can be specified in the config file or chosen in the GUI.</p> <ul> <li><code>generic_confocal_3D_unet</code> - alias for <code>confocal_3D_unet_ovules_ds2x</code> see below</li> <li><code>generic_light_sheet_3D_unet</code> - alias for <code>lightsheet_3D_unet_root_ds1x</code> see below</li> <li><code>confocal_3D_unet_ovules_ds1x</code> - a variant of 3D U-Net trained on confocal images of Arabidopsis ovules on original resolution, voxel size: (0.235x0.075x0.075 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>confocal_3D_unet_ovules_ds2x</code> - a variant of 3D U-Net trained on confocal images of Arabidopsis ovules on 1/2 resolution, voxel size: (0.235x0.150x0.150 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>confocal_3D_unet_ovules_ds3x</code> - a variant of 3D U-Net trained on confocal images of Arabidopsis ovules on 1/3 resolution, voxel size: (0.235x0.225x0.225 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>confocal_2D_unet_ovules_ds2x</code> - a variant of 2D U-Net trained on confocal images of Arabidopsis ovules. Training the 2D U-Net is done on the Z-slices (1/2 resolution, pixel size: 0.150x0.150 \u00b5m^3) with BCEDiceLoss</li> <li><code>confocal_3D_unet_ovules_nuclei_ds1x</code> - a variant of 3D U-Net trained on confocal images of Arabidopsis ovules nuclei stain on original resolution, voxel size: (0.35x0.1x0.1 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_3D_unet_root_ds1x</code> - a variant of 3D U-Net trained on light-sheet images of Arabidopsis lateral root on original resolution, voxel size: (0.25x0.1625x0.1625 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_3D_unet_root_ds2x</code> - a variant of 3D U-Net trained on light-sheet images of Arabidopsis lateral root on 1/2 resolution, voxel size: (0.25x0.325x0.325 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_3D_unet_root_ds3x</code> - a variant of 3D U-Net trained on light-sheet images of Arabidopsis lateral root on 1/3 resolution, voxel size: (0.25x0.4875x0.4875 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_2D_unet_root_ds1x</code> - a variant of 2D U-Net trained on light-sheet images of Arabidopsis lateral root. Training the 2D U-Net is done on the Z-slices (pixel size: 0.1625x0.1625 \u00b5m^3) with BCEDiceLoss</li> <li><code>lightsheet_3D_unet_root_nuclei_ds1x</code> - a variant of 3D U-Net trained on light-sheet images Arabidopsis lateral root nuclei on original resolution, voxel size: (0.25x0.1625x0.1625 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_2D_unet_root_nuclei_ds1x</code> - a variant of 2D U-Net trained on light-sheet images Arabidopsis lateral root nuclei. Training the 2D U-Net is done on the Z-slices (pixel size: 0.1625x0.1625 \u00b5m^3) with BCEDiceLoss.</li> <li><code>confocal_3D_unet_sa_meristem_cells</code> - a variant of 3D U-Net trained on confocal images of shoot apical meristem dataset from: Jonsson, H., Willis, L., &amp; Refahi, Y. (2017). Research data supporting Cell size and growth regulation in the Arabidopsis thaliana apical stem cell niche. https://doi.org/10.17863/CAM.7793. voxel size: (0.25x0.25x0.25 \u00b5m^3) (ZYX)</li> <li><code>confocal_2D_unet_sa_meristem_cells</code> - a variant of 2D U-Net trained on confocal images of shoot apical meristem dataset from: Jonsson, H., Willis, L., &amp; Refahi, Y. (2017). Research data supporting Cell size and growth regulation in the Arabidopsis thaliana apical stem cell niche. https://doi.org/10.17863/CAM.7793.  pixel size: (25x0.25 \u00b5m^3) (YX)</li> <li><code>lightsheet_3D_unet_mouse_embryo_cells</code> - A variant of 3D U-Net trained to predict the cell boundaries in live light-sheet images of ex-vivo developing mouse embryo. Voxel size: (0.2\u00d70.2\u00d71 \u00b5m^3) (XYZ)</li> <li><code>confocal_3D_unet_mouse_embryo_nuclei</code> - A variant of 3D U-Net trained to predict the cell boundaries in live light-sheet images of ex-vivo developing mouse embryo. Voxel size: (0.2\u00d70.2\u00d71 \u00b5m^3) (XYZ)</li> </ul> <p>Selecting a given network name (either in the config file or GUI) will download the network into the <code>~/.plantseg_models</code> directory. Detailed description of network training can be found in our paper.</p> <p>The PlantSeg home directory can be configured with the <code>PLANTSEG_HOME</code> environment variable.</p> <pre><code>export PLANTSEG_HOME=\"/path/to/plantseg/home\"\n</code></pre>"},{"location":"chapters/plantseg_models/training/","title":"Training on New Data","text":"<p>For training new models we rely on the pytorch-3dunet. A similar configuration file can be used for training on new data and all the instructions can be found in the repo.</p>"},{"location":"chapters/plantseg_models/training/#adding-models","title":"Adding Models","text":"<ol> <li>Put these three files in one directory:<ol> <li>configuration file used for training: <code>config_train.yml</code></li> <li>snapshot of the best model across training: <code>best_checkpoint.pytorch</code></li> <li>snapshot of the last model saved during training: <code>last_checkpoint.pytorch</code></li> </ol> </li> <li>Click \"Add Custom Model\" in the GUI and follow the instruction</li> </ol>"},{"location":"chapters/plantseg_models/training/#alternative-old-method","title":"Alternative Old Method","text":"<p>When the network is trained, it is enough to create <code>~/.plantseg_models/MY_MODEL_NAME</code> directory and copy the following files into it:</p> <ul> <li>configuration file used for training: <code>config_train.yml</code></li> <li>snapshot of the best model across training: <code>best_checkpoint.pytorch</code></li> <li>snapshot of the last model saved during training: <code>last_checkpoint.pytorch</code></li> </ul> <p>The later two files are automatically generated during training and contain all neural networks parameters.</p> <p>Now you can simply use your model for prediction by setting the model_name key to <code>MY_MODEL_NAME</code>.</p> <p>If you want your model to be part of the open-source model zoo provided with this package, please contact us.</p>"},{"location":"chapters/python_api/","title":"PlantSeg Python API","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/python_api/cnn_predictions/","title":"PlantSeg CNN Predictions","text":""},{"location":"chapters/python_api/cnn_predictions/#plantseg.predictions.functional.predictions.unet_predictions","title":"<code>plantseg.predictions.functional.predictions.unet_predictions(raw: np.ndarray, model_name: Optional[str], model_id: Optional[str], patch: Tuple[int, int, int] = (80, 160, 160), single_batch_mode: bool = True, device: str = 'cuda', model_update: bool = False, disable_tqdm: bool = False, handle_multichannel: bool = False, config_path: Optional[Path] = None, model_weights_path: Optional[Path] = None, **kwargs) -&gt; np.ndarray</code>","text":"<p>Generate predictions from raw data using a specified 3D U-Net model.</p> <p>This function handles both single and multi-channel outputs from the model, returning appropriately shaped arrays based on the output channel configuration.</p> <p>Parameters:</p> <ul> <li> <code>raw</code>               (<code>ndarray</code>)           \u2013            <p>Raw input data as a 3D array of shape (Z, Y, X).</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model to use.</p> </li> <li> <code>patch</code>               (<code>Tuple[int, int, int]</code>, default:                   <code>(80, 160, 160)</code> )           \u2013            <p>Patch size for prediction. Defaults to (80, 160, 160).</p> </li> <li> <code>single_batch_mode</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use a single batch for prediction. Defaults to True.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda'</code> )           \u2013            <p>The computation device ('cpu', 'cuda', etc.). Defaults to 'cuda'.</p> </li> <li> <code>model_update</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to update the model to the latest version. Defaults to False.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, disables the tqdm progress bar. Defaults to False.</p> </li> <li> <code>handle_multichannel</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, handles multi-channel output properly. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>pmap</code> (              <code>ndarray</code> )          \u2013            <p>The predicted boundaries as a 3D (Z, Y, X) or 4D (C, Z, Y, X) array, normalized between 0 and 1.</p> </li> </ul> Source code in <code>plantseg/predictions/functional/predictions.py</code> <pre><code>def unet_predictions(\n    raw: np.ndarray,\n    model_name: Optional[str],\n    model_id: Optional[str],\n    patch: Tuple[int, int, int] = (80, 160, 160),\n    single_batch_mode: bool = True,\n    device: str = 'cuda',\n    model_update: bool = False,\n    disable_tqdm: bool = False,\n    handle_multichannel: bool = False,\n    config_path: Optional[Path] = None,\n    model_weights_path: Optional[Path] = None,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Generate predictions from raw data using a specified 3D U-Net model.\n\n    This function handles both single and multi-channel outputs from the model,\n    returning appropriately shaped arrays based on the output channel configuration.\n\n    Args:\n        raw (np.ndarray): Raw input data as a 3D array of shape (Z, Y, X).\n        model_name (str): The name of the model to use.\n        patch (Tuple[int, int, int], optional): Patch size for prediction. Defaults to (80, 160, 160).\n        single_batch_mode (bool, optional): Whether to use a single batch for prediction. Defaults to True.\n        device (str, optional): The computation device ('cpu', 'cuda', etc.). Defaults to 'cuda'.\n        model_update (bool, optional): Whether to update the model to the latest version. Defaults to False.\n        disable_tqdm (bool, optional): If True, disables the tqdm progress bar. Defaults to False.\n        handle_multichannel (bool, optional): If True, handles multi-channel output properly. Defaults to False.\n\n    Returns:\n        pmap (np.ndarray): The predicted boundaries as a 3D (Z, Y, X) or 4D (C, Z, Y, X) array, normalized between 0 and 1.\n    \"\"\"\n    if config_path is not None:  # Safari mode for custom models outside zoos\n        gui_logger.info('Safari prediction: Running model from custom config path.')\n        model, model_config, model_path = model_zoo.get_model_by_config_path(config_path, model_weights_path)\n    elif model_id is not None:  # BioImage.IO zoo mode\n        gui_logger.info('BioImage.IO prediction: Running model from BioImage.IO model zoo.')\n        model, model_config, model_path = model_zoo.get_model_by_id(model_id)\n    elif model_name is not None:  # PlantSeg zoo mode\n        gui_logger.info('Zoo prediction: Running model from PlantSeg official zoo.')\n        model, model_config, model_path = model_zoo.get_model_by_name(model_name, model_update=model_update)\n    else:\n        raise ValueError('Either `model_name` or `model_id` or `model_path` must be provided.')\n    state = torch.load(model_path, map_location='cpu')\n\n    if 'model_state_dict' in state:  # Model weights format may vary between versions\n        state = state['model_state_dict']\n    model.load_state_dict(state)\n\n    patch_halo = kwargs['patch_halo'] if 'patch_halo' in kwargs else get_patch_halo(model_name)  # lazy else statement\n\n    predictor = ArrayPredictor(\n        model=model,\n        in_channels=model_config['in_channels'],\n        out_channels=model_config['out_channels'],\n        device=device,\n        patch=patch,\n        patch_halo=patch_halo,\n        single_batch_mode=single_batch_mode,\n        headless=False,\n        verbose_logging=False,\n        disable_tqdm=disable_tqdm,\n    )\n\n    if int(model_config['in_channels']) &gt; 1:  # if multi-channel input\n        raw = fix_input_shape_to_CZYX(raw)\n        multichannel_input = True\n    else:\n        raw = fix_input_shape_to_ZYX(raw)\n        multichannel_input = False\n    raw = raw.astype('float32')\n    augs = get_test_augmentations(raw)  # using full raw to compute global normalization mean and std\n    stride = get_stride_shape(patch)\n    slice_builder = SliceBuilder(raw, label_dataset=None, patch_shape=patch, stride_shape=stride)\n    test_dataset = ArrayDataset(raw, slice_builder, augs, halo_shape=patch_halo, multichannel=multichannel_input, verbose_logging=False)\n\n    pmaps = predictor(test_dataset)  # pmaps either (C, Z, Y, X) or (C, Y, X)\n\n    if int(model_config['out_channels']) &gt; 1 and handle_multichannel:  # if multi-channel output and who called this function can handle it\n        napari_formatted_logging(\n            f'`unet_predictions()` has `handle_multichannel`={handle_multichannel}',\n            thread=\"unet_predictions\",\n            level='warning',\n        )\n        pmaps = fix_input_shape_to_CZYX(pmaps)  # make (C, Y, X) to (C, 1, Y, X) and keep (C, Z, Y, X) unchanged\n    else:  # otherwise use old mechanism\n        pmaps = fix_input_shape_to_ZYX(pmaps[0])\n\n    return pmaps\n</code></pre>"},{"location":"chapters/python_api/data_processing/","title":"Data Processing","text":"<p>Basic data processing functions are provided in the <code>data_processing</code> module. These functions are used to preprocess data before training a model, or to post-process the output of a model.</p>"},{"location":"chapters/python_api/data_processing/#generic-functions","title":"Generic Functions","text":""},{"location":"chapters/python_api/data_processing/#plantseg.dataprocessing.functional.dataprocessing.normalize_01","title":"<code>plantseg.dataprocessing.functional.dataprocessing.normalize_01(data: np.ndarray) -&gt; np.ndarray</code>","text":"<p>Normalize a numpy array between 0 and 1 and converts it to float32.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>Input numpy array</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>normalized_data</code> (              <code>ndarray</code> )          \u2013            <p>Normalized numpy array</p> </li> </ul> Source code in <code>plantseg/dataprocessing/functional/dataprocessing.py</code> <pre><code>def normalize_01(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Normalize a numpy array between 0 and 1 and converts it to float32.\n\n    Args:\n        data (np.ndarray): Input numpy array\n\n    Returns:\n        normalized_data (np.ndarray): Normalized numpy array\n    \"\"\"\n    return (data - np.min(data)) / (np.max(data) - np.min(data) + 1e-12).astype('float32')\n</code></pre>"},{"location":"chapters/python_api/data_processing/#plantseg.dataprocessing.functional.dataprocessing.scale_image_to_voxelsize","title":"<code>plantseg.dataprocessing.functional.dataprocessing.scale_image_to_voxelsize(image: np.ndarray, input_voxel_size: tuple[float, float, float], output_voxel_size: tuple[float, float, float], order: int = 0) -&gt; np.ndarray</code>","text":"<p>Scale an image from a given voxel size to another voxel size.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to scale</p> </li> <li> <code>input_voxel_size</code>               (<code>tuple[float, float, float]</code>)           \u2013            <p>Input voxel size</p> </li> <li> <code>output_voxel_size</code>               (<code>tuple[float, float, float]</code>)           \u2013            <p>Output voxel size</p> </li> <li> <code>order</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Interpolation order, must be 0 for segmentation and 1, 2 for images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>scaled_image</code> (              <code>ndarray</code> )          \u2013            <p>Scaled image as numpy array</p> </li> </ul> Source code in <code>plantseg/dataprocessing/functional/dataprocessing.py</code> <pre><code>def scale_image_to_voxelsize(\n    image: np.ndarray,\n    input_voxel_size: tuple[float, float, float],\n    output_voxel_size: tuple[float, float, float],\n    order: int = 0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Scale an image from a given voxel size to another voxel size.\n\n    Args:\n        image (np.ndarray): Input image to scale\n        input_voxel_size (tuple[float, float, float]): Input voxel size\n        output_voxel_size (tuple[float, float, float]): Output voxel size\n        order (int): Interpolation order, must be 0 for segmentation and 1, 2 for images\n\n    Returns:\n        scaled_image (np.ndarray): Scaled image as numpy array\n    \"\"\"\n    factor = compute_scaling_factor(input_voxel_size, output_voxel_size)\n    return image_rescale(image, factor, order=order)\n</code></pre>"},{"location":"chapters/python_api/data_processing/#plantseg.dataprocessing.functional.dataprocessing.image_rescale","title":"<code>plantseg.dataprocessing.functional.dataprocessing.image_rescale(image: np.ndarray, factor: tuple[float, float, float], order: int) -&gt; np.ndarray</code>","text":"<p>Scale an image by a given factor in each dimension</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to scale</p> </li> <li> <code>factor</code>               (<code>tuple[float, float, float]</code>)           \u2013            <p>Scaling factor in each dimension</p> </li> <li> <code>order</code>               (<code>int</code>)           \u2013            <p>Interpolation order, must be 0 for segmentation and 1, 2 for images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>scaled_image</code> (              <code>ndarray</code> )          \u2013            <p>Scaled image as numpy array</p> </li> </ul> Source code in <code>plantseg/dataprocessing/functional/dataprocessing.py</code> <pre><code>def image_rescale(image: np.ndarray, factor: tuple[float, float, float], order: int) -&gt; np.ndarray:\n    \"\"\"\n    Scale an image by a given factor in each dimension\n\n    Args:\n        image (np.ndarray): Input image to scale\n        factor (tuple[float, float, float]): Scaling factor in each dimension\n        order (int): Interpolation order, must be 0 for segmentation and 1, 2 for images\n\n    Returns:\n        scaled_image (np.ndarray): Scaled image as numpy array\n    \"\"\"\n    if np.array_equal(factor, [1.0, 1.0, 1.0]):\n        return image\n    else:\n        return zoom(image, zoom=factor, order=order)\n</code></pre>"},{"location":"chapters/python_api/data_processing/#plantseg.dataprocessing.functional.dataprocessing.image_median","title":"<code>plantseg.dataprocessing.functional.dataprocessing.image_median(image: np.ndarray, radius: int) -&gt; np.ndarray</code>","text":"<p>Apply median smoothing on an image with a given radius.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to apply median smoothing</p> </li> <li> <code>radius</code>               (<code>int</code>)           \u2013            <p>Radius of the median filter</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>median_image</code> (              <code>ndarray</code> )          \u2013            <p>Median smoothed image as numpy array</p> </li> </ul> Source code in <code>plantseg/dataprocessing/functional/dataprocessing.py</code> <pre><code>def image_median(image: np.ndarray, radius: int) -&gt; np.ndarray:\n    \"\"\"\n    Apply median smoothing on an image with a given radius.\n\n    Args:\n        image (np.ndarray): Input image to apply median smoothing\n        radius (int): Radius of the median filter\n\n    Returns:\n        median_image (np.ndarray): Median smoothed image as numpy array\n    \"\"\"\n    if image.shape[0] == 1:\n        shape = image.shape\n        median_image = median(image[0], disk(radius))\n        return median_image.reshape(shape)\n    else:\n        return median(image, ball(radius))\n</code></pre>"},{"location":"chapters/python_api/data_processing/#plantseg.dataprocessing.functional.dataprocessing.image_gaussian_smoothing","title":"<code>plantseg.dataprocessing.functional.dataprocessing.image_gaussian_smoothing(image: np.ndarray, sigma: float) -&gt; np.ndarray</code>","text":"<p>Apply gaussian smoothing on an image with a given sigma.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to apply gaussian smoothing</p> </li> <li> <code>sigma</code>               (<code>float</code>)           \u2013            <p>Sigma value for gaussian smoothing</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>smoothed_image</code> (              <code>ndarray</code> )          \u2013            <p>Gaussian smoothed image as numpy array</p> </li> </ul> Source code in <code>plantseg/dataprocessing/functional/dataprocessing.py</code> <pre><code>def image_gaussian_smoothing(image: np.ndarray, sigma: float) -&gt; np.ndarray:\n    \"\"\"\n    Apply gaussian smoothing on an image with a given sigma.\n\n    Args:\n        image (np.ndarray): Input image to apply gaussian smoothing\n        sigma (float): Sigma value for gaussian smoothing\n\n    Returns:\n        smoothed_image (np.ndarray): Gaussian smoothed image as numpy array\n    \"\"\"\n    image = image.astype('float32')\n    max_sigma = (np.array(image.shape) - 1) / 3\n    sigma_array = np.minimum(max_sigma, np.ones(max_sigma.ndim) * sigma)\n    return gaussianSmoothing(image, sigma_array)\n</code></pre>"},{"location":"chapters/python_api/data_processing/#plantseg.dataprocessing.functional.dataprocessing.image_crop","title":"<code>plantseg.dataprocessing.functional.dataprocessing.image_crop(image: np.ndarray, crop_str: str) -&gt; np.ndarray</code>","text":"<p>Crop an image from a crop string like [:, 10:30:, 10:20]</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to crop</p> </li> <li> <code>crop_str</code>               (<code>str</code>)           \u2013            <p>Crop string</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>cropped_image</code> (              <code>ndarray</code> )          \u2013            <p>Cropped image as numpy array</p> </li> </ul> Source code in <code>plantseg/dataprocessing/functional/dataprocessing.py</code> <pre><code>def image_crop(image: np.ndarray, crop_str: str) -&gt; np.ndarray:\n    \"\"\"\n    Crop an image from a crop string like [:, 10:30:, 10:20]\n\n    Args:\n        image (np.ndarray): Input image to crop\n        crop_str (str): Crop string\n\n    Returns:\n        cropped_image (np.ndarray): Cropped image as numpy array\n    \"\"\"\n    crop_str = crop_str.replace('[', '').replace(']', '')\n    slices = tuple(\n        (slice(*(int(i) if i else None for i in part.strip().split(':'))) if ':' in part else int(part.strip()))\n        for part in crop_str.split(',')\n    )\n    return image[slices]\n</code></pre>"},{"location":"chapters/python_api/data_processing/#segmentation-functions","title":"Segmentation Functions","text":""},{"location":"chapters/python_api/data_processing/#plantseg.dataprocessing.functional.labelprocessing.relabel_segmentation","title":"<code>plantseg.dataprocessing.functional.labelprocessing.relabel_segmentation(segmentation_image: np.ndarray) -&gt; np.ndarray</code>","text":"<p>Relabel contiguously a segmentation image, non-touching instances with same id will be relabeled differently. To be noted that measure.label is different from ndimage.label.</p> <p>Parameters:</p> <ul> <li> <code>segmentation_image</code>               (<code>ndarray</code>)           \u2013            <p>segmentation image to relabel</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_segmentation_image</code> (              <code>ndarray</code> )          \u2013            <p>relabeled segmentation image</p> </li> </ul> Source code in <code>plantseg/dataprocessing/functional/labelprocessing.py</code> <pre><code>def relabel_segmentation(segmentation_image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Relabel contiguously a segmentation image, non-touching instances with same id will be relabeled differently.\n    To be noted that measure.label is different from ndimage.label.\n\n    Args:\n        segmentation_image (np.ndarray): segmentation image to relabel\n\n    Returns:\n        new_segmentation_image (np.ndarray): relabeled segmentation image\n\n    \"\"\"\n    return measure.label(segmentation_image)\n</code></pre>"},{"location":"chapters/python_api/data_processing/#plantseg.dataprocessing.functional.labelprocessing.set_background_to_value","title":"<code>plantseg.dataprocessing.functional.labelprocessing.set_background_to_value(segmentation_image: np.ndarray, value: int = 0) -&gt; np.ndarray</code>","text":"<p>Set the largest segment (usually this is the background but not always) to a certain value.</p> <p>Parameters:</p> <ul> <li> <code>segmentation_image</code>               (<code>ndarray</code>)           \u2013            <p>segmentation image to relabel</p> </li> <li> <code>value</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>value to set the background to, default is 0</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_segmentation_image</code> (              <code>ndarray</code> )          \u2013            <p>segmentation image with background set to value</p> </li> </ul> Source code in <code>plantseg/dataprocessing/functional/labelprocessing.py</code> <pre><code>def set_background_to_value(segmentation_image: np.ndarray, value: int = 0) -&gt; np.ndarray:\n    \"\"\"\n    Set the largest segment (usually this is the background but not always) to a certain value.\n\n    Args:\n        segmentation_image (np.ndarray): segmentation image to relabel\n        value (int): value to set the background to, default is 0\n\n    Returns:\n        new_segmentation_image (np.ndarray): segmentation image with background set to value\n    \"\"\"\n    segmentation_image += 1\n    idx, counts = np.unique(segmentation_image, return_counts=True)\n    bg_idx = idx[np.argmax(counts)]\n    return np.where(segmentation_image == bg_idx, value, segmentation_image)\n</code></pre>"},{"location":"chapters/python_api/data_processing/#advanced-functions","title":"Advanced Functions","text":"<p>Documentation in Progress</p> <p>This page is under development.</p>"},{"location":"chapters/python_api/io/","title":"Input/Output","text":"<p>All the input/output operations are handled by the <code>plantseg.io</code> module. This module provides functions to read and write data in different formats. The supported formats are <code>tiff</code>, <code>h5</code>, and <code>zarr</code>, <code>jpeg</code>, <code>png</code>. </p>"},{"location":"chapters/python_api/io/#reading","title":"Reading","text":""},{"location":"chapters/python_api/io/#plantseg.io.smart_load","title":"<code>plantseg.io.smart_load(path, key=None, info_only=False, default=load_tiff) -&gt; Union[tuple, tuple[np.ndarray, tuple]]</code>","text":"<p>Load a dataset from a file and returns some meta info about it. The loader is chosen based on the file extension. Supported formats are: tiff, h5, zarr, and PIL images. If the format is not supported, a default loader can be provided (default: load_tiff).</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to the file to load.</p> </li> <li> <code>key</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>key of the dataset to load (if h5).</p> </li> <li> <code>info_only</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if true will return a tuple with infos such as voxel resolution, units and shape.</p> </li> <li> <code>default</code>               (<code>callable</code>, default:                   <code>load_tiff</code> )           \u2013            <p>default loader if the type is not understood.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>stack</code> (              <code>ndarray</code> )          \u2013            <p>numpy array with the image data.</p> </li> <li> <code>infos</code> (              <code>tuple</code> )          \u2013            <p>tuple with the voxel size, shape, metadata and voxel size unit (if info_only is True).</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data, infos = smart_load('path/to/file.tif')\n&gt;&gt;&gt; data, infos = smart_load('path/to/file.h5', key='raw')\n</code></pre> Source code in <code>plantseg/io/io.py</code> <pre><code>def smart_load(path, key=None, info_only=False, default=load_tiff) -&gt; Union[tuple, tuple[np.ndarray, tuple]]:\n    \"\"\"\n    Load a dataset from a file and returns some meta info about it. The loader is chosen based on the file extension.\n    Supported formats are: tiff, h5, zarr, and PIL images.\n    If the format is not supported, a default loader can be provided (default: load_tiff).\n\n    Args:\n        path (str): path to the file to load.\n        key (str): key of the dataset to load (if h5).\n        info_only (bool): if true will return a tuple with infos such as voxel resolution, units and shape.\n        default (callable): default loader if the type is not understood.\n\n    Returns:\n        stack (np.ndarray): numpy array with the image data.\n        infos (tuple): tuple with the voxel size, shape, metadata and voxel size unit (if info_only is True).\n\n    Examples:\n        &gt;&gt;&gt; data, infos = smart_load('path/to/file.tif')\n        &gt;&gt;&gt; data, infos = smart_load('path/to/file.h5', key='raw')\n\n    \"\"\"\n    _, ext = os.path.splitext(path)\n    if ext in H5_EXTENSIONS:\n        return load_h5(path, key, info_only=info_only)\n\n    elif ext in TIFF_EXTENSIONS:\n        return load_tiff(path, info_only=info_only)\n\n    elif ext in PIL_EXTENSIONS:\n        return load_pill(path, info_only=info_only)\n\n    elif ext in ZARR_EXTENSIONS:\n        return load_zarr(path, key, info_only=info_only)\n\n    else:\n        print(f\"No default found for {ext}, reverting to default loader\")\n        return default(path)\n</code></pre>"},{"location":"chapters/python_api/io/#writing","title":"Writing","text":""},{"location":"chapters/python_api/io/#plantseg.io.create_tiff","title":"<code>plantseg.io.create_tiff(path: str, stack: np.ndarray, voxel_size: tuple[float, float, float], voxel_size_unit: str = 'um') -&gt; None</code>","text":"<p>Create a tiff file from a numpy array</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path of the new file</p> </li> <li> <code>stack</code>               (<code>ndarray</code>)           \u2013            <p>numpy array to save as tiff</p> </li> <li> <code>voxel_size</code>               (<code>list or tuple</code>)           \u2013            <p>tuple of the voxel size</p> </li> <li> <code>voxel_size_unit</code>               (<code>str</code>, default:                   <code>'um'</code> )           \u2013            <p>units of the voxel size</p> </li> </ul> Source code in <code>plantseg/io/tiff.py</code> <pre><code>def create_tiff(path: str, stack: np.ndarray, voxel_size: tuple[float, float, float], voxel_size_unit: str = 'um') -&gt; None:\n    \"\"\"\n    Create a tiff file from a numpy array\n\n    Args:\n        path (str): path of the new file\n        stack (np.ndarray): numpy array to save as tiff\n        voxel_size (list or tuple): tuple of the voxel size\n        voxel_size_unit (str): units of the voxel size\n\n    \"\"\"\n    # taken from: https://pypi.org/project/tifffile docs\n    z, y, x = stack.shape\n    stack = stack.reshape(1, z, 1, y, x, 1) # dimensions in TZCYXS order\n    spacing, y, x = voxel_size\n    resolution = (1. / x, 1. / y)\n    # Save output results as tiff\n    tifffile.imwrite(path,\n                     data=stack,\n                     dtype=stack.dtype,\n                     imagej=True,\n                     resolution=resolution,\n                     metadata={'axes': 'TZCYXS', 'spacing': spacing, 'unit': voxel_size_unit},\n                     compression='zlib')\n</code></pre>"},{"location":"chapters/python_api/io/#plantseg.io.create_h5","title":"<code>plantseg.io.create_h5(path: str, stack: np.ndarray, key: str, voxel_size: tuple[float, float, float] = (1.0, 1.0, 1.0), mode: str = 'a') -&gt; None</code>","text":"<p>Create a dataset inside a h5 file from a numpy array.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>file path.</p> </li> <li> <code>stack</code>               (<code>ndarray</code>)           \u2013            <p>numpy array to save as dataset in the h5 file.</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>key of the dataset in the h5 file.</p> </li> <li> <code>voxel_size</code>               (<code>tuple[float, float, float]</code>, default:                   <code>(1.0, 1.0, 1.0)</code> )           \u2013            <p>voxel size in micrometers.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'a'</code> )           \u2013            <p>mode to open the h5 file ['w', 'a'].</p> </li> </ul> Source code in <code>plantseg/io/h5.py</code> <pre><code>def create_h5(path: str,\n              stack: np.ndarray,\n              key: str,\n              voxel_size: tuple[float, float, float] = (1.0, 1.0, 1.0),\n              mode: str = 'a') -&gt; None:\n    \"\"\"\n    Create a dataset inside a h5 file from a numpy array.\n\n    Args:\n        path (str): file path.\n        stack (np.ndarray): numpy array to save as dataset in the h5 file.\n        key (str): key of the dataset in the h5 file.\n        voxel_size (tuple[float, float, float]: voxel size in micrometers.\n        mode (str): mode to open the h5 file ['w', 'a'].\n\n    \"\"\"\n\n    with h5py.File(path, mode) as f:\n        if key in f:\n            del f[key]\n        f.create_dataset(key, data=stack, compression='gzip')\n        # save voxel_size\n        f[key].attrs['element_size_um'] = voxel_size\n</code></pre>"},{"location":"chapters/python_api/io/#plantseg.io.create_zarr","title":"<code>plantseg.io.create_zarr(path: str, stack: np.ndarray, key: str, voxel_size: tuple[float, float, float] = (1.0, 1.0, 1.0), mode: str = 'a') -&gt; None</code>","text":"<p>Create a Zarr array from a NumPy array.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The file path to the Zarr file.</p> </li> <li> <code>stack</code>               (<code>ndarray</code>)           \u2013            <p>The NumPy array to save as a dataset.</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>The internal key of the desired dataset.</p> </li> <li> <code>voxel_size</code>               (<code>tuple[float, float, float]</code>, default:                   <code>(1.0, 1.0, 1.0)</code> )           \u2013            <p>The voxel size in micrometers.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'a'</code> )           \u2013            <p>The mode to open the Zarr file ['w', 'a'].</p> </li> </ul> Source code in <code>plantseg/io/zarr.py</code> <pre><code>def create_zarr(path: str,\n                stack: np.ndarray,\n                key: str,\n                voxel_size: tuple[float, float, float] = (1.0, 1.0, 1.0),\n                mode: str = 'a') -&gt; None:\n    \"\"\"\n    Create a Zarr array from a NumPy array.\n\n    Args:\n        path (str): The file path to the Zarr file.\n        stack (np.ndarray): The NumPy array to save as a dataset.\n        key (str): The internal key of the desired dataset.\n        voxel_size (tuple[float, float, float]): The voxel size in micrometers.\n        mode (str): The mode to open the Zarr file ['w', 'a'].\n\n    \"\"\"\n    zarr_file = zarr.open_group(path, mode)\n    zarr_file.create_dataset(key, data=stack, compression='gzip', overwrite=True)\n    zarr_file[key].attrs['element_size_um'] = voxel_size\n</code></pre>"},{"location":"chapters/python_api/io/#utilities","title":"Utilities","text":""},{"location":"chapters/python_api/io/#plantseg.io.load_shape","title":"<code>plantseg.io.load_shape(path: str, key: str = None) -&gt; tuple[int, ...]</code>","text":"<p>Load only the stack shape from a file using the smart loader.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to the file to load.</p> </li> <li> <code>key</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>key of the dataset to load (if h5 or zarr).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[int, ...]</code>           \u2013            <p>shape (tuple[int, ...]) shape of the image stack.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; shape = load_shape('path/to/file.tif')\n&gt;&gt;&gt; print(shape)\n(10, 512, 512)\n</code></pre> Source code in <code>plantseg/io/io.py</code> <pre><code>def load_shape(path: str, key: str = None) -&gt; tuple[int, ...]:\n    \"\"\"\n    Load only the stack shape from a file using the smart loader.\n\n    Args:\n        path (str): path to the file to load.\n        key (str): key of the dataset to load (if h5 or zarr).\n\n    Returns:\n        shape (tuple[int, ...]) shape of the image stack.\n\n    Examples:\n        &gt;&gt;&gt; shape = load_shape('path/to/file.tif')\n        &gt;&gt;&gt; print(shape)\n        (10, 512, 512)\n    \"\"\"\n    _, data_shape, _, _ = smart_load(path, key=key, info_only=True)\n    return data_shape\n</code></pre>"},{"location":"chapters/python_api/io/#tiff-utilities","title":"Tiff Utilities","text":""},{"location":"chapters/python_api/io/#plantseg.io.tiff.read_tiff_voxel_size","title":"<code>plantseg.io.tiff.read_tiff_voxel_size(file_path: str) -&gt; tuple[tuple[float, float, float], str]</code>","text":"<p>Returns the voxels size and the voxel units for imagej and ome style tiff (if absent returns [1, 1, 1], um)</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str</code>)           \u2013            <p>path to the tiff file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[float, float, float]</code>           \u2013            <p>voxel size</p> </li> <li> <code>str</code>           \u2013            <p>voxel size unit</p> </li> </ul> Source code in <code>plantseg/io/tiff.py</code> <pre><code>def read_tiff_voxel_size(file_path: str) -&gt; tuple[tuple[float, float, float], str]:\n    \"\"\"\n    Returns the voxels size and the voxel units for imagej and ome style tiff (if absent returns [1, 1, 1], um)\n\n    Args:\n        file_path (str): path to the tiff file\n\n    Returns:\n        voxel size\n        voxel size unit\n\n    \"\"\"\n    with tifffile.TiffFile(file_path) as tiff:\n        if tiff.imagej_metadata is not None:\n            [z, y, x], voxel_size_unit = _read_imagej_meta(tiff)\n\n        elif tiff.ome_metadata is not None:\n            [z, y, x], voxel_size_unit = _read_ome_meta(tiff)\n\n        else:\n            # default voxel size\n            warnings.warn('No metadata found. '\n                          'Reverting to default voxel size (1., 1., 1.) um')\n            x, y, z = 1., 1., 1.\n            voxel_size_unit = 'um'\n\n        return (z, y, x), voxel_size_unit\n</code></pre>"},{"location":"chapters/python_api/io/#h5-utilities","title":"H5 Utilities","text":""},{"location":"chapters/python_api/io/#plantseg.io.h5.list_keys","title":"<code>plantseg.io.h5.list_keys(path: str) -&gt; list[str]</code>","text":"<p>List all keys in a h5 file</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to the h5 file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>keys</code> (              <code>list[str]</code> )          \u2013            <p>A list of keys in the h5 file.</p> </li> </ul> Source code in <code>plantseg/io/h5.py</code> <pre><code>def list_keys(path: str) -&gt; list[str]:\n    \"\"\"\n    List all keys in a h5 file\n\n    Args:\n        path (str): path to the h5 file\n\n    Returns:\n        keys (list[str]): A list of keys in the h5 file.\n\n    \"\"\"\n    def _recursive_find_keys(f, base='/'):\n        _list_keys = []\n        for key, dataset in f.items():\n            if isinstance(dataset, h5py.Group):\n                new_base = f\"{base}{key}/\"\n                _list_keys += _recursive_find_keys(dataset, new_base)\n\n            elif isinstance(dataset, h5py.Dataset):\n                _list_keys.append(f'{base}{key}')\n        return _list_keys\n\n    with h5py.File(path, 'r') as h5_f:\n        return _recursive_find_keys(h5_f)\n</code></pre>"},{"location":"chapters/python_api/io/#plantseg.io.h5.del_h5_key","title":"<code>plantseg.io.h5.del_h5_key(path: str, key: str, mode: str = 'a') -&gt; None</code>","text":"<p>helper function to delete a dataset from a h5file</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to the h5file</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>key of the dataset to delete</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'a'</code> )           \u2013            <p>mode to open the h5 file ['r', 'r+']</p> </li> </ul> Source code in <code>plantseg/io/h5.py</code> <pre><code>def del_h5_key(path: str, key: str, mode: str = 'a') -&gt; None:\n    \"\"\"\n    helper function to delete a dataset from a h5file\n\n    Args:\n        path (str): path to the h5file\n        key (str): key of the dataset to delete\n        mode (str): mode to open the h5 file ['r', 'r+']\n\n    \"\"\"\n    with h5py.File(path, mode) as f:\n        if key in f:\n            del f[key]\n            f.close()\n</code></pre>"},{"location":"chapters/python_api/io/#plantseg.io.h5.rename_h5_key","title":"<code>plantseg.io.h5.rename_h5_key(path: str, old_key: str, new_key: str, mode='r+') -&gt; None</code>","text":"<p>Rename the 'old_key' dataset to 'new_key' </p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>path to the h5 file</p> </li> <li> <code>old_key</code>               (<code>str</code>)           \u2013            <p>old key name</p> </li> <li> <code>new_key</code>               (<code>str</code>)           \u2013            <p>new key name</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'r+'</code> )           \u2013            <p>mode to open the h5 file ['r', 'r+']</p> </li> </ul> Source code in <code>plantseg/io/h5.py</code> <pre><code>def rename_h5_key(path: str, old_key: str, new_key: str, mode='r+') -&gt; None:\n    \"\"\" \n    Rename the 'old_key' dataset to 'new_key' \n\n    Args:\n        path (str): path to the h5 file\n        old_key (str): old key name\n        new_key (str): new key name\n        mode (str): mode to open the h5 file ['r', 'r+']\n\n    \"\"\"\n    with h5py.File(path, mode) as f:\n        if old_key in f:\n            f[new_key] = f[old_key]\n            del f[old_key]\n            f.close()\n</code></pre>"},{"location":"chapters/python_api/io/#zarr-utilities","title":"Zarr Utilities","text":""},{"location":"chapters/python_api/io/#plantseg.io.zarr.list_keys","title":"<code>plantseg.io.zarr.list_keys(path: str) -&gt; list[str]</code>","text":"<p>List all keys in a Zarr file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the Zarr file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>keys</code> (              <code>list[str]</code> )          \u2013            <p>A list of keys in the Zarr file.</p> </li> </ul> Source code in <code>plantseg/io/zarr.py</code> <pre><code>def list_keys(path: str) -&gt; list[str]:\n    \"\"\"\n    List all keys in a Zarr file.\n\n    Args:\n        path (str): The path to the Zarr file.\n\n    Returns:\n        keys (list[str]): A list of keys in the Zarr file.\n    \"\"\"\n    def _recursive_find_keys(zarr_group: zarr.Group, base: Path = Path('')) -&gt; list[str]:\n        _list_keys = []\n        for key, dataset in zarr_group.items():\n            if isinstance(dataset, zarr.Group):\n                new_base = base / key\n                _list_keys.extend(_recursive_find_keys(dataset, new_base))\n            elif isinstance(dataset, zarr.Array):\n                _list_keys.append(str(base / key))\n        return _list_keys\n\n    zarr_file = zarr.open_group(path, 'r')\n    return _recursive_find_keys(zarr_file)\n</code></pre>"},{"location":"chapters/python_api/io/#plantseg.io.zarr.del_zarr_key","title":"<code>plantseg.io.zarr.del_zarr_key(path: str, key: str, mode: str = 'a') -&gt; None</code>","text":"<p>Delete a dataset from a Zarr file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the Zarr file.</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>The internal key of the dataset to be deleted.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'a'</code> )           \u2013            <p>The mode to open the Zarr file ['w', 'a'].</p> </li> </ul> Source code in <code>plantseg/io/zarr.py</code> <pre><code>def del_zarr_key(path: str, key: str, mode: str = 'a') -&gt; None:\n    \"\"\"\n    Delete a dataset from a Zarr file.\n\n    Args:\n        path (str): The path to the Zarr file.\n        key (str): The internal key of the dataset to be deleted.\n        mode (str): The mode to open the Zarr file ['w', 'a'].\n\n    \"\"\"\n    zarr_file = zarr.open_group(path, mode)\n    if key in zarr_file:\n        del zarr_file[key]\n</code></pre>"},{"location":"chapters/python_api/io/#plantseg.io.zarr.rename_zarr_key","title":"<code>plantseg.io.zarr.rename_zarr_key(path: str, old_key: str, new_key: str, mode='r+') -&gt; None</code>","text":"<p>Rename a dataset in a Zarr file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the Zarr file.</p> </li> <li> <code>old_key</code>               (<code>str</code>)           \u2013            <p>The current key of the dataset.</p> </li> <li> <code>new_key</code>               (<code>str</code>)           \u2013            <p>The new key for the dataset.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'r+'</code> )           \u2013            <p>The mode to open the Zarr file ['r+'].</p> </li> </ul> Source code in <code>plantseg/io/zarr.py</code> <pre><code>def rename_zarr_key(path: str, old_key: str, new_key: str, mode='r+') -&gt; None:\n    \"\"\"\n    Rename a dataset in a Zarr file.\n\n    Args:\n        path (str): The path to the Zarr file.\n        old_key (str): The current key of the dataset.\n        new_key (str): The new key for the dataset.\n        mode (str): The mode to open the Zarr file ['r+'].\n\n    \"\"\"\n    zarr_file = zarr.open_group(path, mode)\n    if old_key in zarr_file:\n        zarr_file[new_key] = zarr_file[old_key]\n        del zarr_file[old_key]\n</code></pre>"},{"location":"chapters/python_api/segmentation/","title":"PlantSeg Segmentation","text":"<p>The PlantSeg segmentation module implements all segmentation routine in plantseg.</p> <ul> <li>DT Watershed </li> </ul> <ul> <li>GASP </li> </ul> <ul> <li>Multicut </li> </ul> <ul> <li>Mutex Watershed </li> </ul> <ul> <li>Lifted Multicut </li> </ul> <ul> <li>Simple ITK Watershed </li> </ul>"},{"location":"chapters/python_api/segmentation/#plantseg.segmentation.functional.segmentation.dt_watershed","title":"<code>plantseg.segmentation.functional.segmentation.dt_watershed(boundary_pmaps: np.ndarray, threshold: float = 0.5, sigma_seeds: float = 1.0, stacked: bool = False, sigma_weights: float = 2.0, min_size: int = 100, alpha: float = 1.0, pixel_pitch: Optional[tuple[int, ...]] = None, apply_nonmax_suppression: bool = False, n_threads: Optional[int] = None, mask: Optional[np.ndarray] = None) -&gt; np.ndarray</code>","text":"<p>Performs watershed segmentation using distance transforms on boundary probability maps.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>Input height maps, typically boundary probability maps from a CNN.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold applied to boundary maps before distance transform.</p> </li> <li> <code>sigma_seeds</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Smoothing factor for the watershed seed map..</p> </li> <li> <code>stacked</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, performs watershed slice-by-slice (2D), otherwise in 3D.</p> </li> <li> <code>sigma_weights</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Smoothing factor for the watershed weight map.</p> </li> <li> <code>min_size</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Minimal size of watershed segments.</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Alpha blending factor used to combine the input and distance transform into the watershed weight map.</p> </li> <li> <code>pixel_pitch</code>               (<code>Optional[tuple[int, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Pixel pitch to use for anisotropic distance calculation.</p> </li> <li> <code>apply_nonmax_suppression</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, applies non-maximum suppression to filter out seeds. Needs nifty.</p> </li> <li> <code>n_threads</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of threads for parallel processing, applicable in 2D mode.</p> </li> <li> <code>mask</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>Mask array to exclude certain regions from segmentation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: The labeled segmentation map from the watershed algorithm.</p> </li> </ul> Source code in <code>plantseg/segmentation/functional/segmentation.py</code> <pre><code>def dt_watershed(\n    boundary_pmaps: np.ndarray,\n    threshold: float = 0.5,\n    sigma_seeds: float = 1.0,\n    stacked: bool = False,\n    sigma_weights: float = 2.0,\n    min_size: int = 100,\n    alpha: float = 1.0,\n    pixel_pitch: Optional[tuple[int, ...]] = None,\n    apply_nonmax_suppression: bool = False,\n    n_threads: Optional[int] = None,\n    mask: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"Performs watershed segmentation using distance transforms on boundary probability maps.\n\n    Args:\n        boundary_pmaps (np.ndarray): Input height maps, typically boundary probability maps from a CNN.\n        threshold (float): Threshold applied to boundary maps before distance transform.\n        sigma_seeds (float): Smoothing factor for the watershed seed map..\n        stacked (bool): If True, performs watershed slice-by-slice (2D), otherwise in 3D.\n        sigma_weights (float): Smoothing factor for the watershed weight map.\n        min_size (int): Minimal size of watershed segments.\n        alpha (float): Alpha blending factor used to combine the input and distance transform into the watershed weight map.\n        pixel_pitch (Optional[tuple[int, ...]]): Pixel pitch to use for anisotropic distance calculation.\n        apply_nonmax_suppression (bool): If True, applies non-maximum suppression to filter out seeds. Needs nifty.\n        n_threads (Optional[int]): Number of threads for parallel processing, applicable in 2D mode.\n        mask (Optional[np.ndarray]): Mask array to exclude certain regions from segmentation.\n\n    Returns:\n        np.ndarray: The labeled segmentation map from the watershed algorithm.\n\n    \"\"\"\n    # Prepare the keyword arguments for the watershed function\n    boundary_pmaps = boundary_pmaps.astype('float32')\n    ws_kwargs = {\n        \"threshold\": threshold,\n        \"sigma_seeds\": sigma_seeds,\n        \"sigma_weights\": sigma_weights,\n        \"min_size\": min_size,\n        \"alpha\": alpha,\n        \"pixel_pitch\": pixel_pitch,\n        \"apply_nonmax_suppression\": apply_nonmax_suppression,\n        \"mask\": mask,\n    }\n    if stacked:\n        # Apply watershed in 2D, slice by slice\n        segmentation, _ = stacked_watershed(\n            boundary_pmaps, ws_function=distance_transform_watershed, n_threads=n_threads, **ws_kwargs\n        )\n    else:\n        # Apply watershed in 3D\n        segmentation, _ = distance_transform_watershed(boundary_pmaps, **ws_kwargs)\n\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/segmentation/#plantseg.segmentation.functional.segmentation.gasp","title":"<code>plantseg.segmentation.functional.segmentation.gasp(boundary_pmaps: np.ndarray, superpixels: Optional[np.ndarray] = None, gasp_linkage_criteria: str = 'average', beta: float = 0.5, post_minsize: int = 100, n_threads: int = 6) -&gt; np.ndarray</code>","text":"<p>Perform segmentation using the GASP algorithm with affinity maps.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>Cell boundary predictions.</p> </li> <li> <code>superpixels</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>Superpixel segmentation. If None, GASP will be run from the pixels. Default is None.</p> </li> <li> <code>gasp_linkage_criteria</code>               (<code>str</code>, default:                   <code>'average'</code> )           \u2013            <p>Linkage criteria for GASP. Default is 'average'.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Beta parameter for GASP. Small values steer towards under-segmentation, while high values bias towards over-segmentation. Default is 0.5.</p> </li> <li> <code>post_minsize</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Minimum size of the segments after GASP. Default is 100.</p> </li> <li> <code>n_threads</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of threads used for GASP. Default is 6.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: GASP output segmentation.</p> </li> </ul> Source code in <code>plantseg/segmentation/functional/segmentation.py</code> <pre><code>def gasp(\n    boundary_pmaps: np.ndarray,\n    superpixels: Optional[np.ndarray] = None,\n    gasp_linkage_criteria: str = 'average',\n    beta: float = 0.5,\n    post_minsize: int = 100,\n    n_threads: int = 6,\n) -&gt; np.ndarray:\n    \"\"\"\n    Perform segmentation using the GASP algorithm with affinity maps.\n\n    Args:\n        boundary_pmaps (np.ndarray): Cell boundary predictions.\n        superpixels (Optional[np.ndarray]): Superpixel segmentation. If None, GASP will be run from the pixels. Default is None.\n        gasp_linkage_criteria (str): Linkage criteria for GASP. Default is 'average'.\n        beta (float): Beta parameter for GASP. Small values steer towards under-segmentation, while high values bias towards over-segmentation. Default is 0.5.\n        post_minsize (int): Minimum size of the segments after GASP. Default is 100.\n        n_threads (int): Number of threads used for GASP. Default is 6.\n\n    Returns:\n        np.ndarray: GASP output segmentation.\n    \"\"\"\n    if superpixels is not None:\n        assert boundary_pmaps.shape == superpixels.shape, \"Shape mismatch between boundary_pmaps and superpixels.\"\n        if superpixels.ndim == 2:  # Ensure superpixels is 3D if provided\n            superpixels = superpixels[None, ...]\n\n    # Prepare the arguments for running GASP\n    run_GASP_kwargs = {\n        'linkage_criteria': gasp_linkage_criteria,\n        'add_cannot_link_constraints': False,\n        'use_efficient_implementations': False,\n    }\n\n    # Interpret boundary_pmaps as affinities and prepare for GASP\n    boundary_pmaps = boundary_pmaps.astype('float32')\n    affinities = np.stack([boundary_pmaps] * 3, axis=0)\n\n    offsets = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n    # Shift is required to correct aligned affinities\n    affinities = shift_affinities(affinities, offsets=offsets)\n\n    # invert affinities\n    affinities = 1 - affinities\n\n    # Initialize and run GASP\n    gasp_instance = GaspFromAffinities(\n        offsets,\n        superpixel_generator=None if superpixels is None else (lambda *args, **kwargs: superpixels),\n        run_GASP_kwargs=run_GASP_kwargs,\n        n_threads=n_threads,\n        beta_bias=beta,\n    )\n    segmentation, _ = gasp_instance(affinities)\n\n    # Apply size filtering if specified\n    if post_minsize &gt; 0:\n        segmentation, _ = apply_size_filter(segmentation.astype('uint32'), boundary_pmaps, post_minsize)\n\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/segmentation/#plantseg.segmentation.functional.segmentation.multicut","title":"<code>plantseg.segmentation.functional.segmentation.multicut(boundary_pmaps: np.ndarray, superpixels: np.ndarray, beta: float = 0.5, post_minsize: int = 50) -&gt; np.ndarray</code>","text":"<p>Multicut segmentation from boundary predictions.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>cell boundary predictions, 3D array of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>superpixels</code>               (<code>ndarray</code>)           \u2013            <p>superpixel segmentation. Must have the same shape as boundary_pmaps.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter for the Multicut. A small value will steer the segmentation towards under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)</p> </li> <li> <code>post_minsize</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>minimal size of the segments after Multicut. (default: 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>Multicut output segmentation</p> </li> </ul> Source code in <code>plantseg/segmentation/functional/segmentation.py</code> <pre><code>def multicut(\n    boundary_pmaps: np.ndarray, superpixels: np.ndarray, beta: float = 0.5, post_minsize: int = 50\n) -&gt; np.ndarray:\n    \"\"\"\n    Multicut segmentation from boundary predictions.\n\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary predictions, 3D array of shape (Z, Y, X) with values between 0 and 1.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n        beta (float): beta parameter for the Multicut. A small value will steer the segmentation towards\n            under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after Multicut. (default: 100)\n\n    Returns:\n        segmentation (np.ndarray): Multicut output segmentation\n    \"\"\"\n\n    rag = compute_rag(superpixels)\n\n    # Prob -&gt; edge costs\n    boundary_pmaps = boundary_pmaps.astype('float32')\n    costs = compute_mc_costs(boundary_pmaps, rag, beta=beta)\n\n    # Creating graph\n    graph = nifty.graph.undirectedGraph(rag.numberOfNodes)\n    graph.insertEdges(rag.uvIds())\n\n    # Solving Multicut\n    node_labels = multicut_kernighan_lin(graph, costs)\n    segmentation = nifty.tools.take(node_labels, superpixels)\n\n    # run size threshold\n    if post_minsize &gt; 0:\n        segmentation, _ = apply_size_filter(segmentation.astype('uint32'), boundary_pmaps, post_minsize)\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/segmentation/#plantseg.segmentation.functional.segmentation.mutex_ws","title":"<code>plantseg.segmentation.functional.segmentation.mutex_ws(boundary_pmaps: np.ndarray, superpixels: Optional[np.ndarray] = None, beta: float = 0.5, post_minsize: int = 100, n_threads: int = 6) -&gt; np.ndarray</code>","text":"<p>Wrapper around gasp with mutex_watershed as linkage criteria.</p> <p>Args:magicgui     boundary_pmaps (np.ndarray): cell boundary predictions. 3D array of shape (Z, Y, X) with values between 0 and 1.     superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.         If None, GASP will be run from the pixels. (default: None)     beta (float): beta parameter for GASP. A small value will steer the segmentation towards under-segmentation.         While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)     post_minsize (int): minimal size of the segments after GASP. (default: 100)     n_threads (int): number of threads used for GASP. (default: 6)</p> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>MutexWS output segmentation</p> </li> </ul> Source code in <code>plantseg/segmentation/functional/segmentation.py</code> <pre><code>def mutex_ws(\n    boundary_pmaps: np.ndarray,\n    superpixels: Optional[np.ndarray] = None,\n    beta: float = 0.5,\n    post_minsize: int = 100,\n    n_threads: int = 6,\n) -&gt; np.ndarray:\n    \"\"\"\n    Wrapper around gasp with mutex_watershed as linkage criteria.\n\n    Args:magicgui\n        boundary_pmaps (np.ndarray): cell boundary predictions. 3D array of shape (Z, Y, X) with values between 0 and 1.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n            If None, GASP will be run from the pixels. (default: None)\n        beta (float): beta parameter for GASP. A small value will steer the segmentation towards under-segmentation.\n            While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after GASP. (default: 100)\n        n_threads (int): number of threads used for GASP. (default: 6)\n\n    Returns:\n        segmentation (np.ndarray): MutexWS output segmentation\n\n    \"\"\"\n    return gasp(\n        boundary_pmaps=boundary_pmaps,\n        superpixels=superpixels,\n        gasp_linkage_criteria='mutex_watershed',\n        beta=beta,\n        post_minsize=post_minsize,\n        n_threads=n_threads,\n    )\n</code></pre>"},{"location":"chapters/python_api/segmentation/#plantseg.segmentation.functional.segmentation.lifted_multicut_from_nuclei_pmaps","title":"<code>plantseg.segmentation.functional.segmentation.lifted_multicut_from_nuclei_pmaps(boundary_pmaps: np.ndarray, nuclei_pmaps: np.ndarray, superpixels: np.ndarray, beta: float = 0.5, post_minsize: int = 50) -&gt; np.ndarray</code>","text":"<p>Lifted Multicut segmentation from boundary predictions and nuclei predictions.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>cell boundary predictions, 3D array of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>nuclei_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>nuclei predictions. Must have the same shape as boundary_pmaps and with values between 0 and 1.</p> </li> <li> <code>superpixels</code>               (<code>ndarray</code>)           \u2013            <p>superpixel segmentation. Must have the same shape as boundary_pmaps.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter for the Multicut. A small value will steer the segmentation towards</p> </li> <li> <code>under-segmentation.</code>               (<code>While a high-value bias the segmentation towards the over-segmentation. (default</code>)           \u2013            <p>0.5)</p> </li> <li> <code>post_minsize</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>minimal size of the segments after Multicut. (default: 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>Multicut output segmentation</p> </li> </ul> Source code in <code>plantseg/segmentation/functional/segmentation.py</code> <pre><code>def lifted_multicut_from_nuclei_pmaps(\n    boundary_pmaps: np.ndarray,\n    nuclei_pmaps: np.ndarray,\n    superpixels: np.ndarray,\n    beta: float = 0.5,\n    post_minsize: int = 50,\n) -&gt; np.ndarray:\n    \"\"\"\n    Lifted Multicut segmentation from boundary predictions and nuclei predictions.\n\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary predictions, 3D array of shape (Z, Y, X) with values between 0 and 1.\n        nuclei_pmaps (np.ndarray): nuclei predictions. Must have the same shape as boundary_pmaps and\n            with values between 0 and 1.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n        beta (float): beta parameter for the Multicut. A small value will steer the segmentation towards\n        under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after Multicut. (default: 100)\n\n    Returns:\n        segmentation (np.ndarray): Multicut output segmentation\n    \"\"\"\n    # compute the region adjacency graph\n    rag = compute_rag(superpixels)\n\n    # compute multi cut edges costs\n    boundary_pmaps = boundary_pmaps.astype('float32')\n    costs = compute_mc_costs(boundary_pmaps, rag, beta)\n\n    # assert nuclei pmaps are floats\n    nuclei_pmaps = nuclei_pmaps.astype('float32')\n    input_maps = [nuclei_pmaps]\n    assignment_threshold = 0.9\n\n    # compute lifted multicut features from boundary pmaps\n    lifted_uvs, lifted_costs = lifted_problem_from_probabilities(\n        rag, superpixels, input_maps, assignment_threshold, graph_depth=4\n    )\n\n    # solve the full lifted problem using the kernighan lin approximation introduced in\n    # http://openaccess.thecvf.com/content_iccv_2015/html/Keuper_Efficient_Decomposition_of_ICCV_2015_paper.html\n    node_labels = lmc.lifted_multicut_kernighan_lin(rag, costs, lifted_uvs, lifted_costs)\n    segmentation = project_node_labels_to_pixels(rag, node_labels)\n\n    # run size threshold\n    if post_minsize &gt; 0:\n        segmentation, _ = apply_size_filter(segmentation.astype('uint32'), boundary_pmaps, post_minsize)\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/segmentation/#plantseg.segmentation.functional.segmentation.lifted_multicut_from_nuclei_segmentation","title":"<code>plantseg.segmentation.functional.segmentation.lifted_multicut_from_nuclei_segmentation(boundary_pmaps: np.ndarray, nuclei_seg: np.ndarray, superpixels: np.ndarray, beta: float = 0.5, post_minsize: int = 50) -&gt; np.ndarray</code>","text":"<p>Lifted Multicut segmentation from boundary predictions and nuclei segmentation.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>cell boundary predictions, 3D array of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>nuclei_seg</code>               (<code>ndarray</code>)           \u2013            <p>Nuclei segmentation. Must have the same shape as boundary_pmaps.</p> </li> <li> <code>superpixels</code>               (<code>ndarray</code>)           \u2013            <p>superpixel segmentation. Must have the same shape as boundary_pmaps.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter for the Multicut. A small value will steer the segmentation towards</p> </li> <li> <code>under-segmentation.</code>               (<code>While a high-value bias the segmentation towards the over-segmentation. (default</code>)           \u2013            <p>0.5)</p> </li> <li> <code>post_minsize</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>minimal size of the segments after Multicut. (default: 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>Multicut output segmentation</p> </li> </ul> Source code in <code>plantseg/segmentation/functional/segmentation.py</code> <pre><code>def lifted_multicut_from_nuclei_segmentation(\n    boundary_pmaps: np.ndarray,\n    nuclei_seg: np.ndarray,\n    superpixels: np.ndarray,\n    beta: float = 0.5,\n    post_minsize: int = 50,\n) -&gt; np.ndarray:\n    \"\"\"\n    Lifted Multicut segmentation from boundary predictions and nuclei segmentation.\n\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary predictions, 3D array of shape (Z, Y, X) with values between 0 and 1.\n        nuclei_seg (np.ndarray): Nuclei segmentation. Must have the same shape as boundary_pmaps.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n        beta (float): beta parameter for the Multicut. A small value will steer the segmentation towards\n        under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after Multicut. (default: 100)\n\n    Returns:\n        segmentation (np.ndarray): Multicut output segmentation\n    \"\"\"\n    # compute the region adjacency graph\n    rag = compute_rag(superpixels)\n\n    # compute multi cut edges costs\n    boundary_pmaps = boundary_pmaps.astype('float32')\n    costs = compute_mc_costs(boundary_pmaps, rag, beta)\n    max_cost = np.abs(np.max(costs))\n    lifted_uvs, lifted_costs = lifted_problem_from_segmentation(\n        rag,\n        superpixels,\n        nuclei_seg,\n        overlap_threshold=0.2,\n        graph_depth=4,\n        same_segment_cost=5 * max_cost,\n        different_segment_cost=-5 * max_cost,\n    )\n\n    # solve the full lifted problem using the kernighan lin approximation introduced in\n    # http://openaccess.thecvf.com/content_iccv_2015/html/Keuper_Efficient_Decomposition_of_ICCV_2015_paper.html\n    lifted_costs = lifted_costs.astype('float64')\n    node_labels = lmc.lifted_multicut_kernighan_lin(rag, costs, lifted_uvs, lifted_costs)\n    segmentation = project_node_labels_to_pixels(rag, node_labels)\n\n    # run size threshold\n    if post_minsize &gt; 0:\n        segmentation, _ = apply_size_filter(segmentation.astype('uint32'), boundary_pmaps, post_minsize)\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/segmentation/#plantseg.segmentation.functional.segmentation.simple_itk_watershed","title":"<code>plantseg.segmentation.functional.segmentation.simple_itk_watershed(boundary_pmaps: np.ndarray, threshold: float = 0.5, sigma: float = 1.0, minsize: int = 100) -&gt; np.ndarray</code>","text":"<p>Simple itk watershed segmentation.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>cell boundary predictions. 3D array of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>threshold for the watershed segmentation. (default: 0.5)</p> </li> <li> <code>sigma</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>sigma for the gaussian smoothing. (default: 1.0)</p> </li> <li> <code>minsize</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>minimal size of the segments after segmentation. (default: 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>watershed output segmentation (using SimpleITK)</p> </li> </ul> Source code in <code>plantseg/segmentation/functional/segmentation.py</code> <pre><code>def simple_itk_watershed(\n    boundary_pmaps: np.ndarray, threshold: float = 0.5, sigma: float = 1.0, minsize: int = 100\n) -&gt; np.ndarray:\n    \"\"\"\n    Simple itk watershed segmentation.\n\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary predictions. 3D array of shape (Z, Y, X) with values between 0 and 1.\n        threshold (float): threshold for the watershed segmentation. (default: 0.5)\n        sigma (float): sigma for the gaussian smoothing. (default: 1.0)\n        minsize (int): minimal size of the segments after segmentation. (default: 100)\n\n    Returns:\n        segmentation (np.ndarray): watershed output segmentation (using SimpleITK)\n\n    \"\"\"\n    if not sitk_installed:\n        raise ValueError('please install sitk before running this process')\n\n    if sigma &gt; 0:\n        # fix ws sigma length\n        # ws sigma cannot be shorter than pmaps dims\n        max_sigma = (np.array(boundary_pmaps.shape) - 1) / 3\n        ws_sigma = np.minimum(max_sigma, np.ones(max_sigma.ndim) * sigma)\n        boundary_pmaps = gaussianSmoothing(boundary_pmaps, ws_sigma)\n\n    # Itk watershed + size filtering\n    itk_pmaps = sitk.GetImageFromArray(boundary_pmaps)\n    itk_segmentation = sitk.MorphologicalWatershed(itk_pmaps, threshold, markWatershedLine=False, fullyConnected=False)\n    itk_segmentation = sitk.RelabelComponent(itk_segmentation, minsize)\n    segmentation = sitk.GetArrayFromImage(itk_segmentation).astype(np.uint16)\n    return segmentation\n</code></pre>"},{"location":"snippets/napari/dataprocessing/rescale/","title":"Rescale","text":"From factorTo layer voxel sizeTo layer shapeTo model voxel sizeTo voxel sizeTo shapeSet voxel size <p>Using the <code>From factor</code> mode, the user can rescale the image by a multiplicate factor. For example, if the image has a shape <code>(10, 10, 10)</code> and the user wants to rescale it by a factor of <code>(2, 2, 2)</code>, the new size will be <code>(20, 20, 20)</code>.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Rescaling factor: Define the scaling factor to use for resizing the input image.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To layer voxel size</code> mode, the user can rescale the image to the voxel size of a specific layer. For example, if two images are loaded in the viewer, one with a voxel size of <code>(0.1, 0.1, 0.1)um</code> and the other with a voxel size of <code>(0.1, 0.05, 0.05)um</code>, the user can rescale the first image to the voxel size of the second image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Reference layer: Rescale to same voxel size as selected layer.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To layer shape</code> mode, the user can rescale the image to the shape of a specific layer. For example, if two images are loaded in the viewer, one with a shape <code>(10, 10, 10)</code> and the other with a shape <code>(20, 20, 20)</code>, the user can rescale the first image to the shape of the second image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Reference layer: Rescale to same voxel size as selected layer.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To model voxel size</code> mode, the user can rescale the image to the voxel size of the model. For example, if the model has been trained with data at voxel size of <code>(0.1, 0.1, 0.1)um</code>, the user can rescale the image to this voxel size.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Reference model: Rescale to same voxel size as selected model.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To voxel size</code> mode, the user can rescale the image to a specific voxel size.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Out voxel size: Define the output voxel size. Units are same as imported, (if units are missing default is \"um\").</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To shape</code> mode, the user can rescale the image to a specific shape.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Out shape: Rescale to a manually selected shape.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>Set voxel size</code> mode, the user can set the voxel size of the image to a specific value. This only changes the metadata of the image and does not rescale the image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer to a new voxel size or shape.             <ul><li>Image or Label: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Out voxel size: Define the output voxel size. Units are same as imported, (if units are missing default is \"um\").</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p>"}]}