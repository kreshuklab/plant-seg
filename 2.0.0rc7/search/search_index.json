{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PlantSeg introduction","text":"<p>PlantSeg v2 is out!</p> <p>In this update, we switched to Napari as our front-end! You now can interactively work on your images in PlantSeg.</p> <p>PlantSeg is a tool for 3D and 2D segmentation. The methods used are very generic and can be used for any instance segmentation workflow, but they are tuned towards cell segmentation in plant tissue. The tool is fundamentally composed of two main steps.</p> Figure: PlantSeg Main Workflow <ul> <li> <p>Cell boundary prediction: A convolutional neural network (CNN) is utilized to perform voxel-wise boundary classification. This network is adept at filtering out diverse types and intensities of noise, homogenizing signal strength, and correcting imaging defects such as blurred or missing cell boundaries. This step ensures a high-quality boundary prediction which is crucial for accurate segmentation.</p> </li> <li> <p>Cell Segmentation as graph partitioning: The boundary prediction from the first step serve as the basis for automated segmentation. PlantSeg implements four distinct algorithms for this task, each with unique features tailored to different segmentation needs. This graph partitioning approach is particularly effective for segmenting densely packed cells.</p> </li> </ul> Figure: PlantSeg v2 Interface <p>For a detailed description of the methods employed in PlantSeg, please refer to our manuscript. If you find PlantSeg useful in your research, please consider citing our work:</p> <pre><code>@article{wolny2020accurate,\n  title={Accurate and versatile 3D segmentation of plant tissues at cellular resolution},\n  author={Wolny, Adrian and Cerrone, Lorenzo and Vijayan, Athul and Tofanelli, Rachele and Barro, Amaya Vilches and Louveaux, Marion and Wenzl, Christian and Strauss, S{\\\"o}ren and Wilson-S{\\'a}nchez, David and Lymbouridou, Rena and others},\n  journal={Elife},\n  volume={9},\n  pages={e57613},\n  year={2020},\n  publisher={eLife Sciences Publications Limited}\n}\n</code></pre>"},{"location":"chapters/getting_started/","title":"Quick Start","text":"<p>PlantSeg has two main modes of operation: interactively, using the Napari viewer, or for batch processing from the command line. The following sections will guide you through the installation and usage of PlantSeg in each of these modes.</p>"},{"location":"chapters/getting_started/#interactive-plantseg-with-napari-viewer","title":"Interactive PlantSeg with Napari Viewer","text":"<p>After installing PlantSeg using the installer, there should be a start menu entry to launch PlantSeg. Alternatively, launch <code>plantseg --napari</code> in the terminal.</p> <p>Using the GUI, you can</p> <ul> <li>load and view images</li> <li>apply pre- and post-processing steps and directly see the results</li> <li>perform segmentation</li> <li>interactively proofread segmentations</li> <li>save the processing history as repeatable workflows</li> <li>train custom boundary prediction models</li> </ul> <p>Details to the GUI workflow and its sections can be found in the GUI documentation.  </p>"},{"location":"chapters/getting_started/#run-batch-workflows","title":"Run batch workflows","text":"<p>PlanSeg can apply a set of operations (a workflow) to a batch of images.  </p> <p>To generate a batch workflow, you need to create a workflow <code>yaml</code> file. While you process a single image, all steps are recorded. Once you have saved the resulting image, you can save the recording as a workflow file to be applied to many images.</p> <p>Before you can run the batch workflow, you need to set the correct paths to the files you want to process, and where you want the results to be saved. PlantSeg provides an editor for this; start it from the <code>Output tab</code>, or by running:</p> <pre><code>plantseg --edit [workflow.yaml]\n</code></pre> <p>Once the correct input/output paths are set, you can run the workflow:</p> <pre><code>plantseg --config [workflow.yaml]\n</code></pre> <p>Learn more about Workflows here!</p>"},{"location":"chapters/getting_started/contributing/","title":"Contribute to PlantSeg","text":"<p>PlantSeg is an open-source project, and we welcome contributions from the community. There are many ways to contribute, such as writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests, or writing code that can be incorporated into PlantSeg itself.</p>"},{"location":"chapters/getting_started/contributing/#install-mamba","title":"Install Mamba","text":"<p>The easiest way to install PlantSeg is by using mamba (Miniforge) package manager. If you don't have conda already, install it:</p> LinuxWindows/macOS <p>To download Miniforge open a terminal and type:</p> <pre><code>curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <p>Then install by typing:</p> <pre><code>bash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <p>and follow the installation instructions. Please refer to the Miniforge repo for more information, troubleshooting and uninstallation instructions. The miniforge installation file <code>Miniforge3-*.sh</code> can be deleted now.</p> <p>The first step required to use the pipeline is installing mamba. The installation can be done by downloading the installer from the Miniforge repo. There you can find the download links for the latest version of Miniforge, troubleshooting and uninstallation instructions.</p>"},{"location":"chapters/getting_started/contributing/#getting-started","title":"Getting Started","text":"<p>To set up the development environment, run:</p> <pre><code>conda env create -f environment-dev.yaml\nconda activate plant-seg-dev\n</code></pre> <p>This installs PlantSeg using <code>pip --editable .</code> and all dependencies using conda. (Some dependencies are only available through conda-forge)</p>"},{"location":"chapters/getting_started/contributing/#hierarchical-design-of-plantseg","title":"Hierarchical Design of PlantSeg","text":"<p>Please refer to Python API.</p>"},{"location":"chapters/getting_started/contributing/#coding-style","title":"Coding Style","text":"<p>PlantSeg uses Ruff for linting and formatting. Ruff is compatible with Black for formatting.</p> <p>To ensure proper formatting and commit messages, pre-commit is used. This runs on PRs automatically, to run it locally check out Pre-commit.</p>"},{"location":"chapters/getting_started/contributing/#before-submitting-a-pull-request","title":"Before Submitting a Pull Request","text":""},{"location":"chapters/getting_started/contributing/#run-tests-with-pytest","title":"Run Tests with <code>pytest</code>","text":"<p>Ensure that <code>pytest</code> is installed in your conda environment. To run the tests, simply use:</p> <pre><code>pytest\n</code></pre>"},{"location":"chapters/getting_started/contributing/#check-syntax-with-pre-commit","title":"Check Syntax with <code>pre-commit</code>","text":"<p>The PlantSeg repository uses pre-commit hooks to ensure the code is correctly formatted and free of linting issues. While not mandatory, it is encouraged to check your code before committing by running:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>Commit messages are important. Please follow the Conventional Commits specification.</p> <p>For efficiency, pytest is not included in the pre-commit hooks. Please run the tests separately.</p>"},{"location":"chapters/getting_started/contributing/#videos","title":"Videos","text":"<p>Video encoding settings for this website:</p> <pre><code>ffmpeg -i input.webm -vcodec libx264 -r 20 -crf 28 -tune animation -vf \"scale=-2:'min(1080,ih)'\" output_20fps.mp4\n</code></pre>"},{"location":"chapters/getting_started/installation/","title":"Installation","text":"<p>This is the installation guide for the latest PlantSeg. Please check the installation guide for PlantSeg v1 at PlantSeg Legacy Installation.</p>"},{"location":"chapters/getting_started/installation/#download","title":"Download","text":"<p>Download the installer from heibox and choose according to your platform.</p> <p> Download</p>"},{"location":"chapters/getting_started/installation/#installation_1","title":"Installation","text":"<p>The installer comes complete with its own python installation. During the installation further dependencies need to be downloaded.</p> <p>First Start</p> <p>The first start can be quite slow, as the machine learning models need to be downloaded.</p> Linux and MacOsWindowsconda-forgelatest git version <p>Download the installer, then open a terminal and run the installer:</p> <pre><code>bash PlantSeg*.sh\n</code></pre> <p>On linux, you can start PlantSeg through your start menu.</p> <p>To start PlantSeg in the terminal, navigate to your installation directory (default <code>~/plantseg/</code>) and run <code>bin/plantseg --napari</code>.</p> <p>You might want to add a link to this file to some directory on your $PATH.</p> <p>Alternatively, you can activate the plantseg conda environment: (Replace [INSTALLATION DIR] with the absolute(!) path to your installation)</p> <pre><code>eval \"$(\"[INSTALLATION DIR]/bin/conda\" shell.bash activate \"[INSTALLATION DIR]\")\nplantseg --help\n</code></pre> <p>Download the installer and execute it. As the installer is not signed you  might need to confirm the download and that you want to run the installer.</p> <p>Choose a installation path without spaces in it, as those can cause issues with conda packages.</p> <p>Start PlanSeg through the Windows start menu.</p> <p>If you want to install PlantSeg without the installer, you need to have conda. (We recommend Microforge to get conda, see installing mamba)</p> <pre><code>conda create --name plant-seg plant-seg\nconda activate plant-seg\nplantseg --napari\n</code></pre> <p>To get the latest pre-release features, install PlantSeg from git. You need to have conda and git installed. (We recommend Microforge to get conda, see installing mamba)</p> <pre><code>git clone https://github.com/kreshuklab/plant-seg.git\ncd plant-seg\nconda env create -f environment.yaml\nconda activate plant-seg\nplantseg --napari\n</code></pre> <p>For development, we recommend using the <code>environment-dev.yaml</code> instead!  Also check the contributing section</p>"},{"location":"chapters/getting_started/installation/#updating","title":"Updating","text":"<p>Info</p> <p>Due to an external change, this only works from version 2.0.0rc5 onward. If you are running an older version, please uninstall and reinstall PlantSeg.</p> <p>If you have installed PlantSeg via the installer or from Conda-forge, you can update to a new version right in the GUI! Go to the <code>Plugins</code> menu on top, then click <code>Update Plantset</code>!</p> <p>If you have cloned the git repository, you need to update your local repo:</p> <pre><code>cd plant-seg\ngit pull\nconda env update -f environment.yaml # environment-dev.yaml for development\n</code></pre>"},{"location":"chapters/getting_started/installation/#optional-dependencies","title":"Optional dependencies","text":"<p>Certain compressed TIFF files (e.g., Zlib, ZSTD, LZMA formats) require additional codecs to be processed correctly by PlantSeg. To handle such files, install the <code>imagecodecs</code> package:</p> <pre><code>conda activate plant-seg\npip install imagecodecs\n</code></pre> <p>If you plan to use SimpleITK-based watershed segmentation, you will need to install <code>SimpleITK</code> as an additional dependency:</p> <pre><code>conda activate plant-seg\npip install SimpleITK\n</code></pre>"},{"location":"chapters/getting_started/installation/#installing-plantseg-v1","title":"Installing PlantSeg v1","text":"<p>Please check the installation guide for PlantSeg v1 at PlantSeg Legacy Installation.</p>"},{"location":"chapters/getting_started/known_issues/","title":"Known Issues","text":""},{"location":"chapters/getting_started/known_issues/#boundary-prediction-not-starting","title":"Boundary prediction not starting","text":"<p>Relevant for:</p> <ul> <li>Windows</li> <li>nvidia GPU</li> </ul> <p>On Windows using a nvidia GPUs PlantSeg might appear stuck running the prediction. If this error occurs, check the GPU memory in the task manager, it is probably full.</p> <p>GPUs can use the system memory after running out of memory on the GPU itself. This prevents crashes and errors, but makes all operations very slow. In PlanSeg, this \"feature\" prevents us from finding optimal patch and batch sizes for your GPU. Here is how you can fix the issue:</p>"},{"location":"chapters/getting_started/known_issues/#a-quick-and-dirty-fix-set-patch-and-batch-size-manually","title":"A) Quick and dirty fix: Set patch and batch size manually","text":"<p>In <code>Segmentation</code>, <code>Boundary Prediction</code>, click on <code>Show advanced parameters</code>. Then check the box <code>Manual Patch Size</code> and enter your patch size (min 64x64 in x,y dimension). Set <code>Batch size</code> to <code>One (low VRAM usage)</code>.</p>"},{"location":"chapters/getting_started/known_issues/#b-proper-fix-disable-memory-off-loading","title":"B) Proper fix: Disable memory off-loading","text":"<p>Open your nvidia control panel. Navigate to <code>3D-Settings</code>(1), <code>Program specific</code> (not global)(2). Click on <code>add</code> and choose Python, afterwards it should display the path to the python executable in you PlantSeg installation(3).</p> <p>Scroll to <code>CUDA - Sysmem Fallback Policy</code> and change that to <code>Prefer No Sysmem Fallback</code>(4).</p> <p></p>"},{"location":"chapters/plantseg_interactive_napari/","title":"PlantSeg Interactive - Napari","text":"<p>Interactively apply operations to your images and run the segmentation pipeline using the GUI of PlantSeg. PlantSeg uses napari as its front-end.</p> <p>Start PlantSeg according to your chosen installation method. From the terminal, you can start PlantSeg in the GUI mode with:</p> <pre><code>plantseg --napari\nor\nplantseg -n\n</code></pre> <p></p>"},{"location":"chapters/plantseg_interactive_napari/#parts-of-the-gui","title":"Parts of the GUI","text":""},{"location":"chapters/plantseg_interactive_napari/#top-left-settings","title":"Top left: Settings","text":"<ul> <li>Change your tool (pan, brush, boxselect, ...)</li> <li>Change look of image (opacity, label colors,..)</li> <li>Change tool settings (brush color &amp; size, select shape)</li> </ul>"},{"location":"chapters/plantseg_interactive_napari/#bottom-left-layer-selection","title":"Bottom left: Layer selection","text":"<p>Your loaded layers get displayed here. Toggle their visibility with the eye icon. Below, you can switch to a grid view or a 3D view with the respective icons.</p>"},{"location":"chapters/plantseg_interactive_napari/#right-plantseg","title":"Right: PlantSeg","text":"<p>This is the main interface of PlantSeg. On the bottom you can switch through the different sections of PlantSeg.</p>"},{"location":"chapters/plantseg_interactive_napari/#hotkeys","title":"Hotkeys","text":""},{"location":"chapters/plantseg_interactive_napari/#general","title":"General","text":"<ul> <li><code>Ctrl+i</code> Increase font size</li> <li><code>Ctrl+o</code> Decrease font size</li> <li><code>Ctrl+w</code> Quit PlantSeg</li> </ul>"},{"location":"chapters/plantseg_interactive_napari/#proofreading","title":"Proofreading","text":"<ul> <li><code>n</code> Merge/Split using current scribbles</li> <li><code>j</code> Clean scribbles</li> </ul>"},{"location":"chapters/plantseg_interactive_napari/batch/","title":"Batch processing","text":"<p>Plantseg supports processing a whole stack of images at once through the use of batch scripts.</p> <p>To generate such a workflow script, first process one image in the intended way, and export the result. Then, go to the <code>Batch</code> tab in Napari and save the workflow script to a <code>yaml</code> file.</p> <p>You can review and edit workflow scripts using the build-in workflow editor; open it either directly from the napari <code>Batch</code> tab, or from the command line using <code>plantseg -e</code> .</p> <p>Read more about batch workflows in this chapter.</p> <p>You find the batch processing section in the output tab in napari!</p> <p>Warning</p> <p>Some interactive steps can't be recorded into workflows, especially cropping and proofreading</p>"},{"location":"chapters/plantseg_interactive_napari/batch/#widget-export-batch-workflow","title":"Widget: Export Batch Workflow","text":"<p> <p></p> <p></p>                      Save the workflow as a yaml file             <ul><li>Export directory: Select the directory where the workflow will be exported</li> <li>Workflow name: Name of the exported workflow file.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/import/","title":"Import Images","text":""},{"location":"chapters/plantseg_interactive_napari/import/#widget-open-files","title":"Widget: Open Files","text":"<p> <p></p> <p></p>                      Open a file and return a napari layer.             <ul><li>File type: None</li> <li>File path: Select a file to be imported, the file can be a tiff, h5, png, jpg.</li> <li>Layer name: Define the name of the output layer, default is either image or label.</li> <li>Layer type: Select the type of input you are loading.</li> <li>Stack layout: Stack layout</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/output/","title":"Output","text":"<p>Using the Output tab you can save your work as <code>tiff</code>, <code>h5</code>, or <code>zarr</code>.</p> <p>Once you have exported an image, you can create a workflow file to repeat the processing steps you have performed on image batches.</p>"},{"location":"chapters/plantseg_interactive_napari/output/#widget-output","title":"Widget: Output","text":"<p> <p></p> <p></p>                      Export layers in various formats.             <ul><li>Layer to export: Select all layer to be exported, and (optional) set a custom file name suffix that will be appended at end of the layer name.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/postprocessing/","title":"Post Processing","text":""},{"location":"chapters/plantseg_interactive_napari/postprocessing/#widget-relabel-instances","title":"Widget: Relabel Instances","text":"<p> <p></p> <p></p>                      Relabel an image layer.             <ul><li>Background Label: Background label will be set to 0. Default is None.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/postprocessing/#widget-set-biggest-instance-to-zero","title":"Widget: Set biggest Instance to Zero","text":"<p> <p></p> <p></p>                      Set the biggest instance to zero in a label layer.             <ul><li>Treat 0 as Instance: If ticked, a proper instance segmentation with 0 as background will not be modified.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/postprocessing/#widget-remove-false-positives-by-foreground","title":"Widget: Remove False-Positives by Foreground","text":"<p> <p></p> <p></p>                      Remove false positives from a segmentation layer using a foreground probability layer.             <ul><li>Foreground: Foreground probability layer.</li> <li>Threshold: Threshold value to remove false positives.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/postprocessing/#widget-splitmerge-instances-by-nuclei","title":"Widget: Split/Merge Instances by Nuclei","text":"<p> <p></p> <p></p>                               Widget for correcting over- and under-segmentation of cells based on nuclei segmentation.          This widget allows users to adjust cell segmentation by leveraging nuclei segmentation. It supports         merging over-segmented cells and splitting under-segmented cells, with optional boundary refinement.          Args:             segmentation_cells (Labels): Input layer representing segmented cell instances.             segmentation_nuclei (Labels): Input layer representing segmented nuclei instances.             boundary_pmaps (Image | None, optional): Optional boundary probability map (same shape as input layers).                 Higher values indicate probable cell boundaries, used to refine segmentation.             threshold (tuple[float, float], optional): Merge and split thresholds as percentages (0-100).                 - The first value is the merge threshold: cells with nuclei overlap below this value are merged.                 - The second value is the split threshold: cells with nuclei overlap above this value are split.                 Default is (33, 66).             quantile (tuple[float, float], optional): Minimum and maximum quantile values for filtering nuclei sizes (0-100).                 - The first value excludes the smallest nuclei (e.g., \"0.3\" excludes the smallest 0.3%).                 - The second value excludes the largest nuclei (e.g., \"99.9\" excludes the largest 0.1%).                 Default is (0.3, 99.9).          Returns:             None                      <ul><li>Cell Segmentation: Input layer representing segmented cell instances.</li> <li>Nuclear Segmentation: Input layer representing segmented nuclei instances.</li> <li>Boundary image: Optional boundary probability map (same shape as input layers). Higher values indicate probable cell boundaries, used to refine segmentation.</li> <li>Boundary Threshold (%): Set the percentage range for merging (first value) and splitting (second value) cells. For example, \"33\" means cells with less than 33% overlap with nuclei are merged, and \"66\" means cells with more than 66% overlap are split.</li> <li>Nuclei Size Filter (%): Set the size range to filter nuclei, represented as percentages. For example, \"0.3\" excludes the smallest 30%, and \"99.9\" excludes the largest 0.1% of nuclei.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/preprocessing/","title":"Preprocessing","text":"<p>This section describes the data processing functionalities available in PlantSeg Interactive. This functionalities are available in the <code>Preprocessing</code> tab in the PlantSeg Interactive GUI.</p>"},{"location":"chapters/plantseg_interactive_napari/preprocessing/#widget-crop","title":"Widget: Crop","text":"<p>To crop an image, first add a <code>Shapes</code> layer. You find the <code>Add Shapes Layer</code> button above the layer list on the left side. With the <code>Shapes</code> layer selected, you can add one rectangle using by pressing <code>R</code> and drawing.</p> <p>In the Crop widget, you can also crop on the z dimension if applicable.</p> <p> <p></p> <p></p> <ul><li>Crop shapes layer: This must be a shape layer with a rectangle overlaying the area to crop.</li> <li>Z slices [start, end): Number of z slices to take next to the current selection. START is included, END is not.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/preprocessing/#widget-image-rescaling","title":"Widget: Image Rescaling","text":"<p>Many boundary prediction models are sensitive to the scale of the input. Use the <code>To model voxel size</code> option to match your input to your desired model.</p> From factorTo layer voxel sizeTo layer shapeTo model voxel sizeTo voxel sizeTo shapeSet voxel size <p>Using the <code>From factor</code> mode, the user can rescale the image by a multiplicate factor. For example, if the image has a shape <code>(10, 10, 10)</code> and the user wants to rescale it by a factor of <code>(2, 2, 2)</code>, the new size will be <code>(20, 20, 20)</code>.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p> <p>Using the <code>To layer voxel size</code> mode, the user can rescale the image to the voxel size of a specific layer. For example, if two images are loaded in the viewer, one with a voxel size of <code>(0.1, 0.1, 0.1)um</code> and the other with a voxel size of <code>(0.1, 0.05, 0.05)um</code>, the user can rescale the first image to the voxel size of the second image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p> <p>Using the <code>To layer shape</code> mode, the user can rescale the image to the shape of a specific layer. For example, if two images are loaded in the viewer, one with a shape <code>(10, 10, 10)</code> and the other with a shape <code>(20, 20, 20)</code>, the user can rescale the first image to the shape of the second image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p> <p>Using the <code>To model voxel size</code> mode, the user can rescale the image to the voxel size of the model. For example, if the model has been trained with data at voxel size of <code>(0.1, 0.1, 0.1)um</code>, the user can rescale the image to this voxel size.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p> <p>Using the <code>To voxel size</code> mode, the user can rescale the image to a specific voxel size.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p> <p>Using the <code>To shape</code> mode, the user can rescale the image to a specific shape.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p> <p>Using the <code>Set voxel size</code> mode, the user can set the voxel size of the image to a specific value. This only changes the metadata of the image and does not rescale the image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p>"},{"location":"chapters/plantseg_interactive_napari/preprocessing/#widget-gaussian-smoothing","title":"Widget: Gaussian Smoothing","text":"<p> <p></p> <p></p>                      Apply Gaussian smoothing to an image layer.             <ul><li>Sigma: Define the size of the gaussian smoothing kernel. The larger the more blurred will be the output image.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/preprocessing/#widget-image-pair-operations","title":"Widget: Image Pair Operations","text":"<p> <p></p> <p></p>                      Apply an operation to two image layers.             <ul><li>Layer 1: First image to apply the operation.</li> <li>Layer 2: Second image to apply the operation.</li> <li>Operation: None</li> <li>Normalize input: Normalize the input images to the range [0, 1].</li> <li>Clip output: Clip the output to the range [0, 1].</li> <li>Normalize output: Normalize the output image to the range [0, 1].</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/proofreading/","title":"Manual Proofreading","text":""},{"location":"chapters/plantseg_interactive_napari/proofreading/#widget-proofreading-with-manual-split-and-merge","title":"Widget: Proofreading with Manual Split and Merge","text":"<p>Once the segmentation is done, or a label image is loaded along with a boundary image, the user can start the proofreading process. In the proofreading widget, set the \"Mode\" to \"Layer\" if you are going to correct a loaded image, or set the \"Mode\" to \"File\" if you already had a proofreading session exported as a file. For \"Segmentation\", select the image layer that you want to correct, then click \"Initialise\" to start the proofreading process.</p> <p>After a few second of initialisation, the initialised proofreading widget will be displayed, and two new layers will be added to the Napari viewer, \"Scribbles\" and \"Correct Labels\". Click \"Scribbles\" first, and you may draw on this layer with the \"paint brush\" tool - you can press <code>2</code> or <code>P</code> key to activate the brush tool or find it on the \"layer controls\" panel of the Napari viewer. The scribbles drawn on this layer will be used to split or merge the segmentation: strokes with the same color will merge the instance(s) in the segmentation layer, and strokes with different colors will split the instance(s) in the segmentation layer. After drawing the scribbles, click \"Split / Merge\" or press <code>n</code> to apply the scribbles to the segmentation layer. If you want to undo the last split or merge operation, click \"Undo Last Action\".</p> <p>When you are done or want to stop the proofreading session, you may save the current state of the proofreading session by clicking \"Save current proofreading snapshot\" into a snapshot file, which can be loaded into the proofreading widget later in another session of PlantSeg.</p>"},{"location":"chapters/plantseg_interactive_napari/proofreading/#keybinding","title":"Keybinding","text":"<ul> <li><code>s</code>: Save stack</li> <li><code>n</code>: Merge or split from seeds</li> <li><code>ctrl+n</code>: Undo merge or split from seeds</li> <li><code>c</code>: Clean seeds</li> <li><code>o</code>: Mark/un-mark correct segmentation</li> <li><code>b</code>: show/un-show correct segmentation layer</li> <li><code>j</code>: Update boundaries from segmentation</li> <li><code>k</code>: Update segmentation from boundaries</li> <li><code>ctrl + arrows</code>: to move the field of view</li> <li><code>alt + down/up arrows</code>: to increase or decrease the field of view</li> </ul>"},{"location":"chapters/plantseg_interactive_napari/segmentation/","title":"Segmentation","text":"<p>The segmentation workflow consists of three main steps:</p> <ul> <li>Boundary Prediction</li> <li>Boundary to Superpixels</li> <li>Superpixels to Segmentation</li> </ul>"},{"location":"chapters/plantseg_interactive_napari/segmentation/#step-1-boundary-predictions","title":"Step 1: Boundary Predictions","text":"<p>Choose one of the build-in PlantSeg models, or one from the BioImage.IO Model Zoo.</p> <p>Alternatively, you can import your own model by choosing <code>ADD CUSTOM MODEL</code> from the model selection drop-down menu.</p> PlantSeg ZooBioImage.IO Model Zoo <p> <p></p> <p></p> </p> <p> <p></p> <p></p> </p>"},{"location":"chapters/plantseg_interactive_napari/segmentation/#step-2-boundary-to-superpixels","title":"Step 2: Boundary to Superpixels","text":"<p>Here, the boundary prediction is turned into superpixels by using distance transform watershed.</p> <p> <p></p> <p></p>                      None             <ul><li>Mode: Define if the Watershed will run slice by slice (faster) or on the full volume (slower).</li> <li>Boundary threshold: A low value will increase over-segmentation tendency and a large value increase under-segmentation tendency.</li> <li>Minimum superpixel size: Minimum superpixel size allowed in voxels.</li> <li>Show advanced parameters: Show advanced parameters for the Watershed algorithm.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/segmentation/#step-3-superpixels-to-segmentation","title":"Step 3: Superpixels to Segmentation","text":"<p>DT Watershed tends to over-segment the image, therefor an agglomeration algorithm is used in this third step.</p> <p> <p></p> <p></p> <ul><li>Agglomeration mode: Select which agglomeration algorithm to use.</li> <li>Under/Over segmentation factor: A low value will increase under-segmentation tendency and a large value increase over-segmentation tendency.</li> <li>Minimum segment size: Minimum segment size allowed in voxels.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/unet_training/","title":"UNet Training","text":"<p>With PlantSeg you can train bespoke segmentation models! This is especially useful to get great results on a big dataset for which the build-in models work not perfectly. First proofread some images from the dataset. Then train a new model on this high-quality data, and run it on the whole dataset.</p> <p>You can also fine-tune existing models on your data.</p>"},{"location":"chapters/plantseg_interactive_napari/unet_training/#training-from-a-dataset","title":"Training from a dataset","text":"<p>For training from an dataset stored on disk, create the directories <code>train</code> and <code>val</code>. Your training images must be stored as <code>h5</code> files in these directories. The <code>h5</code> files must contain the input image under the <code>raw</code> key, and the segmentation under the <code>label</code> key.</p> <pre><code>mydataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 first.h5\n\u2502   \u2514\u2500\u2500 second.h5\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 val_one.h5\n    \u2514\u2500\u2500 val_two.h5 \n</code></pre>"},{"location":"chapters/plantseg_interactive_napari/unet_training/#train-from-gui","title":"Train from GUI","text":"<p>To train from images loaded in the GUI, you need a layer containing the input image and one containing the segmentation. Make sure the quality of the segmentation is as good as possible by using the proofreading tool.</p>"},{"location":"chapters/plantseg_interactive_napari/unet_training/#widgets","title":"Widgets","text":"<p> <p></p> <p></p> <ul><li>Train from: Whether to train from a dataset on disk or from loaded layers in Napari.</li> <li>Dataset: Dataset directory. It must contain a `train` and a `val` directory, each containing h5 files. Input/Output keys must be `raw` and `label` respectively.</li> <li>Pretrained model: Optionally select an existing model to retrain. Hover over the name to show the model description. Leave empty to create a new model.</li> <li>Model name: How your new model should be called. Can't be the name of an existing model.</li> <li>Description: Model description will be saved alongside the model.</li> <li>In and Out Channels: Number of input and output channels</li> <li>Feature dimensions: Number of feature maps at each level of the encoder. If it's one number, the number of feature maps is given by the geometric progression: f_maps ^ k, k=1,2,3,4 Can't be modified for pretrained models.</li> <li>Patch size: None</li> <li>Data Resolution: Voxel size in um of the training data. Is initialized correctly from the chosen data if possible.</li> <li>Max iterations: Maximum number of iterations after which the training will be stopped. Stops earlier if the accuracy converges.</li> <li>Microscopy modality: Modality of the model (e.g. confocal, light-sheet ...).</li> <li>Prediction type: Type of prediction (e.g. cell boundaries prediction or nuclei...).</li> <li>Device: None</li> </ul> </p>"},{"location":"chapters/plantseg_legacy/installation/","title":"Installation","text":""},{"location":"chapters/plantseg_legacy/installation/#prerequisites-for-conda-package","title":"Prerequisites for Conda package","text":"<ul> <li>Linux, Windows, macOS (not all features are available on macOS)</li> <li>(Optional) Nvidia GPU with official Nvidia drivers installed for GPU acceleration</li> </ul>"},{"location":"chapters/plantseg_legacy/installation/#install-mamba","title":"Install Mamba","text":"<p>The easiest way to install PlantSeg is by using the conda (Anaconda) or mamba (Miniforge) package manager. We recommend using <code>mamba</code> because it is faster and usually more consistent than <code>conda</code>.</p> LinuxWindows/macOS <p>To download Miniforge open a terminal and type:</p> <pre><code>curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <p>Then install by typing:</p> <pre><code>bash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <p>and follow the installation instructions. Please refer to the Miniforge repo for more information, troubleshooting and uninstallation instructions. The miniforge installation file <code>Miniforge3-*.sh</code> can be deleted now.</p> <p>The first step required to use the pipeline is installing mamba. The installation can be done by downloading the installer from the Miniforge repo. There you can find the download links for the latest version of Miniforge, troubleshooting and uninstallation instructions.</p>"},{"location":"chapters/plantseg_legacy/installation/#install-plantseg-using-mamba","title":"Install PlantSeg using Mamba","text":"<p>PlantSeg can be installed directly by executing in the terminal (or PowerShell on Windows). For <code>conda</code> users, the command is identical, just replace <code>mamba</code> with <code>conda</code>.</p> LinuxWindowsmacOS <ul> <li> <p>NVIDIA GPU version, CUDA=12.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=12.1 plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> <li> <p>NVIDIA GPU version, CUDA=11.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=11.8 plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> <li> <p>CPU version</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch cpuonly plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> </ul> <ul> <li> <p>NVIDIA GPU version, CUDA=12.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=12.1 nifty=1.2.1=*_4 plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> <li> <p>NVIDIA GPU version, CUDA=11.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=11.8 nifty=1.2.1=*_4 plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> <li> <p>CPU version</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch cpuonly nifty=1.2.1=*_4 plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> </ul> <ul> <li> <p>Apple silicon version</p> <pre><code>mamba create -n plant-seg -c pytorch -c conda-forge python=3.11 pytorch::pytorch plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> </ul> <p>If you used older versions of PlantSeg, please delete the old config files in <code>~/.plantseg_models/configs/</code> after installing new PlantSeg.</p> <p>The above command will create new conda environment <code>plant-seg</code> together with all required dependencies.</p> <p>Please refer to the PyTorch website for more information on the available versions of PyTorch and the required CUDA version. The GPU version of Pytorch will also work on CPU only machines but has a much larger installation on disk.</p>"},{"location":"chapters/plantseg_legacy/installation/#optional-dependencies","title":"Optional dependencies","text":"<p>Certain compressed TIFF files (e.g., Zlib, ZSTD, LZMA formats) require additional codecs to be processed correctly by PlantSeg. To handle such files, install the <code>imagecodecs</code> package:</p> <pre><code>conda activate plant-seg\npip install imagecodecs\n</code></pre> <p>If you plan to use SimpleITK-based watershed segmentation, you will need to install <code>SimpleITK</code> as an additional dependency:</p> <pre><code>conda activate plant-seg\npip install SimpleITK\n</code></pre>"},{"location":"chapters/plantseg_legacy/troubleshooting/","title":"Troubleshooting","text":"<p>This section provides solutions to common issues you might encounter while using PlantSeg. Click on a problem to jump to its specific solution.</p> <ul> <li>Font size problems in GUI</li> <li>Problems with <code>--headless</code> and <code>dask[distributed]</code></li> <li>Could not load library <code>libcudnn_ops_infer.so.8</code></li> <li>Missing configuration key errors</li> <li>Cannot import <code>lifted_problem_from_probabilities</code></li> <li>Other issues</li> </ul>"},{"location":"chapters/plantseg_legacy/troubleshooting/#font-size-problems-in-gui","title":"Font size problems in GUI","text":"<p>If you find the font size varies within either Napari or Legacy GUIs, or some buttons or texts are not visible, it might relate to your system's DPI settings or screen resolution. To fix this, you can try to reset the resolution of your system.</p> <p>Related discussions:</p> <ul> <li><code>plantseg --gui</code>, no buttons to begin the workflow #241</li> </ul> <p></p> <p>Other references:</p> <ul> <li>tkinter not recognizing screen resolution correctly</li> <li>High DPI Desktop Application Development on Windows</li> <li>SetProcessDpiAwareness function (shellscalingapi.h)</li> </ul>"},{"location":"chapters/plantseg_legacy/troubleshooting/#problems-with-headless-and-daskdistributed","title":"Problems with <code>--headless</code> and <code>dask[distributed]</code>","text":"<p>If you encounter the following error:</p> <pre><code>ImportError: dask.distributed is not installed.\n</code></pre> <p>Please install <code>dask[distributed]</code> to enable headless mode in PlantSeg. Run the following commands in your terminal:</p> <pre><code>mamba activate plant-seg\nmamba install -c pytorch -c nvidia -c conda-forge dask distributed\n</code></pre>"},{"location":"chapters/plantseg_legacy/troubleshooting/#could-not-load-library-libcudnn_ops_inferso8","title":"Could not load library <code>libcudnn_ops_infer.so.8</code>","text":"<p>If you encounter this error:</p> <pre><code>Could not load library libcudnn_ops_infer.so.8. Error: libcudnn_ops_infer.so.8: cannot open shared object file: No such file or directory\n</code></pre> <p>Resolve this by installing <code>cudnn</code> using the following command:</p> <pre><code>mamba install -c conda-forge cudnn\n</code></pre>"},{"location":"chapters/plantseg_legacy/troubleshooting/#missing-configuration-key-errors","title":"Missing configuration key errors","text":"<p>If you encounter a <code>RuntimeError</code> about a missing key, such as:</p> <pre><code>RuntimeError: key : 'crop_volume' is missing, plant-seg requires 'crop_volume' to run\n</code></pre> <p>This usually means the session configuration file is corrupted or outdated. To fix this:</p> <pre><code>rm ~/.plantseg_models/configs/config_gui_last.yaml\n</code></pre> <p>Ensure your configuration file is properly formatted and includes all required keys. Example configurations can be found in the <code>examples</code> directory of this repository.</p>"},{"location":"chapters/plantseg_legacy/troubleshooting/#cannot-import-lifted_problem_from_probabilities","title":"Cannot import <code>lifted_problem_from_probabilities</code>","text":"<p>If you receive an error related to importing from <code>elf.segmentation.features</code>, reinstall elf:</p> <pre><code>conda install -c conda-forge python-elf\n</code></pre>"},{"location":"chapters/plantseg_legacy/troubleshooting/#other-issues","title":"Other issues","text":"<p>PlantSeg is actively developed, and sometimes model or configuration files saved in <code>~/.plantseg_models</code> may become outdated. If you encounter errors related to configuration loading:</p> <ol> <li>Close the PlantSeg application.</li> <li>Delete the <code>~/.plantsep_models</code> directory.</li> <li>Restart the application and try again.</li> </ol> <p>These steps should help resolve any issues and enhance your experience with PlantSeg.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/","title":"PlantSeg Classic CLI","text":"<p>Deprecated</p> <p>This interface is deprecated and has been removed from PlantSeg v2. Please use the Napari viewer or the command line interface instead, or install PlantSeg v1.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#guide-to-custom-configuration-file","title":"Guide to Custom Configuration File","text":"<p>The configuration file defines all the operations in our pipeline together with the data to be processed. Please refer to config.yaml for a sample pipeline configuration and a detailed explanation of all parameters.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#main-keyssteps","title":"Main Keys/Steps","text":"<ul> <li><code>path</code> attribute: is used to define either the file to process or the directory containing the data.</li> <li><code>preprocessing</code> attribute: contains a simple set of possible operations one would need to run on their data before calling the neural network. This step can be skipped if data is ready for neural network processing. Detailed instructions can be found at Classic GUI (Data Processing).</li> <li><code>cnn_prediction</code> attribute: contains all parameters relevant for predicting with a neural network. Description of all pre-trained models provided with the package is described below. Detailed instructions can be found at Classic GUI (Prediction).</li> <li><code>segmentation</code> attribute: contains all parameters needed to run the partitioning algorithm (i.e., final Segmentation). Detailed instructions can be found at Classic GUI (Segmentation).</li> </ul>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#additional-information","title":"Additional information","text":"<p>The PlantSeg-related files (models, configs) will be placed inside your home directory under <code>~/.plantseg_models</code>.</p> <p>Our pipeline uses the PyTorch library for CNN prediction. PlantSeg can be run on systems without GPU, however for maximum performance, we recommend that the application is run on a machine with a high-performance GPU for deep learning. If the <code>CUDA_VISIBLE_DEVICES</code> environment variable is not specified, the prediction task will be distributed on all available GPUs. E.g. run: <code>CUDA_VISIBLE_DEVICES=0 plantseg --config CONFIG_PATH</code> to restrict prediction to a given GPU.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#configuration-file-example","title":"configuration file example","text":"<p>This modality of using PlantSeg is particularly suited for high throughput processing and for running PlantSeg on a remote server. To use PlantSeg from command line mode, you will need to create a configuration file using a standard text editor  or using the save option of the PlantSeg GUI.</p> <p>Here is an example configuration:</p> <pre><code>path: /home/USERNAME/DATA.tiff # Contains the path to the directory or file to process\n\npreprocessing:\n  # enable/disable preprocessing\n  state: True\n  # create a new sub folder where all results will be stored\n  save_directory: \"PreProcessing\"\n  # rescaling the volume is essential for the generalization of the networks. The rescaling factor can be computed as the resolution\n  # of the volume at hand divided by the resolution of the dataset used in training. Be careful, if the difference is too large check for a different model.\n  factor: [1.0, 1.0, 1.0]\n  # the order of the spline interpolation\n  order: 2\n  # optional: perform Gaussian smoothing or median filtering on the input.\n  filter:\n    # enable/disable filtering\n    state: False\n    # Accepted values: 'gaussian'/'median'\n    type: gaussian\n    # sigma (gaussian) or disc radius (median)\n    param: 1.0\n\ncnn_prediction:\n  # enable/disable UNet prediction\n  state: True\n  # Trained model name, more info on available models and custom models in the README\n  model_name: \"generic_confocal_3D_unet\"\n  # If a CUDA capable gpu is available and corrected setup use \"cuda\", if not you can use \"cpu\" for cpu only inference (slower)\n  device: \"cpu\"\n  # how many subprocesses to use for data loading\n  num_workers: 8\n  # patch size given to the network (adapt to fit in your GPU mem)\n  patch: [32, 128, 128]\n  # stride between patches will be computed as `stride_ratio * patch`\n  # recommended values are in range `[0.5, 0.75]` to make sure the patches have enough overlap to get smooth prediction maps\n  stride_ratio: 0.75\n  # If \"True\" forces downloading networks from the online repos\n  model_update: False\n\ncnn_postprocessing:\n  # enable/disable cnn post processing\n  state: False\n  # if True convert to result to tiff\n  tiff: False\n  # rescaling factor\n  factor: [1, 1, 1]\n  # spline order for rescaling\n  order: 2\n\nsegmentation:\n  # enable/disable segmentation\n  state: True\n  # Name of the algorithm to use for inferences. Options: MultiCut, MutexWS, GASP, DtWatershed\n  name: \"MultiCut\"\n  # Segmentation specific parameters here\n  # balance under-/over-segmentation; 0 - aim for undersegmentation, 1 - aim for oversegmentation. (Not active for DtWatershed)\n  beta: 0.5\n  # directory where to save the results\n  save_directory: \"MultiCut\"\n  # enable/disable watershed\n  run_ws: True\n  # use 2D instead of 3D watershed\n  ws_2D: True\n  # probability maps threshold\n  ws_threshold: 0.5\n  # set the minimum superpixels size\n  ws_minsize: 50\n  # sigma for the gaussian smoothing of the distance transform\n  ws_sigma: 2.0\n  # sigma for the gaussian smoothing of boundary\n  ws_w_sigma: 0\n  # set the minimum segment size in the final segmentation. (Not active for DtWatershed)\n  post_minsize: 50\n\nsegmentation_postprocessing:\n  # enable/disable segmentation post processing\n  state: False\n  # if True convert to result to tiff\n  tiff: False\n  # rescaling factor\n  factor: [1, 1, 1]\n  # spline order for rescaling (keep 0 for segmentation post processing\n  order: 0\n</code></pre> <p>This configuration can be found at config.yaml.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#pipeline-usage-command-line","title":"Pipeline Usage (command line)","text":"<p>To start PlantSeg from the command line. First, activate the newly created conda environment with:</p> <pre><code>conda activate plant-seg\n</code></pre> <p>then, one can just start the pipeline with</p> <pre><code>plantseg --config CONFIG_PATH\n</code></pre> <p>where <code>CONFIG_PATH</code> is the path to a YAML configuration file.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#data-parallelism","title":"Data Parallelism","text":"<p>In the headless mode (i.e. when invoked with <code>plantseg --config CONFIG_PATH</code>) the prediction step will run on all the GPUs using DataParallel. If prediction on all available GPUs is not desirable, restrict the number of GPUs using <code>CUDA_VISIBLE_DEVICES</code>, e.g.</p> <pre><code>CUDA_VISIBLE_DEVICES=0,1 plantseg --config CONFIG_PATH\n</code></pre>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#results","title":"Results","text":"<p>The results are stored together with the source input files inside a nested directory structure. As an example, if we want to run PlantSeg inside a directory with two stacks, we will obtain the following outputs:</p> <pre><code>/file1.tif\n/file2.tif\n/PreProcesing/\n------------&gt;/file1.h5\n------------&gt;/file1.yaml\n------------&gt;/file2.h5\n------------&gt;/file2.yaml\n------------&gt;/generic_confocal_3d_unet/\n-------------------------------------&gt;/file1_prediction.h5\n-------------------------------------&gt;/file1_prediction.yaml\n-------------------------------------&gt;/file2_prediction.h5\n-------------------------------------&gt;/file2_prediction.yaml\n-------------------------------------&gt;/GASP/\n------------------------------------------&gt;/file_1_predions_gasp_average.h5\n------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n------------------------------------------&gt;/file_2_predions_gasp_average.h5\n------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n------------------------------------------&gt;/PostProcessing/\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n</code></pre> <p>The use of this hierarchical directory structure allows PlantSeg to find the necessary files quickly and can be used to test different segmentation algorithms/parameter combinations minimizing the memory overhead on the disk. For the sake of reproducibility, every file is associated with a configuration file \".yaml\" that saves all parameters used to produce the result.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#liftedmulticut-segmentation","title":"LiftedMulticut segmentation","text":"<p>As reported in our paper, if one has a nuclei signal imaged together with the boundary signal, we could leverage the fact that one cell contains only one nucleus and use the <code>LiftedMultict</code> segmentation strategy and obtain improved segmentation. We will use the Arabidopsis thaliana lateral root as an example. The <code>LiftedMulticut</code> strategy consists of running PlantSeg two times:</p> <ol> <li> <p>Using PlantSeg to predict the nuclei probability maps using the <code>lightsheet_unet_bce_dice_nuclei_ds1x</code> network. In this case, only the pre-processing and CNN prediction steps are enabled in the config. See example nuclei prediction config.</p> <pre><code>plantseg --config nuclei_predictions_example.yaml\n</code></pre> </li> <li> <p>Using PlantSeg to segment the input image with the <code>LiftedMulticut</code> algorithm given the nuclei probability maps from the 1st step. See example lifted multicut config. The notable difference is that in the <code>segmentation</code> part of the config, we set <code>name: LiftedMulticut</code> and the <code>nuclei_predictions_path</code> as the path to the directory where the nuclei pmaps were saved in step 1. Also, make sure that the <code>path</code> attribute points to the raw files containing the cell boundary staining (NOT THE NUCLEI).</p> <pre><code>plantseg --config lifted_multicut_example.yaml\n</code></pre> </li> </ol> <p>If case when the nuclei segmentation is given, one should skip step 1., add <code>is_segmentation=True</code> flag in the example lifted multicut config and directly run step 2.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/","title":"PlantSeg from GUI","text":"<p>Deprecated</p> <p>This interface is deprecated and has been removed from PlantSeg v2. Please use the Napari viewer or the command line interface instead, or install PlantSeg v1.</p> <p>The graphical user interface is the easiest way to configure and run PlantSeg. Currently the GUI does not allow to visualize or interact with the data. We recommend using MorphographX or Fiji in order to assert the success and quality of the pipeline results.</p> <p></p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/#file-browser-widget","title":"File Browser Widget","text":"<p>The file browser can be used to select the input files for the pipeline. PlantSeg can run on a single file (button A) or in batch mode for all files inside a directory (button B). If a directory is selected PlantSeg will run on all compatible files inside the directory.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/#main-pipeline-configurator","title":"Main Pipeline Configurator","text":"<p>The central panel of PlantSeg (C) is the core of the pipeline configuration. It can be used for customizing and tuning the pipeline accordingly to the data at hand. Detailed information for each stage can be found at:</p> <ul> <li>Data-Processing</li> <li>CNN-Prediction</li> <li>Segmentation</li> </ul> <p>Any of the above widgets can be run singularly or in sequence (left to right). The order of execution can not be modified.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/#run","title":"Run","text":"<p>The last panel has two main functions. Running the pipeline (D), once the run button is pressed the pipeline starts. The button is inactive until the process is finished. Adding a custom model (E). Custom trained model can be done by using the dedicated popup. Training a new model can be done following the instruction at pytorch-3dunet.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/#results","title":"Results","text":"<p>The results are stored together with the source input files inside a nested directory structure. As example, if we want to run PlantSeg inside a directory with 2 stacks, we will obtain the following outputs:</p> <pre><code>/file1.tif\n/file2.tif\n/PreProcesing/\n------------&gt;/file1.h5\n------------&gt;/file1.yaml\n------------&gt;/file2.h5\n------------&gt;/file2.yaml\n------------&gt;/generic_confocal_3d_unet/\n-------------------------------------&gt;/file1_prediction.h5\n-------------------------------------&gt;/file1_prediction.yaml\n-------------------------------------&gt;/file2_prediction.h5\n-------------------------------------&gt;/file2_prediction.yaml\n-------------------------------------&gt;/GASP/\n------------------------------------------&gt;/file_1_predions_gasp_average.h5\n------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n------------------------------------------&gt;/file_2_predions_gasp_average.h5\n------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n------------------------------------------&gt;/PostProcessing/\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n</code></pre> <p>The use of this hierarchical directory structure allows PlantSeg to easily find the necessary files and can be used to test different combination of segmentation algorithms/parameters minimizing the memory overhead on the disk. For sake of reproducibility, every file is associated with a configuration file \".yaml\" that saves all parameters used to produce the result.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/#start-plantseg-gui","title":"Start PlantSeg GUI","text":"<p>In order to start the PlantSeg app in GUI mode: First, activate the newly created conda environment with:</p> <pre><code>conda activate plant-seg\n</code></pre> <p>then, run the GUI by simply typing:</p> <pre><code>plantseg --gui\n</code></pre>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/cnn_prediction/","title":"CNN Prediction","text":"<p>Deprecated</p> <p>This interface is deprecated and has been removed from PlantSeg v2. Please use the Napari viewer or the command line interface instead, or install PlantSeg v1.</p> <p></p> <p>The CNN prediction widget process the stacks at hand with a Convolutional Neural Network. The output is a boundary classification image, where every voxel gets a value between 0 (not a cell boundary) and 1 (cell boundary).</p> <p>The input image can be a raw stack \"tiff\"/\"h5\" or the output of the PreProcessing widget.</p> <ul> <li> <p>The Model Name menu shows all available models. There are two main basic models available</p> <ol> <li> <p>Generic confocalis a generic model for all confocal datasets. Some examples: </p> </li> <li> <p>Generic lightsheet this is a generic model for all lightsheet datasets.  Some examples:  </p> </li> </ol> </li> <li> <p>Due to memory constraints, usually a complete stack does not fit the GPUs memory,  therefore the Patch size can be used to optimize the performance of the pipeline.  Usually, larger patches cost more memory but can slightly improve performance.  For 2D segmentation, the Patch size relative to the z-axis has to be set to 1.</p> </li> <li> <p>To minimize the boundary effect due to the sliding windows patching, we can use different stride:</p> <ol> <li>Accurate: corresponding to a stride 50% of the patch size (yield best prediction/segmentation accuracy)</li> <li>Balanced: corresponding to a stride 75% of the patch size</li> <li>Draft: corresponding to a stride 95% of the patch size (yield fastest runtime)</li> </ol> </li> <li> <p>The Device type menu can be used to enable or not GPU acceleration. CUDA greatly accelerates the network prediction on Nvidia GPUs. At the moment, we don't support other GPUs manufacturers.</p> </li> </ul>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/data_processing/","title":"Classic Data Processing","text":"<p>Deprecated</p> <p>This interface is deprecated and has been removed from PlantSeg v2. Please use the Napari viewer or the command line interface instead, or install PlantSeg v1.</p> <p> PlantSeg includes essential utilities for data pre-processing and post-processing.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/data_processing/#pre-processing","title":"Pre-Processing","text":"<p>The input for this widget can be either a \"raw\" image or a \"prediction\" image. Input formats allowed are tiff and h5, while output is always h5.</p> <ul> <li> <p>Save Directory can be used to define the output directory.</p> </li> <li> <p>The most critical setting is the Rescaling. It is important to rescale the image to  match the resolution of the data used for training the Neural Network. This operation can be done automatically by clicking on the GUI on Guided. Be careful to use this function only in case of data considerably different from the reference resolution.</p> </li> </ul> <pre><code>As an example:\n  - if your data has the voxel size of 0.3 x 0.1 x 0.1 (ZYX).\n  - and the networks was trained on 0.3 x 0.2 x 0.2 data (reference resolution).\n\nThe required voxel size can be obtained by computing the ratio between your data and the\nreference train dataset. In the example the rescaling factor = 1 x 2 x 2.\n</code></pre> <ul> <li> <p>The Interpolation field controls the interpolation type (0 for nearest neighbors, 1 for linear spline, 2 for quadratic).</p> </li> <li> <p>The last field defines a Filter operation. Implemented there are:</p> <ol> <li>Gaussian Filtering: The parameter is a float and defines the sigma value for the gaussian smoothing. The higher, the wider is filtering kernel.</li> <li>Median Filtering: Apply median operation convolutionally over the image.  The kernel is a sphere of size defined in the parameter field.</li> </ol> </li> </ul>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/data_processing/#post-processing","title":"Post-Processing","text":"<p>A post-processing step can be performed after the CNN-Prediction and the Segmentation. The post-processing options are:</p> <ul> <li> <p>Converting the output to the tiff file format (default is h5).</p> </li> <li> <p>Casting the CNN-Prediction output to data_uint8 drastically reduces the memory footprint of the output  file.</p> </li> </ul> <p>Additionally, the post-processing will scale back your outputs to the original voxels resolutions.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/segmentation/","title":"Segmentation","text":"<p>Deprecated</p> <p>This interface is deprecated and has been removed from PlantSeg v2. Please use the Napari viewer or the command line interface instead, or install PlantSeg v1.</p> <p>The segmentation widget allows using very powerful graph partitioning techniques to obtain a segmentation from the input stacks. The input of this widget should be the output of the CNN-prediction widget. If the boundary prediction stage fails for any reason, a raw image could be used (especially if the cell boundaries are  very sharp, and the noise is low) but usually does not yield satisfactory results.</p> <ul> <li> <p>The Algorithm menu can be used to choose the segmentation algorithm. Available choices are:</p> <ol> <li>GASP (average): is a generalization of the classical hierarchical clustering. It usually delivers very reliable and accurate segmentation. It is the default in PlantSeg.</li> <li>MutexWS: Mutex Watershed is a derivative of the standard Watershed, where we do not need seeds for the  segmentation. This algorithm performs very well in certain types of complex morphology (like )</li> <li>MultiCut: in contrast to the other algorithms is not based on a greedy agglomeration but tries to find the optimal global segmentation. This is, in practice, very hard, and it can be infeasible for huge stacks.</li> <li>DtWatershed: is our implementation of the distance transform Watershed. From the input, we extract a distance map from the boundaries. Based this distance map, seeds are placed at local minima. Then those seeds are used for computing the Watershed segmentation. To speed up the computation of GASP, MutexWS, and MultiCut, an over-segmentation  is obtained using Dt Watershed.</li> </ol> </li> <li> <p>Save Directory defines the sub-directory's name where the segmentation results will be stored.</p> </li> <li> <p>The Under/Over- segmentation factor is the most critical parameters for tuning the segmentation of GASP, MutexWS and MultiCut. A small value will steer the segmentation towards under-segmentation. While a high-value bias the segmentation towards the over-segmentation. This parameter does not affect the distance transform Watershed.</p> </li> <li> <p>If Run Watershed in 2D value is True, the superpixels are created in 2D (over the z slice). While if False makes the superpixels in the whole 3D volume. 3D superpixels are much slower and memory intensive but can improve  the segmentation accuracy.</p> </li> <li> <p>The CNN Prediction Threshold is used for the superpixels extraction and Distance Transform Watershed. It has a crucial role for the watershed seeds extraction and can be used similarly to the \"Unde/Over segmentation factor.\" to bias the final result. A high value translates to less seeds being placed (more under segmentation), while with a low value, more seeds are  placed (more over-segmentation).</p> </li> <li> <p>The input is used by the distance transform Watershed to extract the seed and find the segmentation boundaries. If Watershed Seeds Sigma and Watershed Boundary Sigma are larger than  zero, a gaussian smoothing is applied on the input before the operations above. This is mainly helpful for  the seeds computation but, in most cases, does not impact segmentation quality.</p> </li> <li> <p>The Superpixels Minimum Size applies a size filter to the initial superpixels over-segmentation. This removes Watershed often produces small segments and is usually helpful for the subsequent agglomeration.  Segments smaller than the threshold will be merged with the nearest neighbor segment.</p> </li> <li> <p>Even though GASP, MutexWS, and MultiCut are not very prone to produce small segments, the Cell Minimum Size can be used as a final size processing filter. Segments smaller than the threshold will be merged with the nearest neighbor cell.</p> </li> </ul>"},{"location":"chapters/plantseg_models/","title":"Official Data and Models","text":""},{"location":"chapters/plantseg_models/#datasets","title":"Datasets","text":"<p>We publicly release the datasets used for training the networks which available as part of the PlantSeg package. Please refer to our publication for more details about the datasets:</p> <ul> <li>Arabidopsis thaliana ovules dataset (raw confocal images + ground truth labels)</li> <li>Arabidopsis thaliana lateral root (raw light sheet images + ground truth labels)</li> </ul> <p>Both datasets can be downloaded from our OSF project</p>"},{"location":"chapters/plantseg_models/#pre-trained-networks","title":"Pre-trained Networks","text":"<p>The following pre-trained networks are provided with PlantSeg package out-of-the box and can be specified in the config file or chosen in the GUI.</p> <ul> <li><code>generic_confocal_3D_unet</code> - alias for <code>confocal_3D_unet_ovules_ds2x</code> see below</li> <li><code>generic_light_sheet_3D_unet</code> - alias for <code>lightsheet_3D_unet_root_ds1x</code> see below</li> <li><code>confocal_3D_unet_ovules_ds1x</code> - a variant of 3D U-Net trained on confocal images of Arabidopsis ovules on original resolution, voxel size: (0.235x0.075x0.075 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>confocal_3D_unet_ovules_ds2x</code> - a variant of 3D U-Net trained on confocal images of Arabidopsis ovules on 1/2 resolution, voxel size: (0.235x0.150x0.150 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>confocal_3D_unet_ovules_ds3x</code> - a variant of 3D U-Net trained on confocal images of Arabidopsis ovules on 1/3 resolution, voxel size: (0.235x0.225x0.225 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>confocal_2D_unet_ovules_ds2x</code> - a variant of 2D U-Net trained on confocal images of Arabidopsis ovules. Training the 2D U-Net is done on the Z-slices (1/2 resolution, pixel size: 0.150x0.150 \u00b5m^3) with BCEDiceLoss</li> <li><code>confocal_3D_unet_ovules_nuclei_ds1x</code> - a variant of 3D U-Net trained on confocal images of Arabidopsis ovules nuclei stain on original resolution, voxel size: (0.35x0.1x0.1 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_3D_unet_root_ds1x</code> - a variant of 3D U-Net trained on light-sheet images of Arabidopsis lateral root on original resolution, voxel size: (0.25x0.1625x0.1625 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_3D_unet_root_ds2x</code> - a variant of 3D U-Net trained on light-sheet images of Arabidopsis lateral root on 1/2 resolution, voxel size: (0.25x0.325x0.325 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_3D_unet_root_ds3x</code> - a variant of 3D U-Net trained on light-sheet images of Arabidopsis lateral root on 1/3 resolution, voxel size: (0.25x0.4875x0.4875 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_2D_unet_root_ds1x</code> - a variant of 2D U-Net trained on light-sheet images of Arabidopsis lateral root. Training the 2D U-Net is done on the Z-slices (pixel size: 0.1625x0.1625 \u00b5m^3) with BCEDiceLoss</li> <li><code>lightsheet_3D_unet_root_nuclei_ds1x</code> - a variant of 3D U-Net trained on light-sheet images Arabidopsis lateral root nuclei on original resolution, voxel size: (0.25x0.1625x0.1625 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_2D_unet_root_nuclei_ds1x</code> - a variant of 2D U-Net trained on light-sheet images Arabidopsis lateral root nuclei. Training the 2D U-Net is done on the Z-slices (pixel size: 0.1625x0.1625 \u00b5m^3) with BCEDiceLoss.</li> <li><code>confocal_3D_unet_sa_meristem_cells</code> - a variant of 3D U-Net trained on confocal images of shoot apical meristem dataset from: Jonsson, H., Willis, L., &amp; Refahi, Y. (2017). Research data supporting Cell size and growth regulation in the Arabidopsis thaliana apical stem cell niche. https://doi.org/10.17863/CAM.7793. voxel size: (0.25x0.25x0.25 \u00b5m^3) (ZYX)</li> <li><code>confocal_2D_unet_sa_meristem_cells</code> - a variant of 2D U-Net trained on confocal images of shoot apical meristem dataset from: Jonsson, H., Willis, L., &amp; Refahi, Y. (2017). Research data supporting Cell size and growth regulation in the Arabidopsis thaliana apical stem cell niche. https://doi.org/10.17863/CAM.7793.  pixel size: (25x0.25 \u00b5m^3) (YX)</li> <li><code>lightsheet_3D_unet_mouse_embryo_cells</code> - A variant of 3D U-Net trained to predict the cell boundaries in live light-sheet images of ex-vivo developing mouse embryo. Voxel size: (0.2\u00d70.2\u00d71 \u00b5m^3) (XYZ)</li> <li><code>confocal_3D_unet_mouse_embryo_nuclei</code> - A variant of 3D U-Net trained to predict the cell boundaries in live light-sheet images of ex-vivo developing mouse embryo. Voxel size: (0.2\u00d70.2\u00d71 \u00b5m^3) (XYZ)</li> </ul> <p>Selecting a given network name (either in the config file or GUI) will download the network into the <code>~/.plantseg_models</code> directory. Detailed description of network training can be found in our paper.</p> <p>The PlantSeg home directory can be configured with the <code>PLANTSEG_HOME</code> environment variable.</p> <pre><code>export PLANTSEG_HOME=\"/path/to/plantseg/home\"\n</code></pre>"},{"location":"chapters/plantseg_models/custom_datasets/","title":"Custom Datasets","text":"<p>To train own models from multiple images, you need to create a dataset first. This dataset is just a file structure so PlantSeg knows how to load the training data. It should look like this:</p> <pre><code>mydataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 first.h5\n\u2502   \u2514\u2500\u2500 second.h5\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 val_one.h5\n    \u2514\u2500\u2500 val_two.h5 \n</code></pre> <p>The recommended way to train new models is through the napari training GUI.</p>"},{"location":"chapters/plantseg_models/training/","title":"Advanced training","text":"<p>Warning</p> <p>The recommended way to train models is through the napari training GUI. This section gives additional background information not needed for intended training procedure.</p> <p>For training models, we rely on the pytorch-3dunet. You can import models trained using this library into PlantSeg by choosing <code>ADD CUSTOM MODEL</code> in the model selection drop-down in the segmentation tab. Models are by default stored in <code>~/.plantseg_models/</code>.</p> <p>Each model needs the following files:</p> <ul> <li>Configuration file used for training: <code>config_train.yml</code></li> <li>A snapshot of the best model across training: <code>best_checkpoint.pytorch</code></li> <li>A Snapshot of the last model saved during training: <code>last_checkpoint.pytorch</code></li> </ul> <p>Check the model directory to see examples of the <code>config_train.yml</code></p>"},{"location":"chapters/python_api/","title":"Hierarchical Design of PlantSeg","text":"<p>PlantSeg is organized into three layers:</p> <ol> <li>Functionals (Python API): The foundational layer of PlantSeg, providing its core functionality. This layer can be accessed directly in Python scripts or Jupyter notebooks.</li> <li>Tasks: The intermediate layer of PlantSeg, which encapsulates the functionals to handle resource management and support distributed computing.</li> <li>Napari Widgets: The top layer of PlantSeg, which integrates tasks into user-friendly widgets for easy interaction within graphical interfaces.</li> </ol>"},{"location":"chapters/python_api/functionals/cnn_prediction/","title":"PlantSeg CNN Prediction","text":""},{"location":"chapters/python_api/functionals/cnn_prediction/#plantseg.functionals.prediction.prediction.unet_prediction","title":"<code>plantseg.functionals.prediction.prediction.unet_prediction(raw: np.ndarray, input_layout: ImageLayout, model_name: str | None, model_id: str | None, patch: tuple[int, int, int] | None = None, patch_halo: tuple[int, int, int] | None = None, single_batch_mode: bool = True, device: str = 'cuda', model_update: bool = False, disable_tqdm: bool = False, config_path: Path | None = None, model_weights_path: Path | None = None, tracker=None) -&gt; np.ndarray</code>","text":"<p>Generate prediction from raw data using a specified 3D U-Net model.</p> <p>This function handles both single and multi-channel outputs from the model, returning appropriately shaped arrays based on the output channel configuration.</p> <p>For Bioimage.IO Model Zoo models, weights are downloaded and loaded into <code>UNet3D</code> or <code>UNet2D</code> in <code>plantseg.functionals.training.model</code>, i.e. <code>bioimageio.core</code> is not used. <code>biio_prediction()</code> uses <code>bioimageio.core</code> for loading and running models.</p> <p>Parameters:</p> <ul> <li> <code>raw</code>               (<code>ndarray</code>)           \u2013            <p>Raw input data.</p> </li> <li> <code>Input_layout</code>               (<code>ImageLayout</code>)           \u2013            <p>The layout of the input data.</p> </li> <li> <code>model_name</code>               (<code>str | None</code>)           \u2013            <p>The name of the model to use.</p> </li> <li> <code>model_id</code>               (<code>str | None</code>)           \u2013            <p>The ID of the model from the BioImage.IO model zoo.</p> </li> <li> <code>patch</code>               (<code>tuple[int, int, int]</code>, default:                   <code>None</code> )           \u2013            <p>Patch size for prediction. Defaults to (80, 160, 160).</p> </li> <li> <code>patch_halo</code>               (<code>tuple[int, int, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Halo size around patches. Defaults to None.</p> </li> <li> <code>single_batch_mode</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use a single batch for prediction. Defaults to True.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda'</code> )           \u2013            <p>The computation device ('cpu', 'cuda', etc.). Defaults to 'cuda'.</p> </li> <li> <code>model_update</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to update the model to the latest version. Defaults to False.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, disables the tqdm progress bar. Defaults to False.</p> </li> <li> <code>config_path</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model configuration file. Defaults to None.</p> </li> <li> <code>model_weights_path</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model weights file. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: The predicted boundaries as a 3D (Z, Y, X) or 4D (C, Z, Y, X) array, normalized between 0 and 1.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If neither <code>model_name</code>, <code>model_id</code>, nor <code>config_path</code> are provided.</p> </li> </ul> Source code in <code>plantseg/functionals/prediction/prediction.py</code> <pre><code>def unet_prediction(\n    raw: np.ndarray,\n    input_layout: ImageLayout,\n    model_name: str | None,\n    model_id: str | None,\n    patch: tuple[int, int, int] | None = None,\n    patch_halo: tuple[int, int, int] | None = None,\n    single_batch_mode: bool = True,\n    device: str = \"cuda\",\n    model_update: bool = False,\n    disable_tqdm: bool = False,\n    config_path: Path | None = None,\n    model_weights_path: Path | None = None,\n    tracker=None,\n) -&gt; np.ndarray:\n    \"\"\"Generate prediction from raw data using a specified 3D U-Net model.\n\n    This function handles both single and multi-channel outputs from the model,\n    returning appropriately shaped arrays based on the output channel configuration.\n\n    For Bioimage.IO Model Zoo models, weights are downloaded and loaded into `UNet3D` or `UNet2D`\n    in `plantseg.functionals.training.model`, i.e. `bioimageio.core` is not used. `biio_prediction()` uses\n    `bioimageio.core` for loading and running models.\n\n    Args:\n        raw (np.ndarray): Raw input data.\n        Input_layout (ImageLayout): The layout of the input data.\n        model_name (str | None): The name of the model to use.\n        model_id (str | None): The ID of the model from the BioImage.IO model zoo.\n        patch (tuple[int, int, int], optional): Patch size for prediction. Defaults to (80, 160, 160).\n        patch_halo (tuple[int, int, int] | None, optional): Halo size around patches. Defaults to None.\n        single_batch_mode (bool, optional): Whether to use a single batch for prediction. Defaults to True.\n        device (str, optional): The computation device ('cpu', 'cuda', etc.). Defaults to 'cuda'.\n        model_update (bool, optional): Whether to update the model to the latest version. Defaults to False.\n        disable_tqdm (bool, optional): If True, disables the tqdm progress bar. Defaults to False.\n        config_path (Path | None, optional): Path to the model configuration file. Defaults to None.\n        model_weights_path (Path | None, optional): Path to the model weights file. Defaults to None.\n\n    Returns:\n        np.ndarray: The predicted boundaries as a 3D (Z, Y, X) or 4D (C, Z, Y, X) array, normalized between 0 and 1.\n\n    Raises:\n        ValueError: If neither `model_name`, `model_id`, nor `config_path` are provided.\n    \"\"\"\n\n    if config_path is not None:  # Safari mode for custom models outside zoos\n        logger.info(\"Safari prediction: Running model from custom config path.\")\n        model, model_config, model_path = model_zoo.get_model_by_config_path(\n            config_path, model_weights_path\n        )\n    elif model_id is not None:  # BioImage.IO zoo mode\n        logger.info(\"BioImage.IO prediction: Running model from BioImage.IO model zoo.\")\n        model, model_config, model_path = model_zoo.get_model_by_id(model_id)\n    elif model_name is not None:  # PlantSeg zoo mode\n        logger.info(\"Zoo prediction: Running model from PlantSeg official zoo.\")\n        model, model_config, model_path = model_zoo.get_model_by_name(\n            model_name, model_update=model_update\n        )\n    else:\n        raise ValueError(\n            \"Either `model_name` or `model_id` or `model_path` must be provided.\"\n        )\n    state = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n\n    if \"model_state_dict\" in state:  # Model weights format may vary between versions\n        state = state[\"model_state_dict\"]\n    model.load_state_dict(state)\n\n    if patch_halo is None:\n        try:\n            patch_halo = model_zoo.compute_3D_halo_for_pytorch3dunet(model)\n            logger.info(\"Computed theoretical minimum halo from model: {patch_halo}\")\n        except Exception:\n            logger.warning(\n                \"Could not compute halo from model. Using 0 halo size, you may experience edge artifacts.\"\n            )\n            patch_halo = (0, 0, 0)\n\n    if patch is None:\n        maximum_patch_shape = find_a_max_patch_shape(\n            model, model_config[\"in_channels\"], device\n        )\n        raw_shape = raw.shape if input_layout == \"ZYX\" else (1,) + raw.shape\n        assert len(raw_shape) == 3\n        patch, patch_halo = find_patch_and_halo_shapes(\n            raw_shape, maximum_patch_shape, patch_halo, both_sides=False\n        )\n\n    logger.info(\n        f\"For raw in shape {raw.shape}: set patch shape {patch}, set halo shape {patch_halo}\"\n    )\n\n    predictor = ArrayPredictor(\n        model=model,\n        in_channels=model_config[\"in_channels\"],\n        out_channels=model_config[\"out_channels\"],\n        device=device,\n        patch=patch,\n        patch_halo=patch_halo,\n        single_batch_mode=single_batch_mode,\n        headless=False,\n        verbose_logging=False,\n        disable_tqdm=disable_tqdm,\n        tracker=tracker,\n    )\n\n    if int(model_config[\"in_channels\"]) &gt; 1:  # if multi-channel input\n        raw = fix_layout_to_CZYX(raw, input_layout)\n        multichannel_input = True\n    else:\n        raw = fix_layout_to_ZYX(raw, input_layout)\n        multichannel_input = False\n    raw = raw.astype(\"float32\")\n    augs = get_test_augmentations(\n        raw\n    )  # using full raw to compute global normalization mean and std\n    stride = get_stride_shape(patch)\n    slice_builder = SliceBuilder(\n        raw, label_dataset=None, patch_shape=patch, stride_shape=stride\n    )\n    test_dataset = ArrayDataset(\n        raw,\n        slice_builder,\n        augs,\n        halo_shape=patch_halo,\n        multichannel=multichannel_input,\n        verbose_logging=False,\n    )\n\n    pmaps = predictor(test_dataset)  # pmaps either (C, Z, Y, X) or (C, Y, X)\n    return pmaps\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/","title":"Data Processing","text":"<p>Basic data processing functions are provided in the <code>dataprocessing</code> module. These functions are used to preprocess data before training a model, or to post-process the output of a model.</p>"},{"location":"chapters/python_api/functionals/data_processing/#generic-functions","title":"Generic Functions","text":""},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.normalize_01","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.normalize_01(data: np.ndarray, eps=1e-12) -&gt; np.ndarray</code>","text":"<p>Normalize a numpy array between 0 and 1 and converts it to float32.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>Input numpy array</p> </li> <li> <code>eps</code>               (<code>float</code>, default:                   <code>1e-12</code> )           \u2013            <p>A small value added to the denominator for numerical stability</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>normalized_data</code> (              <code>ndarray</code> )          \u2013            <p>Normalized numpy array</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def normalize_01(data: np.ndarray, eps=1e-12) -&gt; np.ndarray:\n    \"\"\"\n    Normalize a numpy array between 0 and 1 and converts it to float32.\n\n    Args:\n        data (np.ndarray): Input numpy array\n        eps (float): A small value added to the denominator for numerical stability\n\n    Returns:\n        normalized_data (np.ndarray): Normalized numpy array\n    \"\"\"\n    return (data - np.min(data)) / (np.max(data) - np.min(data) + eps).astype(\"float32\")\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.scale_image_to_voxelsize","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.scale_image_to_voxelsize(image: np.ndarray, input_voxel_size: tuple[float, float, float], output_voxel_size: tuple[float, float, float], order: int = 0) -&gt; np.ndarray</code>","text":"<p>Scale an image from a given voxel size to another voxel size.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to scale</p> </li> <li> <code>input_voxel_size</code>               (<code>tuple[float, float, float]</code>)           \u2013            <p>Input voxel size</p> </li> <li> <code>output_voxel_size</code>               (<code>tuple[float, float, float]</code>)           \u2013            <p>Output voxel size</p> </li> <li> <code>order</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Interpolation order, must be 0 for segmentation and 1, 2 for images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>scaled_image</code> (              <code>ndarray</code> )          \u2013            <p>Scaled image as numpy array</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def scale_image_to_voxelsize(\n    image: np.ndarray,\n    input_voxel_size: tuple[float, float, float],\n    output_voxel_size: tuple[float, float, float],\n    order: int = 0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Scale an image from a given voxel size to another voxel size.\n\n    Args:\n        image (np.ndarray): Input image to scale\n        input_voxel_size (tuple[float, float, float]): Input voxel size\n        output_voxel_size (tuple[float, float, float]): Output voxel size\n        order (int): Interpolation order, must be 0 for segmentation and 1, 2 for images\n\n    Returns:\n        scaled_image (np.ndarray): Scaled image as numpy array\n    \"\"\"\n    factor = compute_scaling_factor(input_voxel_size, output_voxel_size)\n    return image_rescale(image, factor, order=order)\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.image_rescale","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.image_rescale(image: np.ndarray, factor: tuple[float, float, float], order: int) -&gt; np.ndarray</code>","text":"<p>Scale an image by a given factor in each dimension</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to scale</p> </li> <li> <code>factor</code>               (<code>tuple[float, float, float]</code>)           \u2013            <p>Scaling factor in each dimension</p> </li> <li> <code>order</code>               (<code>int</code>)           \u2013            <p>Interpolation order, must be 0 for segmentation and 1, 2 for images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>scaled_image</code> (              <code>ndarray</code> )          \u2013            <p>Scaled image as numpy array</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def image_rescale(\n    image: np.ndarray, factor: tuple[float, float, float], order: int\n) -&gt; np.ndarray:\n    \"\"\"\n    Scale an image by a given factor in each dimension\n\n    Args:\n        image (np.ndarray): Input image to scale\n        factor (tuple[float, float, float]): Scaling factor in each dimension\n        order (int): Interpolation order, must be 0 for segmentation and 1, 2 for images\n\n    Returns:\n        scaled_image (np.ndarray): Scaled image as numpy array\n    \"\"\"\n    if np.array_equal(factor, [1.0, 1.0, 1.0]):\n        return image\n    else:\n        return zoom(image, zoom=factor, order=order)\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.image_median","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.image_median(image: np.ndarray, radius: int) -&gt; np.ndarray</code>","text":"<p>Apply median smoothing on an image with a given radius.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to apply median smoothing.</p> </li> <li> <code>radius</code>               (<code>int</code>)           \u2013            <p>Radius of the median filter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Median smoothed image.</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def image_median(image: np.ndarray, radius: int) -&gt; np.ndarray:\n    \"\"\"\n    Apply median smoothing on an image with a given radius.\n\n    Args:\n        image (np.ndarray): Input image to apply median smoothing.\n        radius (int): Radius of the median filter.\n\n    Returns:\n        np.ndarray: Median smoothed image.\n    \"\"\"\n    if radius &lt;= 0:\n        raise ValueError(\"Radius must be a positive integer.\")\n\n    if image.ndim == 2:\n        # 2D image\n        return median(image, disk(radius))\n    elif image.ndim == 3:\n        if image.shape[0] == 1:\n            # Single slice (ZYX or YX) case\n            return median(image[0], disk(radius)).reshape(image.shape)\n        else:\n            # 3D image\n            return median(image, ball(radius))\n    else:\n        raise ValueError(\n            \"Unsupported image dimensionality. Image must be either 2D or 3D.\"\n        )\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.image_gaussian_smoothing","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.image_gaussian_smoothing(image: np.ndarray, sigma: float) -&gt; np.ndarray</code>","text":"<p>Apply gaussian smoothing on an image with a given sigma.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to apply gaussian smoothing</p> </li> <li> <code>sigma</code>               (<code>float</code>)           \u2013            <p>Sigma value for gaussian smoothing</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>smoothed_image</code> (              <code>ndarray</code> )          \u2013            <p>Gaussian smoothed image as numpy array</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def image_gaussian_smoothing(image: np.ndarray, sigma: float) -&gt; np.ndarray:\n    \"\"\"\n    Apply gaussian smoothing on an image with a given sigma.\n\n    Args:\n        image (np.ndarray): Input image to apply gaussian smoothing\n        sigma (float): Sigma value for gaussian smoothing\n\n    Returns:\n        smoothed_image (np.ndarray): Gaussian smoothed image as numpy array\n    \"\"\"\n    image = image.astype(\"float32\")\n    max_sigma = (np.array(image.shape) - 1) / 3\n    sigma_array = np.minimum(max_sigma, np.ones(max_sigma.ndim) * sigma)\n    return gaussianSmoothing(image, sigma_array)\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.image_crop","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.image_crop(image: np.ndarray, crop_str: str) -&gt; np.ndarray</code>","text":"<p>Crop an image from a crop string like [:, 10:30:, 10:20]</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to crop</p> </li> <li> <code>crop_str</code>               (<code>str</code>)           \u2013            <p>Crop string</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>cropped_image</code> (              <code>ndarray</code> )          \u2013            <p>Cropped image as numpy array</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def image_crop(image: np.ndarray, crop_str: str) -&gt; np.ndarray:\n    \"\"\"\n    Crop an image from a crop string like [:, 10:30:, 10:20]\n\n    Args:\n        image (np.ndarray): Input image to crop\n        crop_str (str): Crop string\n\n    Returns:\n        cropped_image (np.ndarray): Cropped image as numpy array\n    \"\"\"\n    crop_str = crop_str.replace(\"[\", \"\").replace(\"]\", \"\")\n    slices = tuple(\n        (\n            slice(*(int(i) if i else None for i in part.strip().split(\":\")))\n            if \":\" in part\n            else int(part.strip())\n        )\n        for part in crop_str.split(\",\")\n    )\n    return image[slices]\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.process_images","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.process_images(image1: np.ndarray, image2: np.ndarray, operation: ImagePairOperation, normalize_input: bool = False, clip_output: bool = False, normalize_output: bool = True) -&gt; np.ndarray</code>","text":"<p>General function for performing image operations with optional preprocessing and post-processing.</p> <p>Parameters:</p> <ul> <li> <code>image1</code>               (<code>ndarray</code>)           \u2013            <p>First input image.</p> </li> <li> <code>image2</code>               (<code>ndarray</code>)           \u2013            <p>Second input image.</p> </li> <li> <code>operation</code>               (<code>str</code>)           \u2013            <p>Operation to perform ('add', 'multiply', 'subtract', 'divide', 'max').</p> </li> <li> <code>normalize_input</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to normalize the input images to the range [0, 1]. Default is False.</p> </li> <li> <code>clip_output</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to clip the resulting image values to the range [0, 1]. Default is False.</p> </li> <li> <code>normalize_output</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to normalize the output image to the range [0, 1]. Default is True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: The resulting image after performing the operation.</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def process_images(\n    image1: np.ndarray,\n    image2: np.ndarray,\n    operation: ImagePairOperation,\n    normalize_input: bool = False,\n    clip_output: bool = False,\n    normalize_output: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"\n    General function for performing image operations with optional preprocessing and post-processing.\n\n    Args:\n        image1 (np.ndarray): First input image.\n        image2 (np.ndarray): Second input image.\n        operation (str): Operation to perform ('add', 'multiply', 'subtract', 'divide', 'max').\n        normalize_input (bool): Whether to normalize the input images to the range [0, 1]. Default is False.\n        clip_output (bool): Whether to clip the resulting image values to the range [0, 1]. Default is False.\n        normalize_output (bool): Whether to normalize the output image to the range [0, 1]. Default is True.\n\n    Returns:\n        np.ndarray: The resulting image after performing the operation.\n    \"\"\"\n    # Preprocessing: Normalize input images if specified\n    if normalize_input:\n        image1, image2 = normalize_01(image1), normalize_01(image2)\n\n    # Perform the specified operation\n    if operation == \"add\":\n        result = image1 + image2\n    elif operation == \"multiply\":\n        result = image1 * image2\n    elif operation == \"subtract\":\n        result = image1 - image2\n    elif operation == \"divide\":\n        result = image1 / image2\n    elif operation == \"max\":\n        result = np.maximum(image1, image2)\n    else:\n        raise ValueError(f\"Unsupported operation: {operation}\")\n\n    # Post-processing: Clip and/or normalize output if specified\n    if clip_output:\n        result = np.clip(result, 0, 1)\n    if normalize_output:\n        result = normalize_01(result)\n\n    return result\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#segmentation-functions","title":"Segmentation Functions","text":""},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.labelprocessing.relabel_segmentation","title":"<code>plantseg.functionals.dataprocessing.labelprocessing.relabel_segmentation(segmentation_image: np.ndarray, background: int | None = None) -&gt; np.ndarray</code>","text":"<p>Relabels contiguously a segmentation image, non-touching instances with same id will be relabeled differently. To be noted that measure.label is different from ndimage.label.</p> <p>1-connectivity     2-connectivity     diagonal connection close-up</p> <pre><code> [ ]           [ ]  [ ]  [ ]             [ ]\n  |               \\  |  /                 |  &lt;- hop 2\n</code></pre> <p>[ ]--[x]--[ ]      [ ]--[x]--[ ]        [x]--[ ]       |               /  |  \\             hop 1      [ ]           [ ]  [ ]  [ ]</p> <p>Parameters:</p> <ul> <li> <code>segmentation_image</code>               (<code>ndarray</code>)           \u2013            <p>A 2D or 3D segmentation image where connected components represent different instances.</p> </li> <li> <code>background</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Label of the background. If None, the function will assume the background                                label is 0. Default is None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: A relabeled segmentation image where each connected component is assigned a unique integer label.</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/labelprocessing.py</code> <pre><code>def relabel_segmentation(\n    segmentation_image: np.ndarray, background: int | None = None\n) -&gt; np.ndarray:\n    r\"\"\"\n    Relabels contiguously a segmentation image, non-touching instances with same id will be relabeled differently.\n    To be noted that measure.label is different from ndimage.label.\n\n    1-connectivity     2-connectivity     diagonal connection close-up\n\n         [ ]           [ ]  [ ]  [ ]             [ ]\n          |               \\  |  /                 |  &lt;- hop 2\n    [ ]--[x]--[ ]      [ ]--[x]--[ ]        [x]--[ ]\n          |               /  |  \\             hop 1\n         [ ]           [ ]  [ ]  [ ]\n\n    Args:\n        segmentation_image (np.ndarray): A 2D or 3D segmentation image where connected components represent different instances.\n        background (int | None, optional): Label of the background. If None, the function will assume the background\n                                           label is 0. Default is None.\n\n    Returns:\n        np.ndarray: A relabeled segmentation image where each connected component is assigned a unique integer label.\n    \"\"\"\n    relabeled_segmentation = measure.label(\n        segmentation_image,\n        background=background,\n        return_num=False,\n        connectivity=None,\n    )\n    assert isinstance(relabeled_segmentation, np.ndarray)\n    return relabeled_segmentation\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.labelprocessing.set_background_to_value","title":"<code>plantseg.functionals.dataprocessing.labelprocessing.set_background_to_value(segmentation_image: np.ndarray, value: int = 0) -&gt; np.ndarray</code>","text":"<p>Sets all occurrences of the background (label 0) in the segmentation image to a new value.</p> <p>Parameters:</p> <ul> <li> <code>segmentation_image</code>               (<code>ndarray</code>)           \u2013            <p>A 2D or 3D numpy array representing an instance segmentation.</p> </li> <li> <code>value</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The value to assign to the background. Default is 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: A segmentation image where all background pixels (originally 0) are set to <code>value</code>.</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/labelprocessing.py</code> <pre><code>def set_background_to_value(\n    segmentation_image: np.ndarray, value: int = 0\n) -&gt; np.ndarray:\n    \"\"\"\n    Sets all occurrences of the background (label 0) in the segmentation image to a new value.\n\n    Args:\n        segmentation_image (np.ndarray): A 2D or 3D numpy array representing an instance segmentation.\n        value (int, optional): The value to assign to the background. Default is 0.\n\n    Returns:\n        np.ndarray: A segmentation image where all background pixels (originally 0) are set to `value`.\n    \"\"\"\n    return np.where(segmentation_image == 0, value, segmentation_image)\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#advanced-functions","title":"Advanced Functions","text":""},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.advanced_dataprocessing.fix_over_under_segmentation_from_nuclei","title":"<code>plantseg.functionals.dataprocessing.advanced_dataprocessing.fix_over_under_segmentation_from_nuclei(cell_seg: np.ndarray, nuclei_seg: np.ndarray, threshold_merge: float, threshold_split: float, quantile_min: float, quantile_max: float, boundary: np.ndarray | None = None) -&gt; np.ndarray</code>","text":"<p>Correct over-segmentation and under-segmentation of cells based on nuclei information.</p> <p>This function uses information from nuclei segmentation to refine cell segmentation by first identifying over-segmented cells (cells mistakenly split into multiple segments) and merging them. It then corrects under-segmented cells (multiple nuclei within a single cell) by splitting them based on nuclei position and optional boundary information.</p> <p>Parameters:</p> <ul> <li> <code>cell_seg</code>               (<code>ndarray</code>)           \u2013            <p>A 2D or 3D array of segmented cells, where each integer represents a unique cell.</p> </li> <li> <code>nuclei_seg</code>               (<code>ndarray</code>)           \u2013            <p>A 2D or 3D array of segmented nuclei, matching the shape of <code>cell_seg</code>. Used to guide merging and splitting.</p> </li> <li> <code>threshold_merge</code>               (<code>float</code>)           \u2013            <p>A value between 0 and 1. Cells with less than this fraction of nuclei overlap are considered over-segmented and will be merged. Default is 0.33.</p> </li> <li> <code>threshold_split</code>               (<code>float</code>)           \u2013            <p>A value between 0 and 1. Cells with more than this fraction of nuclei overlap are considered under-segmented and will be split. Default is 0.66.</p> </li> <li> <code>quantile_min</code>               (<code>float</code>)           \u2013            <p>The lower size limit for nuclei, as a fraction (0-1). Nuclei smaller than this quantile are ignored. Default is 0.3.</p> </li> <li> <code>quantile_max</code>               (<code>float</code>)           \u2013            <p>The upper size limit for nuclei, as a fraction (0-1). Nuclei larger than this quantile are ignored. Default is 0.99.</p> </li> <li> <code>boundary</code>               (<code>ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional boundary map of the same shape as <code>cell_seg</code>. High values indicate cell boundaries and help refine splitting. If None, all regions are treated equally.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Corrected cell segmentation array.</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/advanced_dataprocessing.py</code> <pre><code>def fix_over_under_segmentation_from_nuclei(\n    cell_seg: np.ndarray,\n    nuclei_seg: np.ndarray,\n    threshold_merge: float,\n    threshold_split: float,\n    quantile_min: float,\n    quantile_max: float,\n    boundary: np.ndarray | None = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Correct over-segmentation and under-segmentation of cells based on nuclei information.\n\n    This function uses information from nuclei segmentation to refine cell segmentation by first identifying\n    over-segmented cells (cells mistakenly split into multiple segments) and merging them. It then corrects\n    under-segmented cells (multiple nuclei within a single cell) by splitting them based on nuclei position\n    and optional boundary information.\n\n    Args:\n        cell_seg (np.ndarray): A 2D or 3D array of segmented cells, where each integer represents a unique cell.\n        nuclei_seg (np.ndarray): A 2D or 3D array of segmented nuclei, matching the shape of `cell_seg`.\n            Used to guide merging and splitting.\n        threshold_merge (float, optional): A value between 0 and 1. Cells with less than this fraction of nuclei overlap\n            are considered over-segmented and will be merged. Default is 0.33.\n        threshold_split (float, optional): A value between 0 and 1. Cells with more than this fraction of nuclei overlap\n            are considered under-segmented and will be split. Default is 0.66.\n        quantile_min (float, optional): The lower size limit for nuclei, as a fraction (0-1). Nuclei smaller than this\n            quantile are ignored. Default is 0.3.\n        quantile_max (float, optional): The upper size limit for nuclei, as a fraction (0-1). Nuclei larger than this\n            quantile are ignored. Default is 0.99.\n        boundary (np.ndarray | None, optional): Optional boundary map of the same shape as `cell_seg`. High values\n            indicate cell boundaries and help refine splitting. If None, all regions are treated equally.\n\n    Returns:\n        np.ndarray: Corrected cell segmentation array.\n    \"\"\"\n    # Find overlaps between cells and nuclei\n    cell_counts, nuclei_counts, cell_nuclei_counts = numba_find_overlaps(\n        cell_seg, nuclei_seg\n    )\n\n    # Identify over-segmentation and correct it\n    nuclei_assignments = find_potential_over_seg(\n        nuclei_counts, cell_nuclei_counts, threshold=threshold_merge\n    )\n    corrected_seg = fix_over_segmentation(cell_seg, nuclei_assignments)\n\n    # Identify under-segmentation and correct it\n    cell_counts, nuclei_counts, cell_nuclei_counts = numba_find_overlaps(\n        corrected_seg, nuclei_seg\n    )\n    cell_assignments = find_potential_under_seg(\n        nuclei_counts,\n        cell_counts,\n        cell_nuclei_counts,\n        threshold=threshold_split,\n        quantiles_clip=(quantile_min, quantile_max),\n    )\n\n    boundary_pmap = np.ones_like(cell_seg) if boundary is None else boundary\n    return fix_under_segmentation(\n        corrected_seg, nuclei_seg, boundary_pmap, cell_assignments, cell_idx=None\n    )\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.advanced_dataprocessing.remove_false_positives_by_foreground_probability","title":"<code>plantseg.functionals.dataprocessing.advanced_dataprocessing.remove_false_positives_by_foreground_probability(segmentation: np.ndarray, foreground: np.ndarray, threshold: float) -&gt; tuple[np.ndarray, np.ndarray]</code>","text":"<p>Splits an instance segmentation into two based on a foreground probability threshold.</p> <ol> <li>Relabels the input segmentation sequentially (preserving 0 as background).</li> <li>Computes the mean foreground probability for each region.</li> <li>Assigns regions with mean &gt;= threshold to the <code>kept</code> map; the rest to the <code>removed</code> map.</li> <li>Relabels both outputs sequentially for compact labeling.</li> </ol> <p>Parameters:</p> <ul> <li> <code>segmentation</code>               (<code>ndarray</code>)           \u2013            <p>2D or 3D segmentation array; each non-zero integer is a distinct region.</p> </li> <li> <code>foreground</code>               (<code>ndarray</code>)           \u2013            <p>Same shape as <code>segmentation</code>, values in [0, 1] representing per-pixel probabilities.</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>Regions with mean probability below this are considered false positives.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>kept</code> (              <code>ndarray</code> )          \u2013            <p>Segmentation of regions with mean probability &gt;= threshold, relabeled sequentially.</p> </li> <li> <code>removed</code> (              <code>ndarray</code> )          \u2013            <p>Segmentation of regions with mean probability &lt; threshold, relabeled sequentially.</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/advanced_dataprocessing.py</code> <pre><code>def remove_false_positives_by_foreground_probability(\n    segmentation: np.ndarray,\n    foreground: np.ndarray,\n    threshold: float,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Splits an instance segmentation into two based on a foreground probability threshold.\n\n    1. Relabels the input segmentation sequentially (preserving 0 as background).\n    2. Computes the mean foreground probability for each region.\n    3. Assigns regions with mean &gt;= threshold to the `kept` map; the rest to the `removed` map.\n    4. Relabels both outputs sequentially for compact labeling.\n\n    Args:\n        segmentation (np.ndarray): 2D or 3D segmentation array; each non-zero integer is a distinct region.\n        foreground (np.ndarray): Same shape as `segmentation`, values in [0, 1] representing per-pixel probabilities.\n        threshold (float): Regions with mean probability below this are considered false positives.\n\n    Returns:\n        kept (np.ndarray): Segmentation of regions with mean probability &gt;= threshold, relabeled sequentially.\n        removed (np.ndarray): Segmentation of regions with mean probability &lt; threshold, relabeled sequentially.\n    \"\"\"\n    # TODO: use `relabel_sequential` to recover the original labels\n\n    if segmentation.shape != foreground.shape:\n        raise ValueError(\"Segmentation and probability map must have the same shape.\")\n    if foreground.min() &lt; 0 or foreground.max() &gt; 1:\n        raise ValueError(\"Foreground must be a probability map with values in [0, 1].\")\n\n    instances, _, _ = relabel_sequential(\n        segmentation\n    )  # The label 0 is assumed to denote the bg and is never remapped.\n\n    kept = np.zeros_like(instances, dtype=instances.dtype)\n    removed = np.zeros_like(instances, dtype=instances.dtype)\n\n    # Measure and split\n    regions = regionprops(instances)\n    for region in tqdm.tqdm(regions, desc=\"Splitting regions\"):\n        bbox = region.bbox\n        if instances.ndim == 3:\n            slices = (\n                slice(bbox[0], bbox[3]),\n                slice(bbox[1], bbox[4]),\n                slice(bbox[2], bbox[5]),\n            )\n        else:\n            slices = (\n                slice(bbox[0], bbox[2]),\n                slice(bbox[1], bbox[3]),\n            )\n\n        mask = instances[slices] == region.label\n        prob = foreground[slices][mask]\n        mean_prob = prob.mean() if prob.size &gt; 0 else 0.0\n\n        if mean_prob &gt;= threshold:\n            kept[slices][mask] = region.label\n        else:\n            removed[slices][mask] = region.label\n\n    # Relabel outputs so labels are compact\n    kept, _, _ = relabel_sequential(kept)\n    removed, _, _ = relabel_sequential(removed)\n\n    return kept, removed\n</code></pre>"},{"location":"chapters/python_api/functionals/io/","title":"Input/Output","text":"<p>All the input/output operations are handled by the <code>plantseg.io</code> module. This module provides functions to read and write data in different formats. The supported formats are <code>tiff</code>, <code>h5</code>, and <code>zarr</code>, <code>jpeg</code>, <code>png</code>.</p>"},{"location":"chapters/python_api/functionals/io/#reading","title":"Reading","text":""},{"location":"chapters/python_api/functionals/io/#plantseg.io.smart_load","title":"<code>plantseg.io.smart_load(path: Path, key: str | None = None, default=load_tiff) -&gt; np.ndarray</code>","text":"<p>Load a dataset from a file. The loader is chosen based on the file extension. Supported formats are: tiff, h5, zarr, and PIL images. If the format is not supported, a default loader can be provided (default: load_tiff).</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>path to the file to load.</p> </li> <li> <code>key</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>key of the dataset to load (if h5 or zarr).</p> </li> <li> <code>default</code>               (<code>callable</code>, default:                   <code>load_tiff</code> )           \u2013            <p>default loader if the type is not understood.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>stack</code> (              <code>ndarray</code> )          \u2013            <p>numpy array with the image data.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = smart_load('path/to/file.tif')\n&gt;&gt;&gt; data = smart_load('path/to/file.h5', key='raw')\n</code></pre> Source code in <code>plantseg/io/io.py</code> <pre><code>def smart_load(path: Path, key: str | None = None, default=load_tiff) -&gt; np.ndarray:\n    \"\"\"\n    Load a dataset from a file. The loader is chosen based on the file extension.\n    Supported formats are: tiff, h5, zarr, and PIL images.\n    If the format is not supported, a default loader can be provided (default: load_tiff).\n\n    Args:\n        path (Path): path to the file to load.\n        key (str): key of the dataset to load (if h5 or zarr).\n        default (callable): default loader if the type is not understood.\n\n    Returns:\n        stack (np.ndarray): numpy array with the image data.\n\n    Examples:\n        &gt;&gt;&gt; data = smart_load('path/to/file.tif')\n        &gt;&gt;&gt; data = smart_load('path/to/file.h5', key='raw')\n\n    \"\"\"\n    ext = (path.suffix).lower()\n    if key == \"\":\n        key = None\n\n    if ext in H5_EXTENSIONS:\n        return load_h5(path, key)\n\n    elif ext in TIFF_EXTENSIONS:\n        return load_tiff(path)\n\n    elif ext in PIL_EXTENSIONS:\n        return load_pil(path)\n\n    elif ext in ZARR_EXTENSIONS:\n        return load_zarr(path, key)\n\n    else:\n        logger.warning(f\"No default found for {ext}, reverting to default loader.\")\n        return default(path)\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#writing","title":"Writing","text":""},{"location":"chapters/python_api/functionals/io/#plantseg.io.create_tiff","title":"<code>plantseg.io.create_tiff(path: Path, stack: np.ndarray, voxel_size: VoxelSize, layout: str = 'ZYX') -&gt; None</code>","text":"<p>Create a tiff file from a numpy array</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>path to save the tiff file</p> </li> <li> <code>stack</code>               (<code>ndarray</code>)           \u2013            <p>numpy array to save as tiff</p> </li> <li> <code>voxel_size</code>               (<code>list or tuple</code>)           \u2013            <p>tuple of the voxel size</p> </li> <li> <code>voxel_size_unit</code>               (<code>str</code>)           \u2013            <p>units of the voxel size</p> </li> </ul> Source code in <code>plantseg/io/tiff.py</code> <pre><code>def create_tiff(\n    path: Path, stack: np.ndarray, voxel_size: VoxelSize, layout: str = \"ZYX\"\n) -&gt; None:\n    \"\"\"\n    Create a tiff file from a numpy array\n\n    Args:\n        path (Path): path to save the tiff file\n        stack (np.ndarray): numpy array to save as tiff\n        voxel_size (list or tuple): tuple of the voxel size\n        voxel_size_unit (str): units of the voxel size\n\n    \"\"\"\n    # taken from: https://pypi.org/project/tifffile docs\n    # dimensions in TZCYXS order\n    if layout == \"ZYX\":\n        assert stack.ndim == 3, \"Stack dimensions must be in ZYX order\"\n        z, y, x = stack.shape\n        stack = stack.reshape(1, z, 1, y, x, 1)\n\n    elif layout == \"YX\":\n        assert stack.ndim == 2, \"Stack dimensions must be in YX order\"\n        y, x = stack.shape\n        stack = stack.reshape(1, 1, 1, y, x, 1)\n\n    elif layout == \"CYX\":\n        assert stack.ndim == 3, \"Stack dimensions must be in CYX order\"\n        c, y, x = stack.shape\n        stack = stack.reshape(1, 1, c, y, x, 1)\n\n    elif layout == \"ZCYX\":\n        assert stack.ndim == 4, \"Stack dimensions must be in ZCYX order\"\n        z, c, y, x = stack.shape\n        stack = stack.reshape(1, z, c, y, x, 1)\n\n    elif layout == \"CZYX\":\n        assert stack.ndim == 4, \"Stack dimensions must be in CZYX order\"\n        c, z, y, x = stack.shape\n        stack = stack.reshape(1, z, c, y, x, 1)\n\n    else:\n        raise ValueError(f\"Layout {layout} not supported\")\n\n    if voxel_size.voxels_size is not None:\n        assert len(voxel_size.voxels_size) == 3, (\n            \"Voxel size must have 3 elements (z, y, x)\"\n        )\n        spacing, y, x = voxel_size.voxels_size\n    else:\n        spacing, y, x = (1.0, 1.0, 1.0)\n\n    resolution = (1.0 / x, 1.0 / y)\n    # Save output results as tiff\n    tifffile.imwrite(\n        path,\n        data=stack,\n        dtype=stack.dtype,\n        imagej=True,\n        resolution=resolution,\n        metadata={\"axes\": \"TZCYXS\", \"spacing\": spacing, \"unit\": voxel_size.unit},\n        compression=\"zlib\",\n    )\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#plantseg.io.create_h5","title":"<code>plantseg.io.create_h5(path: Path, stack: np.ndarray, key: str, voxel_size: Optional[VoxelSize] = None, mode: str = 'a') -&gt; None</code>","text":"<p>Create a dataset inside a h5 file from a numpy array.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>path to the h5 file</p> </li> <li> <code>stack</code>               (<code>ndarray</code>)           \u2013            <p>numpy array to save as dataset in the h5 file.</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>key of the dataset in the h5 file.</p> </li> <li> <code>voxel_size</code>               (<code>VoxelSize</code>, default:                   <code>None</code> )           \u2013            <p>voxel size of the dataset.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'a'</code> )           \u2013            <p>mode to open the h5 file ['w', 'a'].</p> </li> </ul> Source code in <code>plantseg/io/h5.py</code> <pre><code>def create_h5(\n    path: Path,\n    stack: np.ndarray,\n    key: str,\n    voxel_size: Optional[VoxelSize] = None,\n    mode: str = \"a\",\n) -&gt; None:\n    \"\"\"\n    Create a dataset inside a h5 file from a numpy array.\n\n    Args:\n        path (Path): path to the h5 file\n        stack (np.ndarray): numpy array to save as dataset in the h5 file.\n        key (str): key of the dataset in the h5 file.\n        voxel_size (VoxelSize): voxel size of the dataset.\n        mode (str): mode to open the h5 file ['w', 'a'].\n\n    \"\"\"\n\n    if key is None:\n        raise ValueError(\"Key is required to create a dataset in a h5 file.\")\n\n    if key == \"\":\n        raise ValueError(\"Key cannot be empty to create a dataset in a h5 file.\")\n\n    with h5py.File(path, mode) as f:\n        if key in f:\n            del f[key]\n        f.create_dataset(key, data=stack, compression=\"gzip\")\n        # save voxel_size\n        if voxel_size is not None and voxel_size.voxels_size is not None:\n            f[key].attrs[\"element_size_um\"] = voxel_size.voxels_size\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#plantseg.io.create_zarr","title":"<code>plantseg.io.create_zarr(path: Path, stack: np.ndarray, key: str, voxel_size: VoxelSize, mode: str = 'a') -&gt; None</code>","text":"<p>Create a Zarr array from a NumPy array.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the Zarr file.</p> </li> <li> <code>stack</code>               (<code>ndarray</code>)           \u2013            <p>The NumPy array to save as a dataset.</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>The internal key of the desired dataset.</p> </li> <li> <code>voxel_size</code>               (<code>VoxelSize</code>)           \u2013            <p>The voxel size of the dataset.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'a'</code> )           \u2013            <p>The mode to open the Zarr file ['w', 'a'].</p> </li> </ul> Source code in <code>plantseg/io/zarr.py</code> <pre><code>def create_zarr(\n    path: Path,\n    stack: np.ndarray,\n    key: str,\n    voxel_size: VoxelSize,\n    mode: str = \"a\",\n) -&gt; None:\n    \"\"\"\n    Create a Zarr array from a NumPy array.\n\n    Args:\n        path (Path): The path to the Zarr file.\n        stack (np.ndarray): The NumPy array to save as a dataset.\n        key (str): The internal key of the desired dataset.\n        voxel_size (VoxelSize): The voxel size of the dataset.\n        mode (str): The mode to open the Zarr file ['w', 'a'].\n    \"\"\"\n    if not key:\n        raise ValueError(\"Key cannot be None or empty.\")\n\n    zarr_file = zarr.open_group(store=str(path), mode=mode)\n\n    if IS_ZARR_V3:\n        # v3 requires explicit `create_array`, and write manually\n        assert isinstance(zarr_file, zarr.Group), f\"Invalid Zarr file: {path}\"\n        zarr_file.create_array(name=key, shape=stack.shape, dtype=stack.dtype)\n        zarr_file[key][:] = stack\n    else:\n        # v2 allows `create_dataset` with `overwrite`\n        zarr_file.create_dataset(key, data=stack, compression=\"gzip\", overwrite=True)\n\n    zarr_file[key].attrs[\"element_size_um\"] = voxel_size.voxels_size\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#tiff-utilities","title":"Tiff Utilities","text":""},{"location":"chapters/python_api/functionals/io/#plantseg.io.tiff.read_tiff_voxel_size","title":"<code>plantseg.io.tiff.read_tiff_voxel_size(file_path: Path) -&gt; VoxelSize</code>","text":"<p>Returns the voxels size and the voxel units for imagej and ome style tiff (if absent returns [1, 1, 1], um)</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>path to the tiff file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>VoxelSize</code> (              <code>VoxelSize</code> )          \u2013            <p>voxel size and unit</p> </li> </ul> Source code in <code>plantseg/io/tiff.py</code> <pre><code>def read_tiff_voxel_size(file_path: Path) -&gt; VoxelSize:\n    \"\"\"\n    Returns the voxels size and the voxel units for imagej and ome style tiff (if absent returns [1, 1, 1], um)\n\n    Args:\n        file_path (Path): path to the tiff file\n\n    Returns:\n        VoxelSize: voxel size and unit\n\n    \"\"\"\n    with tifffile.TiffFile(file_path) as tiff:\n        if tiff.imagej_metadata is not None:\n            return _read_imagej_meta(tiff)\n\n        elif tiff.ome_metadata is not None:\n            return _read_ome_meta(tiff)\n\n        warnings.warn(\"No metadata found.\")\n        return VoxelSize()\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#h5-utilities","title":"H5 Utilities","text":""},{"location":"chapters/python_api/functionals/io/#plantseg.io.h5.list_h5_keys","title":"<code>plantseg.io.h5.list_h5_keys(path: Path) -&gt; list[str]</code>","text":"<p>List all keys in a h5 file</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>path to the h5 file (Path object)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>keys</code> (              <code>list[str]</code> )          \u2013            <p>A list of keys in the h5 file.</p> </li> </ul> Source code in <code>plantseg/io/h5.py</code> <pre><code>def list_h5_keys(path: Path) -&gt; list[str]:\n    \"\"\"\n    List all keys in a h5 file\n\n    Args:\n        path (Path): path to the h5 file (Path object)\n\n    Returns:\n        keys (list[str]): A list of keys in the h5 file.\n\n    \"\"\"\n    _validate_h5_file(path)\n\n    def _recursive_find_keys(f, base=\"/\"):\n        _list_keys = []\n        for key, dataset in f.items():\n            if isinstance(dataset, h5py.Group):\n                new_base = f\"{base}{key}/\"\n                _list_keys += _recursive_find_keys(dataset, new_base)\n\n            elif isinstance(dataset, h5py.Dataset):\n                _list_keys.append(f\"{base}{key}\")\n        return _list_keys\n\n    with h5py.File(path, \"r\") as h5_f:\n        return _recursive_find_keys(h5_f)\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#plantseg.io.h5.del_h5_key","title":"<code>plantseg.io.h5.del_h5_key(path: Path, key: str, mode: str = 'a') -&gt; None</code>","text":"<p>helper function to delete a dataset from a h5file</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>path to the h5 file (Path object)</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>key of the dataset to delete</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'a'</code> )           \u2013            <p>mode to open the h5 file ['r', 'r+']</p> </li> </ul> Source code in <code>plantseg/io/h5.py</code> <pre><code>def del_h5_key(path: Path, key: str, mode: str = \"a\") -&gt; None:\n    \"\"\"\n    helper function to delete a dataset from a h5file\n\n    Args:\n        path (Path): path to the h5 file (Path object)\n        key (str): key of the dataset to delete\n        mode (str): mode to open the h5 file ['r', 'r+']\n\n    \"\"\"\n    _validate_h5_file(path)\n    with h5py.File(path, mode) as f:\n        if key in f:\n            del f[key]\n            f.close()\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#plantseg.io.h5.rename_h5_key","title":"<code>plantseg.io.h5.rename_h5_key(path: Path, old_key: str, new_key: str, mode='r+') -&gt; None</code>","text":"<p>Rename the 'old_key' dataset to 'new_key'</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>path to the h5 file (Path object)</p> </li> <li> <code>old_key</code>               (<code>str</code>)           \u2013            <p>old key name</p> </li> <li> <code>new_key</code>               (<code>str</code>)           \u2013            <p>new key name</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'r+'</code> )           \u2013            <p>mode to open the h5 file ['r', 'r+']</p> </li> </ul> Source code in <code>plantseg/io/h5.py</code> <pre><code>def rename_h5_key(path: Path, old_key: str, new_key: str, mode=\"r+\") -&gt; None:\n    \"\"\"\n    Rename the 'old_key' dataset to 'new_key'\n\n    Args:\n        path (Path): path to the h5 file (Path object)\n        old_key (str): old key name\n        new_key (str): new key name\n        mode (str): mode to open the h5 file ['r', 'r+']\n\n    \"\"\"\n    _validate_h5_file(path)\n    with h5py.File(path, mode) as f:\n        if old_key in f:\n            f[new_key] = f[old_key]\n            del f[old_key]\n            f.close()\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#zarr-utilities","title":"Zarr Utilities","text":""},{"location":"chapters/python_api/functionals/io/#plantseg.io.zarr.list_zarr_keys","title":"<code>plantseg.io.zarr.list_zarr_keys(path: Path) -&gt; list[str]</code>","text":"<p>List all keys in a Zarr file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the Zarr file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of keys in the Zarr file.</p> </li> </ul> Source code in <code>plantseg/io/zarr.py</code> <pre><code>def list_zarr_keys(path: Path) -&gt; list[str]:\n    \"\"\"\n    List all keys in a Zarr file.\n\n    Args:\n        path (Path): The path to the Zarr file.\n\n    Returns:\n        list[str]: A list of keys in the Zarr file.\n    \"\"\"\n\n    def _recursive_find_keys(\n        zarr_group: zarr.Group, base: Path = Path(\"\")\n    ) -&gt; list[str]:\n        _list_keys = []\n        for key, dataset in zarr_group.members():\n            if isinstance(dataset, zarr.Group):\n                new_base = base / key\n                _list_keys.extend(_recursive_find_keys(dataset, new_base))\n            elif isinstance(dataset, zarr.Array):\n                _list_keys.append(str(base / key))\n        return _list_keys\n\n    zarr_file = zarr.open_group(store=path, mode=\"r\")\n    return _recursive_find_keys(zarr_file)\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#plantseg.io.zarr.del_zarr_key","title":"<code>plantseg.io.zarr.del_zarr_key(path: Path, key: str, mode: str = 'a') -&gt; None</code>","text":"<p>Delete a dataset from a Zarr file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the Zarr file.</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>The internal key of the dataset to be deleted.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'a'</code> )           \u2013            <p>The mode to open the Zarr file ['w', 'a'].</p> </li> </ul> Source code in <code>plantseg/io/zarr.py</code> <pre><code>def del_zarr_key(path: Path, key: str, mode: str = \"a\") -&gt; None:\n    \"\"\"\n    Delete a dataset from a Zarr file.\n\n    Args:\n        path (Path): The path to the Zarr file.\n        key (str): The internal key of the dataset to be deleted.\n        mode (str): The mode to open the Zarr file ['w', 'a'].\n\n    \"\"\"\n    zarr_file = zarr.open_group(store=path, mode=mode)\n    if key in zarr_file:\n        del zarr_file[key]\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#plantseg.io.zarr.rename_zarr_key","title":"<code>plantseg.io.zarr.rename_zarr_key(path: Path, old_key: str, new_key: str, mode: str = 'r+') -&gt; None</code>","text":"<p>Rename a dataset in a Zarr file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the Zarr file.</p> </li> <li> <code>old_key</code>               (<code>str</code>)           \u2013            <p>The current key of the dataset.</p> </li> <li> <code>new_key</code>               (<code>str</code>)           \u2013            <p>The new key for the dataset.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'r+'</code> )           \u2013            <p>The mode to open the Zarr file ['r+'].</p> </li> </ul> Source code in <code>plantseg/io/zarr.py</code> <pre><code>def rename_zarr_key(path: Path, old_key: str, new_key: str, mode: str = \"r+\") -&gt; None:\n    \"\"\"\n    Rename a dataset in a Zarr file.\n\n    Args:\n        path (Path): The path to the Zarr file.\n        old_key (str): The current key of the dataset.\n        new_key (str): The new key for the dataset.\n        mode (str): The mode to open the Zarr file ['r+'].\n\n    \"\"\"\n    zarr_file = zarr.open_group(store=path, mode=mode)\n    if old_key in zarr_file:\n        zarr_file[new_key] = zarr_file[old_key]\n        del zarr_file[old_key]\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/","title":"PlantSeg Segmentation","text":"<p>The PlantSeg segmentation module implements all segmentation routine in plantseg.</p>"},{"location":"chapters/python_api/functionals/segmentation/#dt-watershed","title":"DT Watershed","text":""},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.dt_watershed","title":"<code>plantseg.functionals.segmentation.dt_watershed(boundary_pmaps: np.ndarray, threshold: float = 0.5, sigma_seeds: float = 1.0, stacked: bool = False, sigma_weights: float = 2.0, min_size: int = 100, alpha: float = 1.0, pixel_pitch: Optional[tuple[int, ...]] = None, apply_nonmax_suppression: bool = False, n_threads: Optional[int] = None, mask: Optional[np.ndarray] = None) -&gt; np.ndarray</code>","text":"<p>Performs watershed segmentation using distance transforms on boundary probability maps.</p> <p>This function applies the distance transform watershed algorithm to the input boundary probability maps, either slice-by-slice or in original shape depending on the 'stacked' parameter. The watershed method is applied to the boundary probability maps with optional pre-processing like thresholding, smoothing, and masking.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>Input array of boundary probability maps, often obtained from deep learning models. Each pixel/voxel value represents the probability of being part of a boundary.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold value applied to the boundary probability map before computing the distance transform. Values below this threshold are considered background. Defaults to 0.5.</p> </li> <li> <code>sigma_seeds</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Standard deviation for Gaussian smoothing applied to the seed map (used for initializing the watershed regions). Higher values result in more smoothed seeds. Defaults to 1.0.</p> </li> <li> <code>stacked</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, performs watershed segmentation on each 2D slice of a 3D volume independently (slice-by-slice). If False, performs watershed segmentation in 3D for volumetric data or 2D for 2D input. Defaults to False.</p> </li> <li> <code>sigma_weights</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Standard deviation for Gaussian smoothing applied to the weight map. The weight map combines the distance transform and input map to guide the watershed. Larger values result in smoother weight maps. Defaults to 2.0.</p> </li> <li> <code>min_size</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Minimum size of the segmented regions to retain. Regions smaller than this size are removed. Defaults to 100.</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Blending factor to combine the input boundary probability maps and the distance transform when constructing the weight map. A higher alpha prioritizes the input maps. Defaults to 1.0.</p> </li> <li> <code>pixel_pitch</code>               (<code>Optional[tuple[int, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Voxel anisotropy factors (e.g., spacing along different axes) to use during the distance transform. If None, the distances are computed isotropically. For anisotropic volumes, this should match the voxel spacing. Defaults to None.</p> </li> <li> <code>apply_nonmax_suppression</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, applies non-maximum suppression to the detected seeds, reducing seed redundancy. This requires the Nifty library. Defaults to False.</p> </li> <li> <code>n_threads</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of threads to use for parallel processing in 2D mode (stacked mode). If None, the default number of threads will be used. Defaults to None.</p> </li> <li> <code>mask</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>A binary mask that excludes certain regions from segmentation. Only regions within the mask will be considered. If None, all regions are included. Must have the same shape as 'boundary_pmaps'. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: A labeled segmentation map where each region is assigned a unique label.</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def dt_watershed(\n    boundary_pmaps: np.ndarray,\n    threshold: float = 0.5,\n    sigma_seeds: float = 1.0,\n    stacked: bool = False,\n    sigma_weights: float = 2.0,\n    min_size: int = 100,\n    alpha: float = 1.0,\n    pixel_pitch: Optional[tuple[int, ...]] = None,\n    apply_nonmax_suppression: bool = False,\n    n_threads: Optional[int] = None,\n    mask: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"Performs watershed segmentation using distance transforms on boundary probability maps.\n\n    This function applies the distance transform watershed algorithm to the input boundary\n    probability maps, either slice-by-slice or in original shape depending on the 'stacked' parameter.\n    The watershed method is applied to the boundary probability maps with optional pre-processing\n    like thresholding, smoothing, and masking.\n\n    Args:\n        boundary_pmaps (np.ndarray): Input array of boundary probability maps, often obtained\n            from deep learning models. Each pixel/voxel value represents the probability of\n            being part of a boundary.\n        threshold (float, optional): Threshold value applied to the boundary probability map\n            before computing the distance transform. Values below this threshold are considered\n            background. Defaults to 0.5.\n        sigma_seeds (float, optional): Standard deviation for Gaussian smoothing applied to\n            the seed map (used for initializing the watershed regions). Higher values result\n            in more smoothed seeds. Defaults to 1.0.\n        stacked (bool, optional): If True, performs watershed segmentation on each 2D slice of\n            a 3D volume independently (slice-by-slice). If False, performs watershed segmentation\n            in 3D for volumetric data or 2D for 2D input. Defaults to False.\n        sigma_weights (float, optional): Standard deviation for Gaussian smoothing applied\n            to the weight map. The weight map combines the distance transform and input map\n            to guide the watershed. Larger values result in smoother weight maps. Defaults to 2.0.\n        min_size (int, optional): Minimum size of the segmented regions to retain. Regions\n            smaller than this size are removed. Defaults to 100.\n        alpha (float, optional): Blending factor to combine the input boundary probability maps\n            and the distance transform when constructing the weight map. A higher alpha\n            prioritizes the input maps. Defaults to 1.0.\n        pixel_pitch (Optional[tuple[int, ...]], optional): Voxel anisotropy factors (e.g., spacing\n            along different axes) to use during the distance transform. If None, the distances are\n            computed isotropically. For anisotropic volumes, this should match the voxel spacing.\n            Defaults to None.\n        apply_nonmax_suppression (bool, optional): If True, applies non-maximum suppression to\n            the detected seeds, reducing seed redundancy. This requires the Nifty library.\n            Defaults to False.\n        n_threads (Optional[int], optional): Number of threads to use for parallel processing in\n            2D mode (stacked mode). If None, the default number of threads will be used.\n            Defaults to None.\n        mask (Optional[np.ndarray], optional): A binary mask that excludes certain regions from\n            segmentation. Only regions within the mask will be considered. If None, all regions\n            are included. Must have the same shape as 'boundary_pmaps'. Defaults to None.\n\n    Returns:\n        np.ndarray: A labeled segmentation map where each region is assigned a unique label.\n\n    \"\"\"\n    # Prepare the keyword arguments for the watershed function\n    boundary_pmaps = boundary_pmaps.astype(\"float32\")\n    ws_kwargs = {\n        \"threshold\": threshold,\n        \"sigma_seeds\": sigma_seeds,\n        \"sigma_weights\": sigma_weights,\n        \"min_size\": min_size,\n        \"alpha\": alpha,\n        \"pixel_pitch\": pixel_pitch,\n        \"apply_nonmax_suppression\": apply_nonmax_suppression,\n        \"mask\": mask,\n    }\n    if stacked:\n        # Apply watershed slice by slice (for 3D data)\n        segmentation, _ = stacked_watershed(\n            boundary_pmaps,\n            ws_function=distance_transform_watershed,\n            n_threads=n_threads,\n            **ws_kwargs,\n        )\n    else:\n        # Apply watershed in 3D for 3D data or in 2D for 2D data\n        segmentation, _ = distance_transform_watershed(boundary_pmaps, **ws_kwargs)\n\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/#gasp","title":"GASP","text":""},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.gasp","title":"<code>plantseg.functionals.segmentation.gasp(boundary_pmaps: np.ndarray, superpixels: Optional[np.ndarray] = None, gasp_linkage_criteria: str = 'average', beta: float = 0.5, post_minsize: int = 100, n_threads: int = 6) -&gt; np.ndarray</code>","text":"<p>Perform segmentation using the GASP algorithm with affinity maps.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>Cell boundary prediction.</p> </li> <li> <code>superpixels</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>Superpixel segmentation. If None, GASP will be run from the pixels. Default is None.</p> </li> <li> <code>gasp_linkage_criteria</code>               (<code>str</code>, default:                   <code>'average'</code> )           \u2013            <p>Linkage criteria for GASP. Default is 'average'.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Beta parameter for GASP. Small values steer towards under-segmentation, while high values bias towards over-segmentation. Default is 0.5.</p> </li> <li> <code>post_minsize</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Minimum size of the segments after GASP. Default is 100.</p> </li> <li> <code>n_threads</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of threads used for GASP. Default is 6.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: GASP output segmentation.</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def gasp(\n    boundary_pmaps: np.ndarray,\n    superpixels: Optional[np.ndarray] = None,\n    gasp_linkage_criteria: str = \"average\",\n    beta: float = 0.5,\n    post_minsize: int = 100,\n    n_threads: int = 6,\n) -&gt; np.ndarray:\n    \"\"\"\n    Perform segmentation using the GASP algorithm with affinity maps.\n\n    Args:\n        boundary_pmaps (np.ndarray): Cell boundary prediction.\n        superpixels (Optional[np.ndarray]): Superpixel segmentation. If None, GASP will be run from the pixels. Default is None.\n        gasp_linkage_criteria (str): Linkage criteria for GASP. Default is 'average'.\n        beta (float): Beta parameter for GASP. Small values steer towards under-segmentation, while high values bias towards over-segmentation. Default is 0.5.\n        post_minsize (int): Minimum size of the segments after GASP. Default is 100.\n        n_threads (int): Number of threads used for GASP. Default is 6.\n\n    Returns:\n        np.ndarray: GASP output segmentation.\n    \"\"\"\n    remove_singleton = False\n    if superpixels is not None:\n        assert boundary_pmaps.shape == superpixels.shape, (\n            \"Shape mismatch between boundary_pmaps and superpixels.\"\n        )\n        if superpixels.ndim == 2:  # Ensure superpixels is 3D if provided\n            superpixels = superpixels[None, ...]\n            boundary_pmaps = boundary_pmaps[None, ...]\n            remove_singleton = True\n\n    # Prepare the arguments for running GASP\n    run_GASP_kwargs = {\n        \"linkage_criteria\": gasp_linkage_criteria,\n        \"add_cannot_link_constraints\": False,\n        \"use_efficient_implementations\": False,\n    }\n\n    # Interpret boundary_pmaps as affinities and prepare for GASP\n    boundary_pmaps = boundary_pmaps.astype(\"float32\")\n    affinities = np.stack([boundary_pmaps] * 3, axis=0)\n\n    offsets = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n    # Shift is required to correct aligned affinities\n    affinities = shift_affinities(affinities, offsets=offsets)\n\n    # invert affinities\n    affinities = 1 - affinities\n\n    # Initialize and run GASP\n    gasp_instance = GaspFromAffinities(\n        offsets,\n        superpixel_generator=None\n        if superpixels is None\n        else (lambda *args, **kwargs: superpixels),\n        run_GASP_kwargs=run_GASP_kwargs,\n        n_threads=n_threads,\n        beta_bias=beta,\n    )\n    segmentation, _ = gasp_instance(affinities)\n\n    # Apply size filtering if specified\n    if post_minsize &gt; 0:\n        segmentation, _ = apply_size_filter(\n            segmentation.astype(\"uint32\"), boundary_pmaps, post_minsize\n        )\n\n    if remove_singleton:\n        segmentation = segmentation[0]\n\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/#multicut","title":"Multicut","text":""},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.multicut","title":"<code>plantseg.functionals.segmentation.multicut(boundary_pmaps: np.ndarray, superpixels: np.ndarray, beta: float = 0.5, post_minsize: int = 50) -&gt; np.ndarray</code>","text":"<p>Multicut segmentation from boundary prediction.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>cell boundary prediction, 3D array of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>superpixels</code>               (<code>ndarray</code>)           \u2013            <p>superpixel segmentation. Must have the same shape as boundary_pmaps.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter for the Multicut. A small value will steer the segmentation towards under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)</p> </li> <li> <code>post_minsize</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>minimal size of the segments after Multicut. (default: 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>Multicut output segmentation</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def multicut(\n    boundary_pmaps: np.ndarray,\n    superpixels: np.ndarray,\n    beta: float = 0.5,\n    post_minsize: int = 50,\n) -&gt; np.ndarray:\n    \"\"\"\n    Multicut segmentation from boundary prediction.\n\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary prediction, 3D array of shape (Z, Y, X) with values between 0 and 1.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n        beta (float): beta parameter for the Multicut. A small value will steer the segmentation towards\n            under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after Multicut. (default: 100)\n\n    Returns:\n        segmentation (np.ndarray): Multicut output segmentation\n    \"\"\"\n\n    rag = compute_rag(superpixels)\n\n    # Prob -&gt; edge costs\n    boundary_pmaps = boundary_pmaps.astype(\"float32\")\n    costs = compute_mc_costs(boundary_pmaps, rag, beta=beta)\n\n    # Creating graph\n    graph = nifty.graph.undirectedGraph(rag.numberOfNodes)\n    graph.insertEdges(rag.uvIds())\n\n    # Solving Multicut\n    node_labels = multicut_kernighan_lin(graph, costs)\n    segmentation = nifty.tools.take(node_labels, superpixels)\n\n    # run size threshold\n    if post_minsize &gt; 0:\n        segmentation, _ = apply_size_filter(\n            segmentation.astype(\"uint32\"), boundary_pmaps, post_minsize\n        )\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/#mutex-watershed","title":"Mutex Watershed","text":""},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.mutex_ws","title":"<code>plantseg.functionals.segmentation.mutex_ws(boundary_pmaps: np.ndarray, superpixels: Optional[np.ndarray] = None, beta: float = 0.5, post_minsize: int = 100, n_threads: int = 6) -&gt; np.ndarray</code>","text":"<p>Wrapper around gasp with mutex_watershed as linkage criteria.</p> <p>Args:magicgui     boundary_pmaps (np.ndarray): cell boundary prediction. 3D array of shape (Z, Y, X) with values between 0 and 1.     superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.         If None, GASP will be run from the pixels. (default: None)     beta (float): beta parameter for GASP. A small value will steer the segmentation towards under-segmentation.         While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)     post_minsize (int): minimal size of the segments after GASP. (default: 100)     n_threads (int): number of threads used for GASP. (default: 6)</p> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>MutexWS output segmentation</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def mutex_ws(\n    boundary_pmaps: np.ndarray,\n    superpixels: Optional[np.ndarray] = None,\n    beta: float = 0.5,\n    post_minsize: int = 100,\n    n_threads: int = 6,\n) -&gt; np.ndarray:\n    \"\"\"\n    Wrapper around gasp with mutex_watershed as linkage criteria.\n\n    Args:magicgui\n        boundary_pmaps (np.ndarray): cell boundary prediction. 3D array of shape (Z, Y, X) with values between 0 and 1.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n            If None, GASP will be run from the pixels. (default: None)\n        beta (float): beta parameter for GASP. A small value will steer the segmentation towards under-segmentation.\n            While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after GASP. (default: 100)\n        n_threads (int): number of threads used for GASP. (default: 6)\n\n    Returns:\n        segmentation (np.ndarray): MutexWS output segmentation\n\n    \"\"\"\n    return gasp(\n        boundary_pmaps=boundary_pmaps,\n        superpixels=superpixels,\n        gasp_linkage_criteria=\"mutex_watershed\",\n        beta=beta,\n        post_minsize=post_minsize,\n        n_threads=n_threads,\n    )\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/#lifted-multicut","title":"Lifted Multicut","text":""},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.segmentation.lifted_multicut_from_nuclei_pmaps","title":"<code>plantseg.functionals.segmentation.segmentation.lifted_multicut_from_nuclei_pmaps(boundary_pmaps: np.ndarray, nuclei_pmaps: np.ndarray, superpixels: np.ndarray, beta: float = 0.5, post_minsize: int = 50) -&gt; np.ndarray</code>","text":"<p>Lifted Multicut segmentation from boundary prediction and nuclei prediction.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>cell boundary prediction, 3D array of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>nuclei_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>nuclei prediction. Must have the same shape as boundary_pmaps and with values between 0 and 1.</p> </li> <li> <code>superpixels</code>               (<code>ndarray</code>)           \u2013            <p>superpixel segmentation. Must have the same shape as boundary_pmaps.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter for the Multicut. A small value will steer the segmentation towards under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)</p> </li> <li> <code>post_minsize</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>minimal size of the segments after Multicut. (default: 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>Multicut output segmentation</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def lifted_multicut_from_nuclei_pmaps(\n    boundary_pmaps: np.ndarray,\n    nuclei_pmaps: np.ndarray,\n    superpixels: np.ndarray,\n    beta: float = 0.5,\n    post_minsize: int = 50,\n) -&gt; np.ndarray:\n    \"\"\"\n    Lifted Multicut segmentation from boundary prediction and nuclei prediction.\n\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary prediction, 3D array of shape (Z, Y, X) with values between 0 and 1.\n        nuclei_pmaps (np.ndarray): nuclei prediction. Must have the same shape as boundary_pmaps and\n            with values between 0 and 1.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n        beta (float): beta parameter for the Multicut. A small value will steer the segmentation towards\n            under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after Multicut. (default: 100)\n\n    Returns:\n        segmentation (np.ndarray): Multicut output segmentation\n    \"\"\"\n    if nuclei_pmaps.max() &gt; 1 or nuclei_pmaps.min() &lt; 0:\n        raise ValueError(\"nuclei_pmaps should be between 0 and 1\")\n\n    # compute the region adjacency graph\n    rag = compute_rag(superpixels)\n\n    # compute multi cut edges costs\n    boundary_pmaps = boundary_pmaps.astype(\"float32\")\n    costs = compute_mc_costs(boundary_pmaps, rag, beta)\n\n    # assert nuclei pmaps are floats\n    nuclei_pmaps = nuclei_pmaps.astype(\"float32\")\n    input_maps = [nuclei_pmaps]\n    assignment_threshold = 0.9\n\n    # compute lifted multicut features from boundary pmaps\n    lifted_uvs, lifted_costs = lifted_problem_from_probabilities(\n        rag,\n        superpixels.astype(\"uint32\"),\n        input_maps,\n        assignment_threshold,\n        graph_depth=4,\n    )\n\n    # solve the full lifted problem using the kernighan lin approximation introduced in\n    # http://openaccess.thecvf.com/content_iccv_2015/html/Keuper_Efficient_Decomposition_of_ICCV_2015_paper.html\n    node_labels = lmc.lifted_multicut_kernighan_lin(\n        rag, costs, lifted_uvs, lifted_costs\n    )\n    segmentation = project_node_labels_to_pixels(rag, node_labels)\n\n    # run size threshold\n    if post_minsize &gt; 0:\n        segmentation, _ = apply_size_filter(\n            segmentation.astype(\"uint32\"), boundary_pmaps, post_minsize\n        )\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.lifted_multicut_from_nuclei_segmentation","title":"<code>plantseg.functionals.segmentation.lifted_multicut_from_nuclei_segmentation(boundary_pmaps: np.ndarray, nuclei_seg: np.ndarray, superpixels: np.ndarray, beta: float = 0.5, post_minsize: int = 50) -&gt; np.ndarray</code>","text":"<p>Lifted Multicut segmentation from boundary prediction and nuclei segmentation.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>cell boundary prediction, 3D array of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>nuclei_seg</code>               (<code>ndarray</code>)           \u2013            <p>Nuclei segmentation. Must have the same shape as boundary_pmaps.</p> </li> <li> <code>superpixels</code>               (<code>ndarray</code>)           \u2013            <p>superpixel segmentation. Must have the same shape as boundary_pmaps.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter for the Multicut. A small value will steer the segmentation towards under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)</p> </li> <li> <code>post_minsize</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>minimal size of the segments after Multicut. (default: 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>Multicut output segmentation</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def lifted_multicut_from_nuclei_segmentation(\n    boundary_pmaps: np.ndarray,\n    nuclei_seg: np.ndarray,\n    superpixels: np.ndarray,\n    beta: float = 0.5,\n    post_minsize: int = 50,\n) -&gt; np.ndarray:\n    \"\"\"\n    Lifted Multicut segmentation from boundary prediction and nuclei segmentation.\n\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary prediction, 3D array of shape (Z, Y, X) with values between 0 and 1.\n        nuclei_seg (np.ndarray): Nuclei segmentation. Must have the same shape as boundary_pmaps.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n        beta (float): beta parameter for the Multicut. A small value will steer the segmentation towards\n            under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after Multicut. (default: 100)\n\n    Returns:\n        segmentation (np.ndarray): Multicut output segmentation\n    \"\"\"\n    # compute the region adjacency graph\n    rag = compute_rag(superpixels)\n\n    # compute multi cut edges costs\n    boundary_pmaps = boundary_pmaps.astype(\"float32\")\n    costs = compute_mc_costs(boundary_pmaps, rag, beta)\n    max_cost = np.abs(np.max(costs))\n    lifted_uvs, lifted_costs = lifted_problem_from_segmentation(\n        rag,\n        superpixels,\n        nuclei_seg,\n        overlap_threshold=0.2,\n        graph_depth=4,\n        same_segment_cost=5 * max_cost,\n        different_segment_cost=-5 * max_cost,\n    )\n\n    # solve the full lifted problem using the kernighan lin approximation introduced in\n    # http://openaccess.thecvf.com/content_iccv_2015/html/Keuper_Efficient_Decomposition_of_ICCV_2015_paper.html\n    lifted_costs = lifted_costs.astype(\"float64\")\n    node_labels = lmc.lifted_multicut_kernighan_lin(\n        rag, costs, lifted_uvs, lifted_costs\n    )\n    segmentation = project_node_labels_to_pixels(rag, node_labels)\n\n    # run size threshold\n    if post_minsize &gt; 0:\n        segmentation, _ = apply_size_filter(\n            segmentation.astype(\"uint32\"), boundary_pmaps, post_minsize\n        )\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/#simple-itk-watershed","title":"Simple ITK Watershed","text":""},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.simple_itk_watershed","title":"<code>plantseg.functionals.segmentation.simple_itk_watershed(boundary_pmaps: np.ndarray, threshold: float = 0.5, sigma: float = 1.0, minsize: int = 100) -&gt; np.ndarray</code>","text":"<p>Simple itk watershed segmentation.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>cell boundary prediction. 3D array of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>threshold for the watershed segmentation. (default: 0.5)</p> </li> <li> <code>sigma</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>sigma for the gaussian smoothing. (default: 1.0)</p> </li> <li> <code>minsize</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>minimal size of the segments after segmentation. (default: 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>watershed output segmentation (using SimpleITK)</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def simple_itk_watershed(\n    boundary_pmaps: np.ndarray,\n    threshold: float = 0.5,\n    sigma: float = 1.0,\n    minsize: int = 100,\n) -&gt; np.ndarray:\n    \"\"\"\n    Simple itk watershed segmentation.\n\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary prediction. 3D array of shape (Z, Y, X) with values between 0 and 1.\n        threshold (float): threshold for the watershed segmentation. (default: 0.5)\n        sigma (float): sigma for the gaussian smoothing. (default: 1.0)\n        minsize (int): minimal size of the segments after segmentation. (default: 100)\n\n    Returns:\n        segmentation (np.ndarray): watershed output segmentation (using SimpleITK)\n\n    \"\"\"\n    if not SIMPLE_ITK_INSTALLED:\n        raise ValueError(\"please install sitk before running this process\")\n\n    if sigma &gt; 0:\n        # fix ws sigma length\n        # ws sigma cannot be shorter than pmaps dims\n        max_sigma = (np.array(boundary_pmaps.shape) - 1) / 3\n        ws_sigma = np.minimum(max_sigma, np.ones(max_sigma.ndim) * sigma)\n        boundary_pmaps = gaussianSmoothing(boundary_pmaps, ws_sigma)\n\n    # Itk watershed + size filtering\n    itk_pmaps = sitk.GetImageFromArray(boundary_pmaps)\n    itk_segmentation = sitk.MorphologicalWatershed(\n        itk_pmaps, threshold, markWatershedLine=False, fullyConnected=False\n    )\n    itk_segmentation = sitk.RelabelComponent(itk_segmentation, minsize)\n    segmentation = sitk.GetArrayFromImage(itk_segmentation).astype(np.uint16)\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/","title":"Data Processing Tasks","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#image-preprocessing-tasks","title":"Image Preprocessing Tasks","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#gaussian-smoothing-task","title":"Gaussian smoothing task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.gaussian_smoothing_task","title":"<code>plantseg.tasks.dataprocessing_tasks.gaussian_smoothing_task(image: PlantSegImage, sigma: float) -&gt; PlantSegImage</code>","text":"<p>Apply Gaussian smoothing to a PlantSegImage object.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image</p> </li> <li> <code>sigma</code>               (<code>float</code>)           \u2013            <p>standard deviation of the Gaussian kernel</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef gaussian_smoothing_task(image: PlantSegImage, sigma: float) -&gt; PlantSegImage:\n    \"\"\"\n    Apply Gaussian smoothing to a PlantSegImage object.\n\n    Args:\n        image (PlantSegImage): input image\n        sigma (float): standard deviation of the Gaussian kernel\n\n    \"\"\"\n    if image.is_multichannel:\n        raise ValueError(\"Gaussian smoothing is not supported for multichannel images.\")\n\n    data = image.get_data()\n    smoothed_data = image_gaussian_smoothing(data, sigma=sigma)\n    new_image = image.derive_new(smoothed_data, name=f\"{image.name}_smoothed\")\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#image-cropping-task","title":"Image cropping task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.image_cropping_task","title":"<code>plantseg.tasks.dataprocessing_tasks.image_cropping_task(image: PlantSegImage, rectangle=None, crop_z: tuple[int, int] = (0, 100)) -&gt; PlantSegImage</code>","text":"<p>Crop the image based on the given rectangle and z-slices.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>The image to be cropped.</p> </li> <li> <code>rectangle</code>               (<code>Optional</code>, default:                   <code>None</code> )           \u2013            <p>Rectangle defining the region to crop.</p> </li> <li> <code>crop_z</code>               (<code>tuple[int, int]</code>, default:                   <code>(0, 100)</code> )           \u2013            <p>Z-slice range for cropping.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PlantSegImage</code> (              <code>PlantSegImage</code> )          \u2013            <p>The cropped image.</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef image_cropping_task(\n    image: PlantSegImage, rectangle=None, crop_z: tuple[int, int] = (0, 100)\n) -&gt; PlantSegImage:\n    \"\"\"\n    Crop the image based on the given rectangle and z-slices.\n\n    Args:\n        image (PlantSegImage): The image to be cropped.\n        rectangle (Optional): Rectangle defining the region to crop.\n        crop_z (tuple[int, int]): Z-slice range for cropping.\n\n    Returns:\n        PlantSegImage: The cropped image.\n    \"\"\"\n    data = image.get_data()\n\n    # Compute crop slices\n    if image.dimensionality == ImageDimensionality.TWO:\n        crop_slices = _compute_slices_2d(rectangle, data.shape)\n    else:\n        crop_slices = _compute_slices_3d(rectangle, crop_z, data.shape)\n\n    # Perform cropping on the data\n    cropped_data = _cropping(data, crop_slices)\n\n    # Create and return a new PlantSegImage object from the cropped data\n    cropped_image = image.derive_new(cropped_data, name=f\"{image.name}_cropped\")\n\n    return cropped_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#image-rescale-to-shape-task","title":"Image rescale to shape task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.image_rescale_to_shape_task","title":"<code>plantseg.tasks.dataprocessing_tasks.image_rescale_to_shape_task(image: PlantSegImage, new_shape: tuple[int, ...], order: int = 0) -&gt; PlantSegImage</code>","text":"<p>Rescale an image to a new shape.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image</p> </li> <li> <code>new_shape</code>               (<code>tuple[int, ...]</code>)           \u2013            <p>new shape of the image</p> </li> <li> <code>order</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>order of the interpolation</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef image_rescale_to_shape_task(\n    image: PlantSegImage, new_shape: tuple[int, ...], order: int = 0\n) -&gt; PlantSegImage:\n    \"\"\"Rescale an image to a new shape.\n\n    Args:\n        image (PlantSegImage): input image\n        new_shape (tuple[int, ...]): new shape of the image\n        order (int): order of the interpolation\n    \"\"\"\n    if image.image_layout == ImageLayout.YX:\n        scaling_factor = (new_shape[1] / image.shape[0], new_shape[2] / image.shape[1])\n        spatial_scaling_factor = (1.0, scaling_factor[0], scaling_factor[1])\n    elif image.image_layout == ImageLayout.ZYX:\n        scaling_factor = (\n            new_shape[0] / image.shape[0],\n            new_shape[1] / image.shape[1],\n            new_shape[2] / image.shape[2],\n        )\n        spatial_scaling_factor = scaling_factor\n    elif image.image_layout == ImageLayout.CYX:\n        scaling_factor = (\n            1.0,\n            new_shape[1] / image.shape[1],\n            new_shape[2] / image.shape[2],\n        )\n        spatial_scaling_factor = (1.0, scaling_factor[1], scaling_factor[2])\n    elif image.image_layout == ImageLayout.CZYX:\n        scaling_factor = (\n            1.0,\n            new_shape[0] / image.shape[1],\n            new_shape[1] / image.shape[2],\n            new_shape[2] / image.shape[3],\n        )\n        spatial_scaling_factor = scaling_factor[1:]\n    elif image.image_layout == ImageLayout.ZCYX:\n        scaling_factor = (\n            new_shape[0] / image.shape[0],\n            1.0,\n            new_shape[1] / image.shape[2],\n            new_shape[2] / image.shape[3],\n        )\n        spatial_scaling_factor = (\n            scaling_factor[0],\n            scaling_factor[2],\n            scaling_factor[3],\n        )\n\n    out_data = image_rescale(image.get_data(), scaling_factor, order=order)\n\n    if image.has_valid_voxel_size():\n        out_voxel_size = image.voxel_size.voxelsize_from_factor(spatial_scaling_factor)\n    else:\n        out_voxel_size = VoxelSize()\n\n    new_image = image.derive_new(\n        out_data, name=f\"{image.name}_reshaped\", voxel_size=out_voxel_size\n    )\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#image-rescale-to-voxel-size-task","title":"Image rescale to voxel size task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.image_rescale_to_voxel_size_task","title":"<code>plantseg.tasks.dataprocessing_tasks.image_rescale_to_voxel_size_task(image: PlantSegImage, new_voxels_size: tuple[float, float, float], new_unit: str, order: int = 0) -&gt; PlantSegImage</code>","text":"<p>Rescale an image to a new voxel size.</p> <p>If the voxel size is not defined in the input image, use the set voxel size task to set the voxel size.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image</p> </li> <li> <code>new_voxel_size</code>               (<code>VoxelSize</code>)           \u2013            <p>new voxel size</p> </li> <li> <code>order</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>order of the interpolation</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef image_rescale_to_voxel_size_task(\n    image: PlantSegImage,\n    new_voxels_size: tuple[float, float, float],\n    new_unit: str,\n    order: int = 0,\n) -&gt; PlantSegImage:\n    \"\"\"Rescale an image to a new voxel size.\n\n    If the voxel size is not defined in the input image, use the set voxel size task to set the voxel size.\n\n    Args:\n        image (PlantSegImage): input image\n        new_voxel_size (VoxelSize): new voxel size\n        order (int): order of the interpolation\n\n    \"\"\"\n    new_voxel_size = VoxelSize(voxels_size=new_voxels_size, unit=new_unit)\n    spatial_scaling_factor = image.voxel_size.scalefactor_from_voxelsize(new_voxel_size)\n\n    if image.image_layout == ImageLayout.YX:\n        scaling_factor = (spatial_scaling_factor[1], spatial_scaling_factor[2])\n    elif image.image_layout == ImageLayout.CYX:\n        scaling_factor = (1.0, spatial_scaling_factor[1], spatial_scaling_factor[2])\n    elif image.image_layout == ImageLayout.ZYX:\n        scaling_factor = spatial_scaling_factor\n    elif image.image_layout == ImageLayout.CZYX:\n        scaling_factor = (1.0, *spatial_scaling_factor)\n    elif image.image_layout == ImageLayout.ZCYX:\n        scaling_factor = (spatial_scaling_factor[0], 1.0, *spatial_scaling_factor[1:])\n    else:\n        raise ValueError(f\"Unknown image layout {image.image_layout}\")\n\n    out_data = image_rescale(image.get_data(), scaling_factor, order=order)\n    new_image = image.derive_new(\n        out_data, name=f\"{image.name}_rescaled\", voxel_size=new_voxel_size\n    )\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#set-image-voxel-size-task","title":"Set image voxel size task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.set_voxel_size_task","title":"<code>plantseg.tasks.dataprocessing_tasks.set_voxel_size_task(image: PlantSegImage, voxel_size: tuple[float, float, float]) -&gt; PlantSegImage</code>","text":"<p>Set the voxel size of an image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image</p> </li> <li> <code>voxel_size</code>               (<code>tuple[float, float, float]</code>)           \u2013            <p>new voxel size</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef set_voxel_size_task(\n    image: PlantSegImage, voxel_size: tuple[float, float, float]\n) -&gt; PlantSegImage:\n    \"\"\"Set the voxel size of an image.\n\n    Args:\n        image (PlantSegImage): input image\n        voxel_size (tuple[float, float, float]): new voxel size\n\n    \"\"\"\n    new_voxel_size = VoxelSize(voxels_size=voxel_size)\n    new_image = image.derive_new(\n        image._data,\n        name=f\"{image.name}_set_voxel_size\",\n        voxel_size=new_voxel_size,\n        original_voxel_size=new_voxel_size,\n    )\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#image-pair-operation-task","title":"Image pair operation task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.image_pair_operation_task","title":"<code>plantseg.tasks.dataprocessing_tasks.image_pair_operation_task(image1: PlantSegImage, image2: PlantSegImage, operation: ImagePairOperation, normalize_input: bool = False, clip_output: bool = False, normalize_output: bool = False) -&gt; PlantSegImage</code>","text":"<p>Task to perform an operation on two images.</p> <p>Parameters:</p> <ul> <li> <code>image1</code>               (<code>PlantSegImage</code>)           \u2013            <p>First image to process.</p> </li> <li> <code>Image2</code>               (<code>PlantSegImage</code>)           \u2013            <p>Second image to process.</p> </li> <li> <code>operation</code>               (<code>str</code>)           \u2013            <p>Operation to perform on the images.</p> </li> <li> <code>normalize_input</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Normalize input images before processing.</p> </li> <li> <code>clip_output</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Clip output values to the range [0, 1].</p> </li> <li> <code>normalize_output</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Normalize output values to the range [0, 1].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PlantSegImage</code> (              <code>PlantSegImage</code> )          \u2013            <p>New image resulting from the operation.</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef image_pair_operation_task(\n    image1: PlantSegImage,\n    image2: PlantSegImage,\n    operation: ImagePairOperation,\n    normalize_input: bool = False,\n    clip_output: bool = False,\n    normalize_output: bool = False,\n) -&gt; PlantSegImage:\n    \"\"\"\n    Task to perform an operation on two images.\n\n    Args:\n        image1 (PlantSegImage): First image to process.\n        Image2 (PlantSegImage): Second image to process.\n        operation (str): Operation to perform on the images.\n        normalize_input (bool): Normalize input images before processing.\n        clip_output (bool): Clip output values to the range [0, 1].\n        normalize_output (bool): Normalize output values to the range [0, 1].\n\n    Returns:\n        PlantSegImage: New image resulting from the operation.\n    \"\"\"\n    result = process_images(\n        image1.get_data(),\n        image2.get_data(),\n        operation=operation,\n        normalize_input=normalize_input,\n        clip_output=clip_output,\n        normalize_output=normalize_output,\n    )\n    new_image = image1.derive_new(\n        result, name=f\"{image1.name}_{operation}_{image2.name}\"\n    )\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#label-postprocessing-tasks","title":"Label Postprocessing Tasks","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#remove-false-positives-task","title":"Remove false positives task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.remove_false_positives_by_foreground_probability_task","title":"<code>plantseg.tasks.dataprocessing_tasks.remove_false_positives_by_foreground_probability_task(segmentation: PlantSegImage, foreground: PlantSegImage, threshold: float) -&gt; list[PlantSegImage]</code>","text":"<p>Remove false positives from a segmentation based on the foreground probability.</p> <p>Parameters:</p> <ul> <li> <code>segmentation</code>               (<code>PlantSegImage</code>)           \u2013            <p>input segmentation</p> </li> <li> <code>foreground</code>               (<code>PlantSegImage</code>)           \u2013            <p>input foreground probability</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>threshold value</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef remove_false_positives_by_foreground_probability_task(\n    segmentation: PlantSegImage, foreground: PlantSegImage, threshold: float\n) -&gt; list[PlantSegImage]:\n    \"\"\"Remove false positives from a segmentation based on the foreground probability.\n\n    Args:\n        segmentation (PlantSegImage): input segmentation\n        foreground (PlantSegImage): input foreground probability\n        threshold (float): threshold value\n\n    \"\"\"\n    if segmentation.shape != foreground.shape:\n        raise ValueError(\n            \"Segmentation and foreground probability must have the same shape.\"\n        )\n\n    kept, removed = remove_false_positives_by_foreground_probability(\n        segmentation.get_data(), foreground.get_data(), threshold\n    )\n    image_kept = segmentation.derive_new(\n        kept,\n        name=f\"{segmentation.name}_fg_filtered\",\n    )\n    image_removed = segmentation.derive_new(\n        removed,\n        name=f\"{segmentation.name}_false_positives\",\n    )\n    return [image_kept, image_removed]\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#fix-overunder-segmentation-task","title":"Fix Over/Under segmentation task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.fix_over_under_segmentation_from_nuclei_task","title":"<code>plantseg.tasks.dataprocessing_tasks.fix_over_under_segmentation_from_nuclei_task(cell_seg: PlantSegImage, nuclei_seg: PlantSegImage, threshold_merge: float, threshold_split: float, quantile_min: float, quantile_max: float, boundary: PlantSegImage | None = None) -&gt; PlantSegImage</code>","text":"<p>Task to fix over- and under-segmentation of cells based on nuclear segmentation.</p> <p>Parameters:</p> <ul> <li> <code>cell_seg</code>               (<code>PlantSegImage</code>)           \u2013            <p>Input cell segmentation as a PlantSegImage object.</p> </li> <li> <code>nuclei_seg</code>               (<code>PlantSegImage</code>)           \u2013            <p>Input nuclear segmentation as a PlantSegImage object.</p> </li> <li> <code>threshold_merge</code>               (<code>float</code>)           \u2013            <p>Threshold for merging cells, as a fraction (0-1).</p> </li> <li> <code>threshold_split</code>               (<code>float</code>)           \u2013            <p>Threshold for splitting cells, as a fraction (0-1).</p> </li> <li> <code>quantile_min</code>               (<code>float</code>)           \u2013            <p>Minimum quantile for filtering nuclei sizes, as a fraction (0-1).</p> </li> <li> <code>quantile_max</code>               (<code>float</code>)           \u2013            <p>Maximum quantile for filtering nuclei sizes, as a fraction (0-1).</p> </li> <li> <code>boundary</code>               (<code>PlantSegImage | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional boundary probability map for segmentation refinement.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PlantSegImage</code> (              <code>PlantSegImage</code> )          \u2013            <p>Corrected cell segmentation as a PlantSegImage object.</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef fix_over_under_segmentation_from_nuclei_task(\n    cell_seg: PlantSegImage,\n    nuclei_seg: PlantSegImage,\n    threshold_merge: float,\n    threshold_split: float,\n    quantile_min: float,\n    quantile_max: float,\n    boundary: PlantSegImage | None = None,\n) -&gt; PlantSegImage:\n    \"\"\"\n    Task to fix over- and under-segmentation of cells based on nuclear segmentation.\n\n    Args:\n        cell_seg (PlantSegImage): Input cell segmentation as a PlantSegImage object.\n        nuclei_seg (PlantSegImage): Input nuclear segmentation as a PlantSegImage object.\n        threshold_merge (float): Threshold for merging cells, as a fraction (0-1).\n        threshold_split (float): Threshold for splitting cells, as a fraction (0-1).\n        quantile_min (float): Minimum quantile for filtering nuclei sizes, as a fraction (0-1).\n        quantile_max (float): Maximum quantile for filtering nuclei sizes, as a fraction (0-1).\n        boundary (PlantSegImage | None, optional): Optional boundary probability map for segmentation refinement.\n\n    Returns:\n        PlantSegImage: Corrected cell segmentation as a PlantSegImage object.\n    \"\"\"\n    corrected_data = fix_over_under_segmentation_from_nuclei(\n        cell_seg.get_data(),\n        nuclei_seg.get_data(),\n        threshold_merge=threshold_merge,\n        threshold_split=threshold_split,\n        quantile_min=quantile_min,\n        quantile_max=quantile_max,\n        boundary=boundary.get_data() if boundary else None,\n    )\n    return cell_seg.derive_new(corrected_data, name=f\"{cell_seg.name}_nuc_fixed\")\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#set-biggest-object-as-background-task","title":"Set biggest object as background task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.set_biggest_instance_to_zero_task","title":"<code>plantseg.tasks.dataprocessing_tasks.set_biggest_instance_to_zero_task(image: PlantSegImage, instance_could_be_zero: bool = False) -&gt; PlantSegImage</code>","text":"<p>Task to set the largest segment in a segmentation image to zero.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>Segmentation image to process.</p> </li> <li> <code>instance_could_be_zero</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, 0 might be an instance label,</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PlantSegImage</code> (              <code>PlantSegImage</code> )          \u2013            <p>New segmentation image with largest instance set to 0.</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef set_biggest_instance_to_zero_task(\n    image: PlantSegImage, instance_could_be_zero: bool = False\n) -&gt; PlantSegImage:\n    \"\"\"\n    Task to set the largest segment in a segmentation image to zero.\n\n    Args:\n        image (PlantSegImage): Segmentation image to process.\n        instance_could_be_zero (bool): If True, 0 might be an instance label,\n        add 1 to all labels before processing.\n\n    Returns:\n        PlantSegImage: New segmentation image with largest instance set to 0.\n    \"\"\"\n    if not image.semantic_type == SemanticType.SEGMENTATION:\n        raise ValueError(\"Input image must be a segmentation or mask image.\")\n    data = image.get_data()\n    logger.info(\n        f\"Processing {image.name} with shape {data.shape} and max {data.max()}, min {data.min()}.\"\n    )\n    new_data = set_biggest_instance_to_zero(\n        data, instance_could_be_zero=instance_could_be_zero\n    )\n    new_image = image.derive_new(new_data, name=f\"{image.name}_bg0\")\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#relabel-task","title":"Relabel task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.relabel_segmentation_task","title":"<code>plantseg.tasks.dataprocessing_tasks.relabel_segmentation_task(image: PlantSegImage, background: int | None = None) -&gt; PlantSegImage</code>","text":"<p>Task to relabel a segmentation image contiguously, ensuring non-touching segments with the same ID are relabeled.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>Segmentation image to process.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PlantSegImage</code> (              <code>PlantSegImage</code> )          \u2013            <p>New segmentation image with relabeled instances.</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef relabel_segmentation_task(\n    image: PlantSegImage, background: int | None = None\n) -&gt; PlantSegImage:\n    \"\"\"\n    Task to relabel a segmentation image contiguously, ensuring non-touching\n    segments with the same ID are relabeled.\n\n    Args:\n        image (PlantSegImage): Segmentation image to process.\n\n    Returns:\n        PlantSegImage: New segmentation image with relabeled instances.\n    \"\"\"\n    if not image.semantic_type == SemanticType.SEGMENTATION:\n        raise ValueError(\"Input image must be a segmentation or mask image.\")\n    data = image.get_data()\n    new_data = relabel_segmentation(data, background=background)\n    new_image = image.derive_new(new_data, name=f\"{image.name}_relabeled\")\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/io_tasks/","title":"Import and export tasks","text":""},{"location":"chapters/python_api/tasks/io_tasks/#import-task","title":"Import task","text":""},{"location":"chapters/python_api/tasks/io_tasks/#plantseg.tasks.io_tasks.import_image_task","title":"<code>plantseg.tasks.io_tasks.import_image_task(input_path: Path, semantic_type: str, stack_layout: str, image_name: str | None = None, key: str | None = None, m_slicing: str | None = None) -&gt; PlantSegImage</code>","text":"<p>Task wrapper creating a PlantSegImage object from an image file.</p> <p>Parameters:</p> <ul> <li> <code>input_path</code>               (<code>Path</code>)           \u2013            <p>path to the image file</p> </li> <li> <code>semantic_type</code>               (<code>str</code>)           \u2013            <p>semantic type of the image (raw, segmentation, prediction)</p> </li> <li> <code>stack_layout</code>               (<code>str</code>)           \u2013            <p>stack layout of the image (3D, 2D, 2D_time)</p> </li> <li> <code>image_name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>name of the image, if None the name will be the same as the file name</p> </li> <li> <code>key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>key for the image (used only for h5 and zarr formats)</p> </li> <li> <code>m_slicing</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>m_slicing of the image (None, time, z, y, x)</p> </li> </ul> Source code in <code>plantseg/tasks/io_tasks.py</code> <pre><code>@task_tracker(\n    is_root=True,\n    list_inputs={\n        \"input_path\": RunTimeInputSchema(\n            description=\"Path to a file, or a directory containing files (all files will be imported) or list of paths.\",\n            required=True,\n            is_input_file=True,\n        ),\n    },\n)\ndef import_image_task(\n    input_path: Path,\n    semantic_type: str,\n    stack_layout: str,\n    image_name: str | None = None,\n    key: str | None = None,\n    m_slicing: str | None = None,\n) -&gt; PlantSegImage:\n    \"\"\"\n    Task wrapper creating a PlantSegImage object from an image file.\n\n    Args:\n        input_path (Path): path to the image file\n        semantic_type (str): semantic type of the image (raw, segmentation, prediction)\n        stack_layout (str): stack layout of the image (3D, 2D, 2D_time)\n        image_name (str): name of the image, if None the name will be the same as the file name\n        key (str | None): key for the image (used only for h5 and zarr formats)\n        m_slicing (str | None): m_slicing of the image (None, time, z, y, x)\n    \"\"\"\n\n    if image_name is None:\n        image_name = input_path.stem\n\n    return import_image(\n        path=input_path,\n        key=key,\n        image_name=image_name,\n        semantic_type=semantic_type,\n        stack_layout=stack_layout,\n        m_slicing=m_slicing,\n    )\n</code></pre>"},{"location":"chapters/python_api/tasks/io_tasks/#export-task","title":"Export task","text":""},{"location":"chapters/python_api/tasks/io_tasks/#plantseg.tasks.io_tasks.export_image_task","title":"<code>plantseg.tasks.io_tasks.export_image_task(image: PlantSegImage, export_directory: Path, name_pattern: str = '{file_name}_export', key: str | None = None, scale_to_origin: bool = True, export_format: str = 'tiff', data_type: str = 'uint16') -&gt; None</code>","text":"<p>Task wrapper for saving an PlantSegImage object to disk.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image to be saved to disk</p> </li> <li> <code>export_directory</code>               (<code>Path</code>)           \u2013            <p>output directory path where the image will be saved</p> </li> <li> <code>name_pattern</code>               (<code>str</code>, default:                   <code>'{file_name}_export'</code> )           \u2013            <p>output file name pattern, can contain the {image_name} or {file_name} tokens to be replaced in the final file name.</p> </li> <li> <code>key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>key for the image (used only for h5 and zarr formats).</p> </li> <li> <code>scale_to_origin</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>scale the voxel size to the original one</p> </li> <li> <code>export_format</code>               (<code>str</code>, default:                   <code>'tiff'</code> )           \u2013            <p>file format (tiff, h5, zarr)</p> </li> <li> <code>data_type</code>               (<code>str</code>, default:                   <code>'uint16'</code> )           \u2013            <p>data type to save the image.</p> </li> </ul> Source code in <code>plantseg/tasks/io_tasks.py</code> <pre><code>@task_tracker(\n    is_leaf=True,\n    list_inputs={\n        \"export_directory\": RunTimeInputSchema(\n            description=\"Output directory path where the image will be saved\",\n            required=True,\n        ),\n        \"name_pattern\": RunTimeInputSchema(\n            description=(\n                \"Output file name pattern. Use placeholder {image_name} for \"\n                \"the napari layer name or {file_name} for the input file name\"\n            ),\n            required=False,\n        ),\n    },\n)\ndef export_image_task(\n    image: PlantSegImage,\n    export_directory: Path,\n    name_pattern: str = \"{file_name}_export\",\n    key: str | None = None,\n    scale_to_origin: bool = True,\n    export_format: str = \"tiff\",\n    data_type: str = \"uint16\",\n) -&gt; None:\n    \"\"\"\n    Task wrapper for saving an PlantSegImage object to disk.\n\n    Args:\n        image (PlantSegImage): input image to be saved to disk\n        export_directory (Path): output directory path where the image will be saved\n        name_pattern (str): output file name pattern, can contain the {image_name} or {file_name} tokens\n            to be replaced in the final file name.\n        key (str | None): key for the image (used only for h5 and zarr formats).\n        scale_to_origin (bool): scale the voxel size to the original one\n        export_format (str): file format (tiff, h5, zarr)\n        data_type (str): data type to save the image.\n    \"\"\"\n    save_image(\n        image=image,\n        export_directory=export_directory,\n        name_pattern=name_pattern,\n        key=key,\n        scale_to_origin=scale_to_origin,\n        export_format=export_format,\n        data_type=data_type,\n    )\n    return None\n</code></pre>"},{"location":"chapters/python_api/tasks/prediction_tasks/","title":"Neural network prediction tasks","text":""},{"location":"chapters/python_api/tasks/prediction_tasks/#unet-prediction-task","title":"UNet prediction task","text":""},{"location":"chapters/python_api/tasks/prediction_tasks/#plantseg.tasks.prediction_tasks.unet_prediction_task","title":"<code>plantseg.tasks.prediction_tasks.unet_prediction_task(image: PlantSegImage, model_name: str | None, model_id: str | None, suffix: str = '_prediction', patch: tuple[int, int, int] | None = None, patch_halo: tuple[int, int, int] | None = None, single_batch_mode: bool = True, device: str = 'cuda', model_update: bool = False, disable_tqdm: bool = False, config_path: Path | None = None, model_weights_path: Path | None = None, _tracker: Optional[PBar_Tracker] = None) -&gt; list[PlantSegImage]</code>","text":"<p>Apply a trained U-Net model to a PlantSegImage object.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image object</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>the name of the model to use</p> </li> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>the ID of the model to use</p> </li> <li> <code>suffix</code>               (<code>str</code>, default:                   <code>'_prediction'</code> )           \u2013            <p>suffix to append to the new image name</p> </li> <li> <code>patch</code>               (<code>tuple[int, int, int]</code>, default:                   <code>None</code> )           \u2013            <p>patch size for prediction</p> </li> <li> <code>single_batch_mode</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to use a single batch for prediction</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda'</code> )           \u2013            <p>the computation device ('cpu', 'cuda', etc.)</p> </li> <li> <code>model_update</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to update the model to the latest version</p> </li> <li> <code>_tracker</code>               (<code>Optional[PBar_Tracker]</code>, default:                   <code>None</code> )           \u2013            <p>Tracker of progress bar. Not used in workflow as it starts with <code>_</code></p> </li> </ul> Source code in <code>plantseg/tasks/prediction_tasks.py</code> <pre><code>@task_tracker\ndef unet_prediction_task(\n    image: PlantSegImage,\n    model_name: str | None,\n    model_id: str | None,\n    suffix: str = \"_prediction\",\n    patch: tuple[int, int, int] | None = None,\n    patch_halo: tuple[int, int, int] | None = None,\n    single_batch_mode: bool = True,\n    device: str = \"cuda\",\n    model_update: bool = False,\n    disable_tqdm: bool = False,\n    config_path: Path | None = None,\n    model_weights_path: Path | None = None,\n    _tracker: Optional[\"PBar_Tracker\"] = None,\n) -&gt; list[PlantSegImage]:\n    \"\"\"\n    Apply a trained U-Net model to a PlantSegImage object.\n\n    Args:\n        image (PlantSegImage): input image object\n        model_name (str): the name of the model to use\n        model_id (str): the ID of the model to use\n        suffix (str): suffix to append to the new image name\n        patch (tuple[int, int, int]): patch size for prediction\n        single_batch_mode (bool): whether to use a single batch for prediction\n        device (str): the computation device ('cpu', 'cuda', etc.)\n        model_update (bool): whether to update the model to the latest version\n        _tracker: Tracker of progress bar. Not used in workflow as it starts with `_`\n    \"\"\"\n    data = image.get_data()\n    input_layout = image.image_layout\n\n    pmaps = unet_prediction(\n        raw=data,\n        input_layout=input_layout.value,\n        model_name=model_name,\n        model_id=model_id,\n        patch=patch,\n        patch_halo=patch_halo,\n        single_batch_mode=single_batch_mode,\n        device=device,\n        model_update=model_update,\n        disable_tqdm=disable_tqdm,\n        config_path=config_path,\n        model_weights_path=model_weights_path,\n        tracker=_tracker,\n    )\n    assert pmaps.ndim == 4, f\"Expected 4D CZXY prediction, got {pmaps.ndim}D\"\n\n    new_images = []\n\n    for i, pmap in enumerate(pmaps):\n        # Input layout is always ZYX this loop\n        pmap = fix_layout(\n            pmap, input_layout=ImageLayout.ZYX.value, output_layout=input_layout.value\n        )\n        new_images.append(\n            image.derive_new(\n                pmap,\n                name=f\"{image.name}_{suffix}_{i}\",\n                semantic_type=SemanticType.PREDICTION,\n                image_layout=input_layout,\n            )\n        )\n\n    return new_images\n</code></pre>"},{"location":"chapters/python_api/tasks/segmentation_tasks/","title":"Segmentation tasks","text":""},{"location":"chapters/python_api/tasks/segmentation_tasks/#distance-transform-watershed-task","title":"Distance transform watershed task","text":""},{"location":"chapters/python_api/tasks/segmentation_tasks/#plantseg.tasks.segmentation_tasks.dt_watershed_task","title":"<code>plantseg.tasks.segmentation_tasks.dt_watershed_task(image: PlantSegImage, threshold: float = 0.5, sigma_seeds: float = 1.0, stacked: bool = False, sigma_weights: float = 2.0, min_size: int = 100, alpha: float = 1.0, pixel_pitch: tuple[int, ...] | None = None, apply_nonmax_suppression: bool = False, n_threads: int | None = None, is_nuclei_image: bool = False, _tracker: Optional[PBar_Tracker] = None) -&gt; PlantSegImage</code>","text":"<p>Distance transform watershed segmentation task.</p> <p>This function applies the distance transform watershed algorithm to segment the input image. It handles both standard boundary probability maps and nuclei images, with options for various preprocessing and segmentation parameters.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>The input image to segment.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold value for the boundary probability maps. Defaults to 0.5.</p> </li> <li> <code>sigma_seeds</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Standard deviation for Gaussian smoothing applied to the seed map. Defaults to 1.0.</p> </li> <li> <code>stacked</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True and the image is 3D, processes the image slice-by-slice (2D). Defaults to False.</p> </li> <li> <code>sigma_weights</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Standard deviation for Gaussian smoothing applied to the weight map. Defaults to 2.0.</p> </li> <li> <code>min_size</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Minimum size of the segments to keep. Smaller segments will be removed. Defaults to 100.</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Blending factor between the input image and the distance transform when computing the weight map. Defaults to 1.0.</p> </li> <li> <code>pixel_pitch</code>               (<code>tuple[int, ...] | None</code>, default:                   <code>None</code> )           \u2013            <p>Anisotropy factors for the distance transform. If None, isotropic distances are assumed. Defaults to None.</p> </li> <li> <code>apply_nonmax_suppression</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to apply non-maximum suppression to the seeds. Requires the Nifty library. Defaults to False.</p> </li> <li> <code>n_threads</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of threads to use for parallel processing in 2D mode. Defaults to None.</p> </li> <li> <code>is_nuclei_image</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, indicates that the input image is a nuclei image, and preprocessing is applied accordingly. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PlantSegImage</code> (              <code>PlantSegImage</code> )          \u2013            <p>The segmented image as a new <code>PlantSegImage</code> object.</p> </li> </ul> Source code in <code>plantseg/tasks/segmentation_tasks.py</code> <pre><code>@task_tracker\ndef dt_watershed_task(\n    image: PlantSegImage,\n    threshold: float = 0.5,\n    sigma_seeds: float = 1.0,\n    stacked: bool = False,\n    sigma_weights: float = 2.0,\n    min_size: int = 100,\n    alpha: float = 1.0,\n    pixel_pitch: tuple[int, ...] | None = None,\n    apply_nonmax_suppression: bool = False,\n    n_threads: int | None = None,\n    is_nuclei_image: bool = False,\n    _tracker: Optional[\"PBar_Tracker\"] = None,\n) -&gt; PlantSegImage:\n    \"\"\"Distance transform watershed segmentation task.\n\n    This function applies the distance transform watershed algorithm to segment the input image.\n    It handles both standard boundary probability maps and nuclei images, with options for\n    various preprocessing and segmentation parameters.\n\n    Args:\n        image (PlantSegImage): The input image to segment.\n        threshold (float, optional): Threshold value for the boundary probability maps.\n            Defaults to 0.5.\n        sigma_seeds (float, optional): Standard deviation for Gaussian smoothing applied to\n            the seed map. Defaults to 1.0.\n        stacked (bool, optional): If True and the image is 3D, processes the image\n            slice-by-slice (2D). Defaults to False.\n        sigma_weights (float, optional): Standard deviation for Gaussian smoothing applied to\n            the weight map. Defaults to 2.0.\n        min_size (int, optional): Minimum size of the segments to keep. Smaller segments\n            will be removed. Defaults to 100.\n        alpha (float, optional): Blending factor between the input image and the distance\n            transform when computing the weight map. Defaults to 1.0.\n        pixel_pitch (tuple[int, ...] | None, optional): Anisotropy factors for the distance\n            transform. If None, isotropic distances are assumed. Defaults to None.\n        apply_nonmax_suppression (bool, optional): Whether to apply non-maximum suppression\n            to the seeds. Requires the Nifty library. Defaults to False.\n        n_threads (int | None, optional): Number of threads to use for parallel processing\n            in 2D mode. Defaults to None.\n        is_nuclei_image (bool, optional): If True, indicates that the input image is a nuclei\n            image, and preprocessing is applied accordingly. Defaults to False.\n\n    Returns:\n        PlantSegImage: The segmented image as a new `PlantSegImage` object.\n    \"\"\"\n    if image.is_multichannel:\n        raise ValueError(\"Multichannel images are not supported for this task.\")\n\n    if image.semantic_type != SemanticType.PREDICTION:\n        logger.warning(\n            \"The input image is not a boundary probability map. The task will still attempt to run, but the results may not be as expected.\"\n        )\n\n    if image.image_layout == ImageLayout.YX and stacked:\n        logger.warning(\n            \"Stack, or 'per slice' is only for 3D images (ZYX). The stack option will be disabled.\"\n        )\n        stacked = False\n\n    if is_nuclei_image:\n        boundary_pmaps = normalize_01(image.get_data())\n        boundary_pmaps = 1.0 - boundary_pmaps\n        mask = boundary_pmaps &lt; threshold\n    else:\n        boundary_pmaps = image.get_data()\n        mask = None\n\n    dt_seg = dt_watershed(\n        boundary_pmaps=boundary_pmaps,\n        threshold=threshold,\n        sigma_seeds=sigma_seeds,\n        stacked=stacked,\n        sigma_weights=sigma_weights,\n        min_size=min_size,\n        alpha=alpha,\n        pixel_pitch=pixel_pitch,\n        apply_nonmax_suppression=apply_nonmax_suppression,\n        n_threads=n_threads,\n        mask=mask,\n    )\n\n    dt_seg_image = image.derive_new(\n        dt_seg,\n        name=f\"{image.name}_dt_watershed\",\n        semantic_type=SemanticType.SEGMENTATION,\n    )\n    return dt_seg_image\n</code></pre>"},{"location":"chapters/python_api/tasks/segmentation_tasks/#cluster-segmentation-task","title":"Cluster segmentation task","text":""},{"location":"chapters/python_api/tasks/segmentation_tasks/#plantseg.tasks.segmentation_tasks.clustering_segmentation_task","title":"<code>plantseg.tasks.segmentation_tasks.clustering_segmentation_task(image: PlantSegImage, over_segmentation: PlantSegImage | None = None, mode='gasp', beta: float = 0.5, post_min_size: int = 100) -&gt; PlantSegImage</code>","text":"<p>Agglomerative segmentation task.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image object</p> </li> <li> <code>over_segmentation</code>               (<code>PlantSegImage</code>, default:                   <code>None</code> )           \u2013            <p>over-segmentation image object</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'gasp'</code> )           \u2013            <p>mode for the agglomerative segmentation</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter</p> </li> <li> <code>post_min_size</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>minimum size for the segments</p> </li> </ul> Source code in <code>plantseg/tasks/segmentation_tasks.py</code> <pre><code>@task_tracker\ndef clustering_segmentation_task(\n    image: PlantSegImage,\n    over_segmentation: PlantSegImage | None = None,\n    mode=\"gasp\",\n    beta: float = 0.5,\n    post_min_size: int = 100,\n) -&gt; PlantSegImage:\n    \"\"\"Agglomerative segmentation task.\n\n    Args:\n        image (PlantSegImage): input image object\n        over_segmentation (PlantSegImage): over-segmentation image object\n        mode (str): mode for the agglomerative segmentation\n        beta (float): beta parameter\n        post_min_size (int): minimum size for the segments\n    \"\"\"\n    if image.is_multichannel:\n        raise ValueError(\"Multichannel images are not supported for this task.\")\n\n    if image.semantic_type != SemanticType.PREDICTION:\n        logger.warning(\n            \"The input image is not a boundary probability map. The task will still attempt to run, but the results may not be as expected.\"\n        )\n\n    boundary_pmaps = image.get_data()\n\n    if over_segmentation is None:\n        superpixels = None\n    else:\n        if over_segmentation.semantic_type != SemanticType.SEGMENTATION:\n            raise ValueError(\"The input over_segmentation is not a segmentation map.\")\n        superpixels = over_segmentation.get_data()\n\n        if boundary_pmaps.shape != superpixels.shape:\n            raise ValueError(\n                \"The boundary probability map and the over-segmentation map should have the same shape.\"\n            )\n\n    if mode == \"gasp\":\n        seg = gasp(\n            boundary_pmaps,\n            superpixels=superpixels,\n            beta=beta,\n            post_minsize=post_min_size,\n        )\n    elif mode == \"multicut\":\n        if superpixels is None:\n            raise ValueError(\"The superpixels are required for the multicut mode.\")\n        seg = multicut(\n            boundary_pmaps,\n            superpixels=superpixels,\n            beta=beta,\n            post_minsize=post_min_size,\n        )\n    elif mode == \"mutex_ws\":\n        seg = mutex_ws(\n            boundary_pmaps,\n            superpixels=superpixels,\n            beta=beta,\n            post_minsize=post_min_size,\n        )\n    else:\n        raise ValueError(\n            f\"Unknown mode: {mode}, select one of ['gasp', 'multicut', 'mutex_ws']\"\n        )\n\n    seg_image = image.derive_new(\n        seg, name=f\"{image.name}_{mode}\", semantic_type=SemanticType.SEGMENTATION\n    )\n    return seg_image\n</code></pre>"},{"location":"chapters/python_api/tasks/segmentation_tasks/#lifted-multicut-task","title":"Lifted Multicut task","text":""},{"location":"chapters/python_api/tasks/segmentation_tasks/#plantseg.tasks.segmentation_tasks.lmc_segmentation_task","title":"<code>plantseg.tasks.segmentation_tasks.lmc_segmentation_task(boundary_pmap: PlantSegImage, superpixels: PlantSegImage, nuclei: PlantSegImage, beta: float = 0.5, post_min_size: int = 100) -&gt; PlantSegImage</code>","text":"<p>Lifted multicut segmentation task.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmap</code>               (<code>PlantSegImage</code>)           \u2013            <p>cell boundary prediction, PlantSegImage of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>superpixels</code>               (<code>PlantSegImage</code>)           \u2013            <p>superpixels/over-segmentation. Must have the same shape as boundary_pmap.</p> </li> <li> <code>nuclei</code>               (<code>PlantSegImage</code>)           \u2013            <p>a nuclear segmentation or prediction map. Must have the same shape as boundary_pmap.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter for the Multicut. A small value will steer the segmentation towards under-segmentation, while a high-value bias the segmentation towards the over-segmentation. (default: 0.5)</p> </li> <li> <code>post_min_size</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>minimal size of the segments after Multicut. (default: 100)</p> </li> </ul> Source code in <code>plantseg/tasks/segmentation_tasks.py</code> <pre><code>@task_tracker\ndef lmc_segmentation_task(\n    boundary_pmap: PlantSegImage,\n    superpixels: PlantSegImage,\n    nuclei: PlantSegImage,\n    beta: float = 0.5,\n    post_min_size: int = 100,\n) -&gt; PlantSegImage:\n    \"\"\"Lifted multicut segmentation task.\n\n    Args:\n        boundary_pmap (PlantSegImage): cell boundary prediction, PlantSegImage of shape (Z, Y, X) with values between 0 and 1.\n        superpixels (PlantSegImage): superpixels/over-segmentation. Must have the same shape as boundary_pmap.\n        nuclei (PlantSegImage): a nuclear segmentation or prediction map. Must have the same shape as boundary_pmap.\n        beta (float): beta parameter for the Multicut.\n            A small value will steer the segmentation towards under-segmentation, while\n            a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_min_size (int): minimal size of the segments after Multicut. (default: 100)\n    \"\"\"\n    if (\n        nuclei.semantic_type is SemanticType.PREDICTION\n        or nuclei.semantic_type is SemanticType.RAW\n    ):\n        lmc = lifted_multicut_from_nuclei_pmaps\n        extra_key = \"nuclei_pmaps\"\n    else:\n        lmc = lifted_multicut_from_nuclei_segmentation\n        extra_key = \"nuclei_seg\"\n\n    segmentation = lmc(\n        boundary_pmaps=boundary_pmap.get_data(),\n        superpixels=superpixels.get_data(),\n        **{extra_key: nuclei.get_data()},\n        beta=beta,\n        post_minsize=post_min_size,\n    )\n\n    ps_seg = superpixels.derive_new(\n        segmentation,\n        name=f\"{superpixels.name}_lmc\",\n        semantic_type=SemanticType.SEGMENTATION,\n    )\n    return ps_seg\n</code></pre>"},{"location":"chapters/workflow_gui/","title":"Working with workflows","text":"<p>Plantseg can create and execute workflows for batch processing. A new workflow can be created in the napari gui, and it can be executed using the commandline interface.</p>"},{"location":"chapters/workflow_gui/#creating-a-workflow","title":"Creating a workflow","text":"<p>To create a new workflow, one must process an example image through the napari GUI. Once the result has been exported, a button to <code>Export Workflow</code> will appear in the input/output tab in napari. Plantseg then creates a <code>yaml</code> file to repeat the workflow for any number of files.</p> <p>The workflow includes input/output paths and naming schemes, so before the new workflow is useable, those need to be adjusted using the editor:</p>"},{"location":"chapters/workflow_gui/#editing-a-workflow","title":"Editing a workflow","text":"<p>Plantseg comes with an editor for the workflow yaml files. It can be opened through the napari gui (<code>Edit Workflow</code>), or from the cli:</p> <pre><code>plantseg --editor [workflow.yaml]\n</code></pre> <p></p>"},{"location":"chapters/workflow_gui/#inputoutput","title":"Input/Output","text":"<p>On the left, the input/output section is shown. You can specify a directory as <code>input_path</code> to use all images in this directory, or just a single image.</p> <p>The name_pattern defines how the exported images are named. You can use the placeholders <code>{file_name}</code> to reference the input file's name, or <code>{image_name}</code> to reference the layer name napari would have given the image.</p>"},{"location":"chapters/workflow_gui/#tasks","title":"Tasks","text":"<p>The right side displays all tasks the workflow contains. Most of them expose some modifiable values.</p> <p>If you need to modify something else, the editor should show you the proper name of the field, so you can edit the file in your favorite text editor.</p>"},{"location":"chapters/workflow_gui/#running-a-workflow","title":"Running a workflow","text":"<p>To finally run a workflow after you have modified the paths to your liking, please use the cli:</p> <pre><code>plantseg --config your_workflow_file.yaml\n</code></pre>"},{"location":"snippets/widgets/preprocessing/rescale/","title":"Rescale","text":"From factorTo layer voxel sizeTo layer shapeTo model voxel sizeTo voxel sizeTo shapeSet voxel size <p>Using the <code>From factor</code> mode, the user can rescale the image by a multiplicate factor. For example, if the image has a shape <code>(10, 10, 10)</code> and the user wants to rescale it by a factor of <code>(2, 2, 2)</code>, the new size will be <code>(20, 20, 20)</code>.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p> <p>Using the <code>To layer voxel size</code> mode, the user can rescale the image to the voxel size of a specific layer. For example, if two images are loaded in the viewer, one with a voxel size of <code>(0.1, 0.1, 0.1)um</code> and the other with a voxel size of <code>(0.1, 0.05, 0.05)um</code>, the user can rescale the first image to the voxel size of the second image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p> <p>Using the <code>To layer shape</code> mode, the user can rescale the image to the shape of a specific layer. For example, if two images are loaded in the viewer, one with a shape <code>(10, 10, 10)</code> and the other with a shape <code>(20, 20, 20)</code>, the user can rescale the first image to the shape of the second image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p> <p>Using the <code>To model voxel size</code> mode, the user can rescale the image to the voxel size of the model. For example, if the model has been trained with data at voxel size of <code>(0.1, 0.1, 0.1)um</code>, the user can rescale the image to this voxel size.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p> <p>Using the <code>To voxel size</code> mode, the user can rescale the image to a specific voxel size.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p> <p>Using the <code>To shape</code> mode, the user can rescale the image to a specific shape.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p> <p>Using the <code>Set voxel size</code> mode, the user can set the voxel size of the image to a specific value. This only changes the metadata of the image and does not rescale the image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.              </p>"},{"location":"snippets/widgets/segmentation/prediction/","title":"Prediction","text":"PlantSeg ZooBioImage.IO Model Zoo <p> <p></p> <p></p> </p> <p> <p></p> <p></p> </p>"}]}