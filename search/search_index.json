{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PlantSeg introduction","text":"<p>PlantSeg v2 is out!</p> <p>We are working on updating the documentation for the new version of PlantSeg. Stay tuned for more updates and new manuscripts!</p> <p>PlantSeg is a tool for 3D and 2D segmentation. The methods used are very generic and can be used for any instance segmentation workflow, but they are tuned towards cell segmentation in plant tissue. The tool is fundamentally composed of two main steps.</p> Figure: PlantSeg Main Workflow <ul> <li> <p>Cell boundary prediction: A convolutional neural network (CNN) is utilized to perform voxel-wise boundary classification. This network is adept at filtering out diverse types and intensities of noise, homogenizing signal strength, and correcting imaging defects such as blurred or missing cell boundaries. This step ensures a high-quality boundary prediction which is crucial for accurate segmentation.</p> </li> <li> <p>Cell Segmentation as graph partitioning: The boundary prediction from the first step serve as the basis for automated segmentation. PlantSeg implements four distinct algorithms for this task, each with unique features tailored to different segmentation needs. This graph partitioning approach is particularly effective for segmenting densely packed cells.</p> </li> </ul> Figure: PlantSeg v2 Interface <p>For a detailed description of the methods employed in PlantSeg, please refer to our manuscript. If you find PlantSeg useful in your research, please consider citing our work:</p> <pre><code>@article{wolny2020accurate,\n  title={Accurate and versatile 3D segmentation of plant tissues at cellular resolution},\n  author={Wolny, Adrian and Cerrone, Lorenzo and Vijayan, Athul and Tofanelli, Rachele and Barro, Amaya Vilches and Louveaux, Marion and Wenzl, Christian and Strauss, S{\\\"o}ren and Wilson-S{\\'a}nchez, David and Lymbouridou, Rena and others},\n  journal={Elife},\n  volume={9},\n  pages={e57613},\n  year={2020},\n  publisher={eLife Sciences Publications Limited}\n}\n</code></pre>"},{"location":"chapters/getting_started/","title":"Quick Start","text":"<p>PlantSeg can be used in three different ways: interactively (using the Napari viewer), as a command line, or with a GUI. The following sections will guide you through the installation and usage of PlantSeg in each of these modes.</p>"},{"location":"chapters/getting_started/#interactive-plantseg-with-napari-viewer","title":"Interactive PlantSeg with Napari Viewer","text":"<p>PlantSeg app can be started from the terminal. First, activate the newly created conda environment with:</p> <pre><code>mamba activate plant-seg\n</code></pre> <p>then, start the plantseg in napari</p> <pre><code>plantseg --napari\n</code></pre> <p>A more in depth guide can be found in our documentation (GUI).</p>"},{"location":"chapters/getting_started/#command-line-plantseg","title":"Command Line PlantSeg","text":"<p>PlantSeg can be configured using <code>YAML</code> config files.</p> <p>First, activate the newly created conda environment with:</p> <pre><code>mamba activate plant-seg\n</code></pre> <p>then, one can just start the pipeline with</p> <pre><code>plantseg --config CONFIG_PATH\n</code></pre> <p>where <code>CONFIG_PATH</code> is the path to the <code>YAML</code> configuration file. See config.yaml for a sample configuration file and our documentation (CLI) for a detailed description of the parameters.</p>"},{"location":"chapters/getting_started/#plantseg-with-legacy-gui","title":"PlantSeg with Legacy GUI","text":"<p>Deprecated</p> <p>This interface is deprecated and has been removed from PlantSeg v2. Please use the Napari viewer or the command line interface instead, or install PlantSeg v1.</p> <p>PlantSeg app can also be started in a GUI mode, where basic user interface allows to configure and run the pipeline. First, activate the newly created conda environment with:</p> <pre><code>mamba activate plant-seg\n</code></pre> <p>then, run the GUI by simply typing:</p> <pre><code>plantseg --gui\n</code></pre> <p>A more in depth guide can be found in our documentation (Classic GUI).</p>"},{"location":"chapters/getting_started/contributing/","title":"Contribute to PlantSeg","text":"<p>PlantSeg is an open-source project, and we welcome contributions from the community. There are many ways to contribute, such as writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests, or writing code that can be incorporated into PlantSeg itself.</p>"},{"location":"chapters/getting_started/contributing/#getting-started","title":"Getting Started","text":"<p>To set up the development environment, run:</p> <pre><code>mamba env create -f environment-dev.yaml\nconda activate plant-seg-dev\n</code></pre> <p>To install PlantSeg in development mode, run:</p> <pre><code>pip install -e . --no-deps\n</code></pre>"},{"location":"chapters/getting_started/contributing/#hierarchical-design-of-plantseg","title":"Hierarchical Design of PlantSeg","text":"<p>Please refer to Python API.</p>"},{"location":"chapters/getting_started/contributing/#coding-style","title":"Coding Style","text":"<p>PlantSeg uses Ruff for linting and formatting. Ruff is compatible with Black for formatting.</p>"},{"location":"chapters/getting_started/contributing/#before-submitting-a-pull-request","title":"Before Submitting a Pull Request","text":""},{"location":"chapters/getting_started/contributing/#run-tests-with-pytest","title":"Run Tests with <code>pytest</code>","text":"<p>Ensure that <code>pytest</code> is installed in your conda environment. To run the tests, simply use:</p> <pre><code>pytest\n</code></pre>"},{"location":"chapters/getting_started/contributing/#check-syntax-with-pre-commit","title":"Check Syntax with <code>pre-commit</code>","text":"<p>The PlantSeg repository uses pre-commit hooks to ensure the code is correctly formatted and free of linting issues. While not mandatory, it is encouraged to check your code before committing by running:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>Commit messages are important. Please follow the Conventional Commits specification.</p> <p>For efficiency, pytest is not included in the pre-commit hooks. Please run the tests separately.</p>"},{"location":"chapters/getting_started/installation/","title":"Installation","text":"<p>This is the installation guide for the latest PlantSeg. Please check the installation guide for PlantSeg v1 at PlantSeg Legacy Installation.</p>"},{"location":"chapters/getting_started/installation/#prerequisites-for-conda-package","title":"Prerequisites for Conda package","text":"<ul> <li>Linux, Windows, macOS (not all features are available on macOS)</li> <li>(Optional) Nvidia GPU with official Nvidia drivers installed for GPU acceleration</li> </ul>"},{"location":"chapters/getting_started/installation/#install-mamba","title":"Install Mamba","text":"<p>The easiest way to install PlantSeg is by using the conda (Anaconda) or mamba (Miniforge) package manager. We recommend using <code>mamba</code> because it is faster and usually more consistent than <code>conda</code>.</p> LinuxWindows/macOS <p>To download Miniforge open a terminal and type:</p> <pre><code>curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <p>Then install by typing:</p> <pre><code>bash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <p>and follow the installation instructions. Please refer to the Miniforge repo for more information, troubleshooting and uninstallation instructions. The miniforge installation file <code>Miniforge3-*.sh</code> can be deleted now.</p> <p>The first step required to use the pipeline is installing mamba. The installation can be done by downloading the installer from the Miniforge repo. There you can find the download links for the latest version of Miniforge, troubleshooting and uninstallation instructions.</p>"},{"location":"chapters/getting_started/installation/#install-plantseg-using-mamba","title":"Install PlantSeg using Mamba","text":"<p>PlantSeg can be installed directly by executing in the terminal (or PowerShell on Windows). For <code>conda</code> users, the command is identical, just replace <code>mamba</code> with <code>conda</code>.</p> LinuxWindowsmacOS <ul> <li> <p>NVIDIA GPU version, CUDA=12.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=12.1 plant-seg --no-channel-priority\n</code></pre> </li> <li> <p>NVIDIA GPU version, CUDA=11.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=11.8 plant-seg --no-channel-priority\n</code></pre> </li> <li> <p>CPU version</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch cpuonly plant-seg --no-channel-priority\n</code></pre> </li> </ul> <ul> <li> <p>NVIDIA GPU version, CUDA=12.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=12.1 nifty=1.2.1=*_4 plant-seg --no-channel-priority\n</code></pre> </li> <li> <p>NVIDIA GPU version, CUDA=11.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=11.8 nifty=1.2.1=*_4 plant-seg --no-channel-priority\n</code></pre> </li> <li> <p>CPU version</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch cpuonly nifty=1.2.1=*_4 plant-seg --no-channel-priority\n</code></pre> </li> </ul> <ul> <li> <p>Apple silicon version</p> <pre><code>mamba create -n plant-seg -c pytorch -c conda-forge python=3.11 pytorch::pytorch plant-seg --no-channel-priority\n</code></pre> </li> </ul> <p>If you used older versions of PlantSeg, please delete the old config files in <code>~/.plantseg_models/configs/</code> after installing new PlantSeg.</p> <p>The above command will create new conda environment <code>plant-seg</code> together with all required dependencies.</p> <p>Please refer to the PyTorch website for more information on the available versions of PyTorch and the required CUDA version. The GPU version of Pytorch will also work on CPU only machines but has a much larger installation on disk.</p>"},{"location":"chapters/getting_started/installation/#optional-dependencies","title":"Optional dependencies","text":"<p>Certain compressed TIFF files (e.g., Zlib, ZSTD, LZMA formats) require additional codecs to be processed correctly by PlantSeg. To handle such files, install the <code>imagecodecs</code> package:</p> <pre><code>conda activate plant-seg\npip install imagecodecs\n</code></pre> <p>If you plan to use SimpleITK-based watershed segmentation, you will need to install <code>SimpleITK</code> as an additional dependency:</p> <pre><code>conda activate plant-seg\npip install SimpleITK\n</code></pre>"},{"location":"chapters/getting_started/installation/#installing-plantseg-v1","title":"Installing PlantSeg v1","text":"<p>Please check the installation guide for PlantSeg v1 at PlantSeg Legacy Installation.</p>"},{"location":"chapters/getting_started/troubleshooting/","title":"Troubleshooting","text":"<p>This section provides solutions to common issues you might encounter while using PlantSeg. Click on a problem to jump to its specific solution.</p> <ul> <li>Font size problems in GUI</li> <li>Problems with <code>--headless</code> and <code>dask[distributed]</code></li> <li>Could not load library <code>libcudnn_ops_infer.so.8</code></li> <li>Missing configuration key errors</li> <li>Cannot import <code>lifted_problem_from_probabilities</code></li> <li>Other issues</li> </ul>"},{"location":"chapters/getting_started/troubleshooting/#font-size-problems-in-gui","title":"Font size problems in GUI","text":"<p>If you find the font size varies within either Napari or Legacy GUIs, or some buttons or texts are not visible, it might relate to your system's DPI settings or screen resolution. To fix this, you can try to reset the resolution of your system.</p> <p>Related discussions:</p> <ul> <li><code>plantseg --gui</code>, no buttons to begin the workflow #241</li> </ul> <p></p> <p>Other references:</p> <ul> <li>tkinter not recognizing screen resolution correctly</li> <li>High DPI Desktop Application Development on Windows</li> <li>SetProcessDpiAwareness function (shellscalingapi.h)</li> </ul>"},{"location":"chapters/getting_started/troubleshooting/#problems-with-headless-and-daskdistributed","title":"Problems with <code>--headless</code> and <code>dask[distributed]</code>","text":"<p>If you encounter the following error:</p> <pre><code>ImportError: dask.distributed is not installed.\n</code></pre> <p>Please install <code>dask[distributed]</code> to enable headless mode in PlantSeg. Run the following commands in your terminal:</p> <pre><code>mamba activate plant-seg\nmamba install -c pytorch -c nvidia -c conda-forge dask distributed\n</code></pre>"},{"location":"chapters/getting_started/troubleshooting/#could-not-load-library-libcudnn_ops_inferso8","title":"Could not load library <code>libcudnn_ops_infer.so.8</code>","text":"<p>If you encounter this error:</p> <pre><code>Could not load library libcudnn_ops_infer.so.8. Error: libcudnn_ops_infer.so.8: cannot open shared object file: No such file or directory\n</code></pre> <p>Resolve this by installing <code>cudnn</code> using the following command:</p> <pre><code>mamba install -c conda-forge cudnn\n</code></pre>"},{"location":"chapters/getting_started/troubleshooting/#missing-configuration-key-errors","title":"Missing configuration key errors","text":"<p>If you encounter a <code>RuntimeError</code> about a missing key, such as:</p> <pre><code>RuntimeError: key : 'crop_volume' is missing, plant-seg requires 'crop_volume' to run\n</code></pre> <p>This usually means the session configuration file is corrupted or outdated. To fix this:</p> <pre><code>rm ~/.plantseg_models/configs/config_gui_last.yaml\n</code></pre> <p>Ensure your configuration file is properly formatted and includes all required keys. Example configurations can be found in the <code>examples</code> directory of this repository.</p>"},{"location":"chapters/getting_started/troubleshooting/#cannot-import-lifted_problem_from_probabilities","title":"Cannot import <code>lifted_problem_from_probabilities</code>","text":"<p>If you receive an error related to importing from <code>elf.segmentation.features</code>, reinstall elf:</p> <pre><code>conda install -c conda-forge python-elf\n</code></pre>"},{"location":"chapters/getting_started/troubleshooting/#other-issues","title":"Other issues","text":"<p>PlantSeg is actively developed, and sometimes model or configuration files saved in <code>~/.plantseg_models</code> may become outdated. If you encounter errors related to configuration loading:</p> <ol> <li>Close the PlantSeg application.</li> <li>Delete the <code>~/.plantsep_models</code> directory.</li> <li>Restart the application and try again.</li> </ol> <p>These steps should help resolve any issues and enhance your experience with PlantSeg.</p>"},{"location":"chapters/plantseg_interactive_napari/","title":"PlantSeg Interactive - Napari","text":"<p>PlantSeg app can also be started using napari as a viewer.</p> <p>First, activate the newly created conda environment with:</p> <pre><code>conda activate plant-seg\n</code></pre> <p>then, start the plantseg in napari</p> <pre><code>plantseg --napari\n</code></pre> <p>This will allow you to interactively learn and test PlantSeg on your data:</p> <p></p>"},{"location":"chapters/plantseg_interactive_napari/extra/","title":"Additional Widgets","text":""},{"location":"chapters/plantseg_interactive_napari/extra/#widget-add-custom-model","title":"Widget: Add Custom Model","text":"<p> <p></p> <p></p>                      None             <ul><li>New model name: None</li> <li>Model location: None</li> <li>Voxel Size: Resolution of the dataset used to model in micrometers per pixel.</li> <li>Description: None</li> <li>Dimensionality: Dimensionality of the model (2D or 3D). Any 2D model can be used for 3D data.</li> <li>Microscopy modality: Modality of the model (e.g. confocal, light-sheet ...).</li> <li>Prediction type: Type of prediction (e.g. cell boundaries prediction or nuclei...).</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/extra/#widget-lifted-multi-cut","title":"Widget: Lifted Multi-Cut","text":"<p>As reported in our paper, if one has a nuclei signal imaged together with the boundary signal, we could leverage the fact that one cell contains only one nucleus and use the <code>LiftedMultict</code> segmentation strategy and obtain improved segmentation.</p> <pre><code>Traceback (most recent call last):\n  File \"/usr/share/miniconda/envs/plant-seg/lib/python3.12/site-packages/markdown_exec/_internal/formatters/python.py\", line 71, in _run_python\n    exec_python(code, code_block_id, exec_globals)\n  File \"/usr/share/miniconda/envs/plant-seg/lib/python3.12/site-packages/markdown_exec/_internal/formatters/_exec_python.py\", line 8, in exec_python\n    exec(compiled, exec_globals)  # noqa: S102\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"&lt;code block: n2&gt;\", line 8, in &lt;module&gt;\n    from plantseg.viewer_napari.widgets import widget_lifted_multicut\nImportError: cannot import name 'widget_lifted_multicut' from 'plantseg.viewer_napari.widgets' (/usr/share/miniconda/envs/plant-seg/lib/python3.12/site-packages/plantseg/viewer_napari/widgets/__init__.py)\n</code></pre>"},{"location":"chapters/plantseg_interactive_napari/import_export/","title":"Images Import and Export","text":""},{"location":"chapters/plantseg_interactive_napari/import_export/#widget-open-files","title":"Widget: Open Files","text":"<p> <p></p> <p></p>                      Open a file and return a napari layer.             <ul><li>File type: None</li> <li>File path (tiff, h5, zarr, png, jpg): Select a file to be imported, the file can be a tiff, h5, png, jpg.</li> <li>Layer name: Define the name of the output layer, default is either image or label.</li> <li>Layer type: Select if the image is a normal image or a segmentation</li> <li>Stack layout: Stack layout</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/import_export/#widget-export-stacks","title":"Widget: Export Stacks","text":"<p> <p></p> <p></p>                      None             <ul><li>Layer to export: Select all layer to be exported, and (optional) set a custom file name suffix that will be appended at end of the layer name.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/import_export/#widget-show-info","title":"Widget: Show Info","text":"<p> <p></p> <p></p>                      Show the information of the selected layer.             <ul><li>Select layer: Select the image or label to show the information.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/preprocessing/","title":"Preprocessing","text":"<p>This section describes the data processing functionalities available in PlantSeg Interactive. This functionality are available in the <code>Preprocessing</code> tab in the PlantSeg Interactive GUI.</p>"},{"location":"chapters/plantseg_interactive_napari/preprocessing/#widget-gaussian-smoothing","title":"Widget: Gaussian Smoothing","text":"<p> <p></p> <p></p>                      Apply Gaussian smoothing to an image layer.             <ul><li>Image: Image layer to apply the smoothing.</li> <li>Sigma: Define the size of the gaussian smoothing kernel. The larger the more blurred will be the output image.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/preprocessing/#widget-image-rescaling","title":"Widget: Image Rescaling","text":"From factorTo layer voxel sizeTo layer shapeTo model voxel sizeTo voxel sizeTo shapeSet voxel size <p>Using the <code>From factor</code> mode, the user can rescale the image by a multiplicate factor. For example, if the image has a shape <code>(10, 10, 10)</code> and the user wants to rescale it by a factor of <code>(2, 2, 2)</code>, the new size will be <code>(20, 20, 20)</code>.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Rescaling factor: Define the scaling factor to use for resizing the input image.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To layer voxel size</code> mode, the user can rescale the image to the voxel size of a specific layer. For example, if two images are loaded in the viewer, one with a voxel size of <code>(0.1, 0.1, 0.1)um</code> and the other with a voxel size of <code>(0.1, 0.05, 0.05)um</code>, the user can rescale the first image to the voxel size of the second image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Reference layer: Rescale to same voxel size as selected layer.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To layer shape</code> mode, the user can rescale the image to the shape of a specific layer. For example, if two images are loaded in the viewer, one with a shape <code>(10, 10, 10)</code> and the other with a shape <code>(20, 20, 20)</code>, the user can rescale the first image to the shape of the second image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Reference layer: Rescale to same voxel size as selected layer.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To model voxel size</code> mode, the user can rescale the image to the voxel size of the model. For example, if the model has been trained with data at voxel size of <code>(0.1, 0.1, 0.1)um</code>, the user can rescale the image to this voxel size.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Reference model: Rescale to same voxel size as selected model.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To voxel size</code> mode, the user can rescale the image to a specific voxel size.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Out voxel size: Define the output voxel size. Units are same as imported, (if units are missing default is \"um\").</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To shape</code> mode, the user can rescale the image to a specific shape.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Out shape: Rescale to a manually selected shape.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>Set voxel size</code> mode, the user can set the voxel size of the image to a specific value. This only changes the metadata of the image and does not rescale the image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Out voxel size: Define the output voxel size. Units are same as imported, (if units are missing default is \"um\").</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/preprocessing/#widget-image-pair-operations","title":"Widget: Image Pair Operations","text":"<p> <p></p> <p></p>                      Apply an operation to two image layers.             <ul><li>Image 1: First image to apply the operation.</li> <li>Image 2: Second image to apply the operation.</li> <li>Operation: None</li> <li>Normalize input: Normalize the input images to the range [0, 1].</li> <li>Clip output: Clip the output to the range [0, 1].</li> <li>Normalize output: Normalize the output image to the range [0, 1].</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/proofreading_auto/","title":"Post-processing (Automatic Proofreading)","text":""},{"location":"chapters/plantseg_interactive_napari/proofreading_auto/#widget-use-foreground-to-remove-false-positives","title":"Widget: Use Foreground to Remove False Positives","text":"<p> <p></p> <p></p>                      Remove false positives from a segmentation layer using a foreground probability layer.             <ul><li>Segmentation: Segmentation layer to remove false positives.</li> <li>Foreground: Foreground probability layer.</li> <li>Threshold: Threshold value to remove false positives.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/proofreading_auto/#widget-use-nuclei-to-fix-under-and-over-segmentation","title":"Widget: Use Nuclei to Fix Under- and Over-segmentation","text":"<p> <p></p> <p></p>                           Widget for correcting over- and under-segmentation of cells based on nuclei segmentation.      This widget allows users to adjust cell segmentation by leveraging nuclei segmentation. It supports     merging over-segmented cells and splitting under-segmented cells, with optional boundary refinement.      Args:         segmentation_cells (Labels): Input layer representing segmented cell instances.         segmentation_nuclei (Labels): Input layer representing segmented nuclei instances.         boundary_pmaps (Image | None, optional): Optional boundary probability map (same shape as input layers).             Higher values indicate probable cell boundaries, used to refine segmentation.         threshold (tuple[float, float], optional): Merge and split thresholds as percentages (0-100).             - The first value is the merge threshold: cells with nuclei overlap below this value are merged.             - The second value is the split threshold: cells with nuclei overlap above this value are split.             Default is (33, 66).         quantile (tuple[float, float], optional): Minimum and maximum quantile values for filtering nuclei sizes (0-100).             - The first value excludes the smallest nuclei (e.g., \"0.3\" excludes the smallest 0.3%).             - The second value excludes the largest nuclei (e.g., \"99.9\" excludes the largest 0.1%).             Default is (0.3, 99.9).      Returns:         None                  <ul><li>Cell instances: Input layer representing segmented cell instances.</li> <li>Nuclear instances: Input layer representing segmented nuclei instances.</li> <li>Boundary image: Optional boundary probability map (same shape as input layers). Higher values indicate probable cell boundaries, used to refine segmentation.</li> <li>Boundary Threshold (%): Set the percentage range for merging (first value) and splitting (second value) cells. For example, \"33\" means cells with less than 33% overlap with nuclei are merged, and \"66\" means cells with more than 66% overlap are split.</li> <li>Nuclei Size Filter (%): Set the size range to filter nuclei, represented as percentages. For example, \"0.3\" excludes the smallest 30%, and \"99.9\" excludes the largest 0.1% of nuclei.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/proofreading_manual/","title":"Manual Proofreading","text":""},{"location":"chapters/plantseg_interactive_napari/proofreading_manual/#widget-proofreading-with-manual-split-and-merge","title":"Widget: Proofreading with Manual Split and Merge","text":"<p>Once the segmentation is done, or a label image is loaded along with a boundary image, the user can start the proofreading process. In the proofreading widget, set the \"Mode\" to \"Layer\" if you are going to correct a loaded image, or set the \"Mode\" to \"File\" if you already had a proofreading session exported as a file. For \"Segmentation\", select the image layer that you want to correct, then click \"Initialise\" to start the proofreading process.</p> <p>After a few second of initialisation, the initialised proofreading widget will be displayed, and two new layers will be added to the Napari viewer, \"Scribbles\" and \"Correct Labels\". Click \"Scribbles\" first, and you may draw on this layer with the \"paint brush\" tool - you can press <code>2</code> or <code>P</code> key to activate the brush tool or find it on the \"layer controls\" panel of the Napari viewer. The scribbles drawn on this layer will be used to split or merge the segmentation: strokes with the same color will merge the instance(s) in the segmentation layer, and strokes with different colors will split the instance(s) in the segmentation layer. After drawing the scribbles, click \"Split / Merge\" or press <code>n</code> to apply the scribbles to the segmentation layer. If you want to undo the last split or merge operation, click \"Undo Last Action\".</p> <p>When you are done or want to stop the proofreading session, you may save the current state of the proofreading session by clicking \"Save current proofreading snapshot\" into a snapshot file, which can be loaded into the proofreading widget later in another session of PlantSeg.</p>"},{"location":"chapters/plantseg_interactive_napari/proofreading_manual/#keybinding","title":"Keybinding","text":"<ul> <li><code>s</code>: Save stack</li> <li><code>n</code>: Merge or split from seeds</li> <li><code>ctrl+n</code>: Undo merge or split from seeds</li> <li><code>c</code>: Clean seeds</li> <li><code>o</code>: Mark/un-mark correct segmentation</li> <li><code>b</code>: show/un-show correct segmentation layer</li> <li><code>j</code>: Update boundaries from segmentation</li> <li><code>k</code>: Update segmentation from boundaries</li> <li><code>ctrl + arrows</code>: to move the field of view</li> <li><code>alt + down/up arrows</code>: to increase or decrease the field of view</li> </ul>"},{"location":"chapters/plantseg_interactive_napari/unet_gasp_workflow/","title":"Main PlantSeg Workflow","text":""},{"location":"chapters/plantseg_interactive_napari/unet_gasp_workflow/#widget-neural-network-prediction-find-boundary","title":"Widget: Neural Network Prediction - Find Boundary","text":"PlantSeg ZooBioImage.IO Model Zoo <p> <p></p> <p></p>                      None             <ul><li>Mode: Select the mode to run the prediction.</li> <li>Image: Raw image to be processed with a neural network.</li> <li>Model filter: Choose to only show models tagged with `plantseg`.</li> <li>PlantSeg model: Select a pretrained PlantSeg model. Current model description: Unet trained on confocal images of Arabidopsis Ovules on 1/2-resolution in XY with BCEDiceLoss.</li> <li>BioImage.IO model: Select a model from BioImage.IO model zoo.</li> </ul> </p> <p> <p></p> <p></p>                      None             <ul><li>Mode: Select the mode to run the prediction.</li> <li>Image: Raw image to be processed with a neural network.</li> <li>Model filter: Choose to only show models tagged with `plantseg`.</li> <li>BioImage.IO model: Select a model from BioImage.IO model zoo.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/unet_gasp_workflow/#widget-watershed-generate-superpixels","title":"Widget: Watershed - Generate Superpixels","text":"<p> <p></p> <p></p>                      None             <ul><li>Boundary image: Raw boundary image or boundary prediction to use as input for Watershed.</li> <li>Mode: Define if the Watershed will run slice by slice (faster) or on the full volume (slower).</li> <li>Boundary threshold: A low value will increase over-segmentation tendency and a large value increase under-segmentation tendency.</li> <li>Minimum segment size: Minimum segment size allowed in voxels.</li> <li>Show advanced parameters: Show advanced parameters for the Watershed algorithm.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/unet_gasp_workflow/#widget-agglomeration-merge-superpixels-into-instances","title":"Widget: Agglomeration - Merge Superpixels into Instances","text":"<p> <p></p> <p></p>                      None             <ul><li>Boundary image: Raw boundary image or boundary prediction to use as input for clustering.</li> <li>Nuclei foreground: Nuclei foreground prediction or segmentation.</li> <li>Over-segmentation: Over-segmentation labels layer (superpixels) to use as input for clustering.</li> <li>Agglomeration mode: Select which agglomeration algorithm to use.</li> <li>Under/Over segmentation factor: A low value will increase under-segmentation tendency and a large value increase over-segmentation tendency.</li> <li>Minimum segment size: Minimum segment size allowed in voxels.</li> </ul> </p>"},{"location":"chapters/plantseg_interactive_napari/unet_training/","title":"UNet Training","text":"<p>Feature in Progress</p> <p>This feature is under development.</p>"},{"location":"chapters/plantseg_legacy/installation/","title":"Installation","text":""},{"location":"chapters/plantseg_legacy/installation/#prerequisites-for-conda-package","title":"Prerequisites for Conda package","text":"<ul> <li>Linux, Windows, macOS (not all features are available on macOS)</li> <li>(Optional) Nvidia GPU with official Nvidia drivers installed for GPU acceleration</li> </ul>"},{"location":"chapters/plantseg_legacy/installation/#install-mamba","title":"Install Mamba","text":"<p>The easiest way to install PlantSeg is by using the conda (Anaconda) or mamba (Miniforge) package manager. We recommend using <code>mamba</code> because it is faster and usually more consistent than <code>conda</code>.</p> LinuxWindows/macOS <p>To download Miniforge open a terminal and type:</p> <pre><code>curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <p>Then install by typing:</p> <pre><code>bash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <p>and follow the installation instructions. Please refer to the Miniforge repo for more information, troubleshooting and uninstallation instructions. The miniforge installation file <code>Miniforge3-*.sh</code> can be deleted now.</p> <p>The first step required to use the pipeline is installing mamba. The installation can be done by downloading the installer from the Miniforge repo. There you can find the download links for the latest version of Miniforge, troubleshooting and uninstallation instructions.</p>"},{"location":"chapters/plantseg_legacy/installation/#install-plantseg-using-mamba","title":"Install PlantSeg using Mamba","text":"<p>PlantSeg can be installed directly by executing in the terminal (or PowerShell on Windows). For <code>conda</code> users, the command is identical, just replace <code>mamba</code> with <code>conda</code>.</p> LinuxWindowsmacOS <ul> <li> <p>NVIDIA GPU version, CUDA=12.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=12.1 plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> <li> <p>NVIDIA GPU version, CUDA=11.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=11.8 plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> <li> <p>CPU version</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch cpuonly plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> </ul> <ul> <li> <p>NVIDIA GPU version, CUDA=12.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=12.1 nifty=1.2.1=*_4 plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> <li> <p>NVIDIA GPU version, CUDA=11.x</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch pytorch-cuda=11.8 nifty=1.2.1=*_4 plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> <li> <p>CPU version</p> <pre><code>mamba create -n plant-seg -c pytorch -c nvidia -c conda-forge pytorch cpuonly nifty=1.2.1=*_4 plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> </ul> <ul> <li> <p>Apple silicon version</p> <pre><code>mamba create -n plant-seg -c pytorch -c conda-forge python=3.11 pytorch::pytorch plant-seg=1.8.1 bioimageio.core --no-channel-priority\n</code></pre> </li> </ul> <p>If you used older versions of PlantSeg, please delete the old config files in <code>~/.plantseg_models/configs/</code> after installing new PlantSeg.</p> <p>The above command will create new conda environment <code>plant-seg</code> together with all required dependencies.</p> <p>Please refer to the PyTorch website for more information on the available versions of PyTorch and the required CUDA version. The GPU version of Pytorch will also work on CPU only machines but has a much larger installation on disk.</p>"},{"location":"chapters/plantseg_legacy/installation/#optional-dependencies","title":"Optional dependencies","text":"<p>Certain compressed TIFF files (e.g., Zlib, ZSTD, LZMA formats) require additional codecs to be processed correctly by PlantSeg. To handle such files, install the <code>imagecodecs</code> package:</p> <pre><code>conda activate plant-seg\npip install imagecodecs\n</code></pre> <p>If you plan to use SimpleITK-based watershed segmentation, you will need to install <code>SimpleITK</code> as an additional dependency:</p> <pre><code>conda activate plant-seg\npip install SimpleITK\n</code></pre>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/","title":"PlantSeg Classic CLI","text":"<p>Deprecated</p> <p>This interface is deprecated and has been removed from PlantSeg v2. Please use the Napari viewer or the command line interface instead, or install PlantSeg v1.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#guide-to-custom-configuration-file","title":"Guide to Custom Configuration File","text":"<p>The configuration file defines all the operations in our pipeline together with the data to be processed. Please refer to config.yaml for a sample pipeline configuration and a detailed explanation of all parameters.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#main-keyssteps","title":"Main Keys/Steps","text":"<ul> <li><code>path</code> attribute: is used to define either the file to process or the directory containing the data.</li> <li><code>preprocessing</code> attribute: contains a simple set of possible operations one would need to run on their data before calling the neural network. This step can be skipped if data is ready for neural network processing. Detailed instructions can be found at Classic GUI (Data Processing).</li> <li><code>cnn_prediction</code> attribute: contains all parameters relevant for predicting with a neural network. Description of all pre-trained models provided with the package is described below. Detailed instructions can be found at Classic GUI (Prediction).</li> <li><code>segmentation</code> attribute: contains all parameters needed to run the partitioning algorithm (i.e., final Segmentation). Detailed instructions can be found at Classic GUI (Segmentation).</li> </ul>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#additional-information","title":"Additional information","text":"<p>The PlantSeg-related files (models, configs) will be placed inside your home directory under <code>~/.plantseg_models</code>.</p> <p>Our pipeline uses the PyTorch library for CNN prediction. PlantSeg can be run on systems without GPU, however for maximum performance, we recommend that the application is run on a machine with a high-performance GPU for deep learning. If the <code>CUDA_VISIBLE_DEVICES</code> environment variable is not specified, the prediction task will be distributed on all available GPUs. E.g. run: <code>CUDA_VISIBLE_DEVICES=0 plantseg --config CONFIG_PATH</code> to restrict prediction to a given GPU.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#configuration-file-example","title":"configuration file example","text":"<p>This modality of using PlantSeg is particularly suited for high throughput processing and for running PlantSeg on a remote server. To use PlantSeg from command line mode, you will need to create a configuration file using a standard text editor  or using the save option of the PlantSeg GUI.</p> <p>Here is an example configuration:</p> <pre><code>path: /home/USERNAME/DATA.tiff # Contains the path to the directory or file to process\n\npreprocessing:\n  # enable/disable preprocessing\n  state: True\n  # create a new sub folder where all results will be stored\n  save_directory: \"PreProcessing\"\n  # rescaling the volume is essential for the generalization of the networks. The rescaling factor can be computed as the resolution\n  # of the volume at hand divided by the resolution of the dataset used in training. Be careful, if the difference is too large check for a different model.\n  factor: [1.0, 1.0, 1.0]\n  # the order of the spline interpolation\n  order: 2\n  # optional: perform Gaussian smoothing or median filtering on the input.\n  filter:\n    # enable/disable filtering\n    state: False\n    # Accepted values: 'gaussian'/'median'\n    type: gaussian\n    # sigma (gaussian) or disc radius (median)\n    param: 1.0\n\ncnn_prediction:\n  # enable/disable UNet prediction\n  state: True\n  # Trained model name, more info on available models and custom models in the README\n  model_name: \"generic_confocal_3D_unet\"\n  # If a CUDA capable gpu is available and corrected setup use \"cuda\", if not you can use \"cpu\" for cpu only inference (slower)\n  device: \"cpu\"\n  # how many subprocesses to use for data loading\n  num_workers: 8\n  # patch size given to the network (adapt to fit in your GPU mem)\n  patch: [32, 128, 128]\n  # stride between patches will be computed as `stride_ratio * patch`\n  # recommended values are in range `[0.5, 0.75]` to make sure the patches have enough overlap to get smooth prediction maps\n  stride_ratio: 0.75\n  # If \"True\" forces downloading networks from the online repos\n  model_update: False\n\ncnn_postprocessing:\n  # enable/disable cnn post processing\n  state: False\n  # if True convert to result to tiff\n  tiff: False\n  # rescaling factor\n  factor: [1, 1, 1]\n  # spline order for rescaling\n  order: 2\n\nsegmentation:\n  # enable/disable segmentation\n  state: True\n  # Name of the algorithm to use for inferences. Options: MultiCut, MutexWS, GASP, DtWatershed\n  name: \"MultiCut\"\n  # Segmentation specific parameters here\n  # balance under-/over-segmentation; 0 - aim for undersegmentation, 1 - aim for oversegmentation. (Not active for DtWatershed)\n  beta: 0.5\n  # directory where to save the results\n  save_directory: \"MultiCut\"\n  # enable/disable watershed\n  run_ws: True\n  # use 2D instead of 3D watershed\n  ws_2D: True\n  # probability maps threshold\n  ws_threshold: 0.5\n  # set the minimum superpixels size\n  ws_minsize: 50\n  # sigma for the gaussian smoothing of the distance transform\n  ws_sigma: 2.0\n  # sigma for the gaussian smoothing of boundary\n  ws_w_sigma: 0\n  # set the minimum segment size in the final segmentation. (Not active for DtWatershed)\n  post_minsize: 50\n\nsegmentation_postprocessing:\n  # enable/disable segmentation post processing\n  state: False\n  # if True convert to result to tiff\n  tiff: False\n  # rescaling factor\n  factor: [1, 1, 1]\n  # spline order for rescaling (keep 0 for segmentation post processing\n  order: 0\n</code></pre> <p>This configuration can be found at config.yaml.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#pipeline-usage-command-line","title":"Pipeline Usage (command line)","text":"<p>To start PlantSeg from the command line. First, activate the newly created conda environment with:</p> <pre><code>conda activate plant-seg\n</code></pre> <p>then, one can just start the pipeline with</p> <pre><code>plantseg --config CONFIG_PATH\n</code></pre> <p>where <code>CONFIG_PATH</code> is the path to a YAML configuration file.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#data-parallelism","title":"Data Parallelism","text":"<p>In the headless mode (i.e. when invoked with <code>plantseg --config CONFIG_PATH</code>) the prediction step will run on all the GPUs using DataParallel. If prediction on all available GPUs is not desirable, restrict the number of GPUs using <code>CUDA_VISIBLE_DEVICES</code>, e.g.</p> <pre><code>CUDA_VISIBLE_DEVICES=0,1 plantseg --config CONFIG_PATH\n</code></pre>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#results","title":"Results","text":"<p>The results are stored together with the source input files inside a nested directory structure. As an example, if we want to run PlantSeg inside a directory with two stacks, we will obtain the following outputs:</p> <pre><code>/file1.tif\n/file2.tif\n/PreProcesing/\n------------&gt;/file1.h5\n------------&gt;/file1.yaml\n------------&gt;/file2.h5\n------------&gt;/file2.yaml\n------------&gt;/generic_confocal_3d_unet/\n-------------------------------------&gt;/file1_prediction.h5\n-------------------------------------&gt;/file1_prediction.yaml\n-------------------------------------&gt;/file2_prediction.h5\n-------------------------------------&gt;/file2_prediction.yaml\n-------------------------------------&gt;/GASP/\n------------------------------------------&gt;/file_1_predions_gasp_average.h5\n------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n------------------------------------------&gt;/file_2_predions_gasp_average.h5\n------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n------------------------------------------&gt;/PostProcessing/\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n</code></pre> <p>The use of this hierarchical directory structure allows PlantSeg to find the necessary files quickly and can be used to test different segmentation algorithms/parameter combinations minimizing the memory overhead on the disk. For the sake of reproducibility, every file is associated with a configuration file \".yaml\" that saves all parameters used to produce the result.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_cli/#liftedmulticut-segmentation","title":"LiftedMulticut segmentation","text":"<p>As reported in our paper, if one has a nuclei signal imaged together with the boundary signal, we could leverage the fact that one cell contains only one nucleus and use the <code>LiftedMultict</code> segmentation strategy and obtain improved segmentation. We will use the Arabidopsis thaliana lateral root as an example. The <code>LiftedMulticut</code> strategy consists of running PlantSeg two times:</p> <ol> <li> <p>Using PlantSeg to predict the nuclei probability maps using the <code>lightsheet_unet_bce_dice_nuclei_ds1x</code> network. In this case, only the pre-processing and CNN prediction steps are enabled in the config. See example nuclei prediction config.</p> <pre><code>plantseg --config nuclei_predictions_example.yaml\n</code></pre> </li> <li> <p>Using PlantSeg to segment the input image with the <code>LiftedMulticut</code> algorithm given the nuclei probability maps from the 1st step. See example lifted multicut config. The notable difference is that in the <code>segmentation</code> part of the config, we set <code>name: LiftedMulticut</code> and the <code>nuclei_predictions_path</code> as the path to the directory where the nuclei pmaps were saved in step 1. Also, make sure that the <code>path</code> attribute points to the raw files containing the cell boundary staining (NOT THE NUCLEI).</p> <pre><code>plantseg --config lifted_multicut_example.yaml\n</code></pre> </li> </ol> <p>If case when the nuclei segmentation is given, one should skip step 1., add <code>is_segmentation=True</code> flag in the example lifted multicut config and directly run step 2.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/","title":"PlantSeg from GUI","text":"<p>Deprecated</p> <p>This interface is deprecated and has been removed from PlantSeg v2. Please use the Napari viewer or the command line interface instead, or install PlantSeg v1.</p> <p>The graphical user interface is the easiest way to configure and run PlantSeg. Currently the GUI does not allow to visualize or interact with the data. We recommend using MorphographX or Fiji in order to assert the success and quality of the pipeline results.</p> <p></p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/#file-browser-widget","title":"File Browser Widget","text":"<p>The file browser can be used to select the input files for the pipeline. PlantSeg can run on a single file (button A) or in batch mode for all files inside a directory (button B). If a directory is selected PlantSeg will run on all compatible files inside the directory.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/#main-pipeline-configurator","title":"Main Pipeline Configurator","text":"<p>The central panel of PlantSeg (C) is the core of the pipeline configuration. It can be used for customizing and tuning the pipeline accordingly to the data at hand. Detailed information for each stage can be found at:</p> <ul> <li>Data-Processing</li> <li>CNN-Prediction</li> <li>Segmentation</li> </ul> <p>Any of the above widgets can be run singularly or in sequence (left to right). The order of execution can not be modified.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/#run","title":"Run","text":"<p>The last panel has two main functions. Running the pipeline (D), once the run button is pressed the pipeline starts. The button is inactive until the process is finished. Adding a custom model (E). Custom trained model can be done by using the dedicated popup. Training a new model can be done following the instruction at pytorch-3dunet.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/#results","title":"Results","text":"<p>The results are stored together with the source input files inside a nested directory structure. As example, if we want to run PlantSeg inside a directory with 2 stacks, we will obtain the following outputs:</p> <pre><code>/file1.tif\n/file2.tif\n/PreProcesing/\n------------&gt;/file1.h5\n------------&gt;/file1.yaml\n------------&gt;/file2.h5\n------------&gt;/file2.yaml\n------------&gt;/generic_confocal_3d_unet/\n-------------------------------------&gt;/file1_prediction.h5\n-------------------------------------&gt;/file1_prediction.yaml\n-------------------------------------&gt;/file2_prediction.h5\n-------------------------------------&gt;/file2_prediction.yaml\n-------------------------------------&gt;/GASP/\n------------------------------------------&gt;/file_1_predions_gasp_average.h5\n------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n------------------------------------------&gt;/file_2_predions_gasp_average.h5\n------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n------------------------------------------&gt;/PostProcessing/\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_1_predions_gasp_average.yaml\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.tiff\n---------------------------------------------------------&gt;/file_2_predions_gasp_average.yaml\n</code></pre> <p>The use of this hierarchical directory structure allows PlantSeg to easily find the necessary files and can be used to test different combination of segmentation algorithms/parameters minimizing the memory overhead on the disk. For sake of reproducibility, every file is associated with a configuration file \".yaml\" that saves all parameters used to produce the result.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/#start-plantseg-gui","title":"Start PlantSeg GUI","text":"<p>In order to start the PlantSeg app in GUI mode: First, activate the newly created conda environment with:</p> <pre><code>conda activate plant-seg\n</code></pre> <p>then, run the GUI by simply typing:</p> <pre><code>plantseg --gui\n</code></pre>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/cnn_prediction/","title":"CNN Prediction","text":"<p>Deprecated</p> <p>This interface is deprecated and has been removed from PlantSeg v2. Please use the Napari viewer or the command line interface instead, or install PlantSeg v1.</p> <p></p> <p>The CNN prediction widget process the stacks at hand with a Convolutional Neural Network. The output is a boundary classification image, where every voxel gets a value between 0 (not a cell boundary) and 1 (cell boundary).</p> <p>The input image can be a raw stack \"tiff\"/\"h5\" or the output of the PreProcessing widget.</p> <ul> <li> <p>The Model Name menu shows all available models. There are two main basic models available</p> <ol> <li> <p>Generic confocalis a generic model for all confocal datasets. Some examples: </p> </li> <li> <p>Generic lightsheet this is a generic model for all lightsheet datasets.  Some examples:  </p> </li> </ol> </li> <li> <p>Due to memory constraints, usually a complete stack does not fit the GPUs memory,  therefore the Patch size can be used to optimize the performance of the pipeline.  Usually, larger patches cost more memory but can slightly improve performance.  For 2D segmentation, the Patch size relative to the z-axis has to be set to 1.</p> </li> <li> <p>To minimize the boundary effect due to the sliding windows patching, we can use different stride:</p> <ol> <li>Accurate: corresponding to a stride 50% of the patch size (yield best prediction/segmentation accuracy)</li> <li>Balanced: corresponding to a stride 75% of the patch size</li> <li>Draft: corresponding to a stride 95% of the patch size (yield fastest runtime)</li> </ol> </li> <li> <p>The Device type menu can be used to enable or not GPU acceleration. CUDA greatly accelerates the network prediction on Nvidia GPUs. At the moment, we don't support other GPUs manufacturers.</p> </li> </ul>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/data_processing/","title":"Classic Data Processing","text":"<p>Deprecated</p> <p>This interface is deprecated and has been removed from PlantSeg v2. Please use the Napari viewer or the command line interface instead, or install PlantSeg v1.</p> <p> PlantSeg includes essential utilities for data pre-processing and post-processing.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/data_processing/#pre-processing","title":"Pre-Processing","text":"<p>The input for this widget can be either a \"raw\" image or a \"prediction\" image. Input formats allowed are tiff and h5, while output is always h5.</p> <ul> <li> <p>Save Directory can be used to define the output directory.</p> </li> <li> <p>The most critical setting is the Rescaling. It is important to rescale the image to  match the resolution of the data used for training the Neural Network. This operation can be done automatically by clicking on the GUI on Guided. Be careful to use this function only in case of data considerably different from the reference resolution.</p> </li> </ul> <pre><code>As an example:\n  - if your data has the voxel size of 0.3 x 0.1 x 0.1 (ZYX).\n  - and the networks was trained on 0.3 x 0.2 x 0.2 data (reference resolution).\n\nThe required voxel size can be obtained by computing the ratio between your data and the\nreference train dataset. In the example the rescaling factor = 1 x 2 x 2.\n</code></pre> <ul> <li> <p>The Interpolation field controls the interpolation type (0 for nearest neighbors, 1 for linear spline, 2 for quadratic).</p> </li> <li> <p>The last field defines a Filter operation. Implemented there are:</p> <ol> <li>Gaussian Filtering: The parameter is a float and defines the sigma value for the gaussian smoothing. The higher, the wider is filtering kernel.</li> <li>Median Filtering: Apply median operation convolutionally over the image.  The kernel is a sphere of size defined in the parameter field.</li> </ol> </li> </ul>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/data_processing/#post-processing","title":"Post-Processing","text":"<p>A post-processing step can be performed after the CNN-Prediction and the Segmentation. The post-processing options are:</p> <ul> <li> <p>Converting the output to the tiff file format (default is h5).</p> </li> <li> <p>Casting the CNN-Prediction output to data_uint8 drastically reduces the memory footprint of the output  file.</p> </li> </ul> <p>Additionally, the post-processing will scale back your outputs to the original voxels resolutions.</p>"},{"location":"chapters/plantseg_legacy/plantseg_classic_gui/segmentation/","title":"Segmentation","text":"<p>Deprecated</p> <p>This interface is deprecated and has been removed from PlantSeg v2. Please use the Napari viewer or the command line interface instead, or install PlantSeg v1.</p> <p>The segmentation widget allows using very powerful graph partitioning techniques to obtain a segmentation from the input stacks. The input of this widget should be the output of the CNN-prediction widget. If the boundary prediction stage fails for any reason, a raw image could be used (especially if the cell boundaries are  very sharp, and the noise is low) but usually does not yield satisfactory results.</p> <ul> <li> <p>The Algorithm menu can be used to choose the segmentation algorithm. Available choices are:</p> <ol> <li>GASP (average): is a generalization of the classical hierarchical clustering. It usually delivers very reliable and accurate segmentation. It is the default in PlantSeg.</li> <li>MutexWS: Mutex Watershed is a derivative of the standard Watershed, where we do not need seeds for the  segmentation. This algorithm performs very well in certain types of complex morphology (like )</li> <li>MultiCut: in contrast to the other algorithms is not based on a greedy agglomeration but tries to find the optimal global segmentation. This is, in practice, very hard, and it can be infeasible for huge stacks.</li> <li>DtWatershed: is our implementation of the distance transform Watershed. From the input, we extract a distance map from the boundaries. Based this distance map, seeds are placed at local minima. Then those seeds are used for computing the Watershed segmentation. To speed up the computation of GASP, MutexWS, and MultiCut, an over-segmentation  is obtained using Dt Watershed.</li> </ol> </li> <li> <p>Save Directory defines the sub-directory's name where the segmentation results will be stored.</p> </li> <li> <p>The Under/Over- segmentation factor is the most critical parameters for tuning the segmentation of GASP, MutexWS and MultiCut. A small value will steer the segmentation towards under-segmentation. While a high-value bias the segmentation towards the over-segmentation. This parameter does not affect the distance transform Watershed.</p> </li> <li> <p>If Run Watershed in 2D value is True, the superpixels are created in 2D (over the z slice). While if False makes the superpixels in the whole 3D volume. 3D superpixels are much slower and memory intensive but can improve  the segmentation accuracy.</p> </li> <li> <p>The CNN Prediction Threshold is used for the superpixels extraction and Distance Transform Watershed. It has a crucial role for the watershed seeds extraction and can be used similarly to the \"Unde/Over segmentation factor.\" to bias the final result. A high value translates to less seeds being placed (more under segmentation), while with a low value, more seeds are  placed (more over-segmentation).</p> </li> <li> <p>The input is used by the distance transform Watershed to extract the seed and find the segmentation boundaries. If Watershed Seeds Sigma and Watershed Boundary Sigma are larger than  zero, a gaussian smoothing is applied on the input before the operations above. This is mainly helpful for  the seeds computation but, in most cases, does not impact segmentation quality.</p> </li> <li> <p>The Superpixels Minimum Size applies a size filter to the initial superpixels over-segmentation. This removes Watershed often produces small segments and is usually helpful for the subsequent agglomeration.  Segments smaller than the threshold will be merged with the nearest neighbor segment.</p> </li> <li> <p>Even though GASP, MutexWS, and MultiCut are not very prone to produce small segments, the Cell Minimum Size can be used as a final size processing filter. Segments smaller than the threshold will be merged with the nearest neighbor cell.</p> </li> </ul>"},{"location":"chapters/plantseg_models/","title":"Official Data and Models","text":""},{"location":"chapters/plantseg_models/#datasets","title":"Datasets","text":"<p>We publicly release the datasets used for training the networks which available as part of the PlantSeg package. Please refer to our publication for more details about the datasets:</p> <ul> <li>Arabidopsis thaliana ovules dataset (raw confocal images + ground truth labels)</li> <li>Arabidopsis thaliana lateral root (raw light sheet images + ground truth labels)</li> </ul> <p>Both datasets can be downloaded from our OSF project</p>"},{"location":"chapters/plantseg_models/#pre-trained-networks","title":"Pre-trained Networks","text":"<p>The following pre-trained networks are provided with PlantSeg package out-of-the box and can be specified in the config file or chosen in the GUI.</p> <ul> <li><code>generic_confocal_3D_unet</code> - alias for <code>confocal_3D_unet_ovules_ds2x</code> see below</li> <li><code>generic_light_sheet_3D_unet</code> - alias for <code>lightsheet_3D_unet_root_ds1x</code> see below</li> <li><code>confocal_3D_unet_ovules_ds1x</code> - a variant of 3D U-Net trained on confocal images of Arabidopsis ovules on original resolution, voxel size: (0.235x0.075x0.075 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>confocal_3D_unet_ovules_ds2x</code> - a variant of 3D U-Net trained on confocal images of Arabidopsis ovules on 1/2 resolution, voxel size: (0.235x0.150x0.150 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>confocal_3D_unet_ovules_ds3x</code> - a variant of 3D U-Net trained on confocal images of Arabidopsis ovules on 1/3 resolution, voxel size: (0.235x0.225x0.225 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>confocal_2D_unet_ovules_ds2x</code> - a variant of 2D U-Net trained on confocal images of Arabidopsis ovules. Training the 2D U-Net is done on the Z-slices (1/2 resolution, pixel size: 0.150x0.150 \u00b5m^3) with BCEDiceLoss</li> <li><code>confocal_3D_unet_ovules_nuclei_ds1x</code> - a variant of 3D U-Net trained on confocal images of Arabidopsis ovules nuclei stain on original resolution, voxel size: (0.35x0.1x0.1 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_3D_unet_root_ds1x</code> - a variant of 3D U-Net trained on light-sheet images of Arabidopsis lateral root on original resolution, voxel size: (0.25x0.1625x0.1625 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_3D_unet_root_ds2x</code> - a variant of 3D U-Net trained on light-sheet images of Arabidopsis lateral root on 1/2 resolution, voxel size: (0.25x0.325x0.325 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_3D_unet_root_ds3x</code> - a variant of 3D U-Net trained on light-sheet images of Arabidopsis lateral root on 1/3 resolution, voxel size: (0.25x0.4875x0.4875 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_2D_unet_root_ds1x</code> - a variant of 2D U-Net trained on light-sheet images of Arabidopsis lateral root. Training the 2D U-Net is done on the Z-slices (pixel size: 0.1625x0.1625 \u00b5m^3) with BCEDiceLoss</li> <li><code>lightsheet_3D_unet_root_nuclei_ds1x</code> - a variant of 3D U-Net trained on light-sheet images Arabidopsis lateral root nuclei on original resolution, voxel size: (0.25x0.1625x0.1625 \u00b5m^3) (ZYX) with BCEDiceLoss</li> <li><code>lightsheet_2D_unet_root_nuclei_ds1x</code> - a variant of 2D U-Net trained on light-sheet images Arabidopsis lateral root nuclei. Training the 2D U-Net is done on the Z-slices (pixel size: 0.1625x0.1625 \u00b5m^3) with BCEDiceLoss.</li> <li><code>confocal_3D_unet_sa_meristem_cells</code> - a variant of 3D U-Net trained on confocal images of shoot apical meristem dataset from: Jonsson, H., Willis, L., &amp; Refahi, Y. (2017). Research data supporting Cell size and growth regulation in the Arabidopsis thaliana apical stem cell niche. https://doi.org/10.17863/CAM.7793. voxel size: (0.25x0.25x0.25 \u00b5m^3) (ZYX)</li> <li><code>confocal_2D_unet_sa_meristem_cells</code> - a variant of 2D U-Net trained on confocal images of shoot apical meristem dataset from: Jonsson, H., Willis, L., &amp; Refahi, Y. (2017). Research data supporting Cell size and growth regulation in the Arabidopsis thaliana apical stem cell niche. https://doi.org/10.17863/CAM.7793.  pixel size: (25x0.25 \u00b5m^3) (YX)</li> <li><code>lightsheet_3D_unet_mouse_embryo_cells</code> - A variant of 3D U-Net trained to predict the cell boundaries in live light-sheet images of ex-vivo developing mouse embryo. Voxel size: (0.2\u00d70.2\u00d71 \u00b5m^3) (XYZ)</li> <li><code>confocal_3D_unet_mouse_embryo_nuclei</code> - A variant of 3D U-Net trained to predict the cell boundaries in live light-sheet images of ex-vivo developing mouse embryo. Voxel size: (0.2\u00d70.2\u00d71 \u00b5m^3) (XYZ)</li> </ul> <p>Selecting a given network name (either in the config file or GUI) will download the network into the <code>~/.plantseg_models</code> directory. Detailed description of network training can be found in our paper.</p> <p>The PlantSeg home directory can be configured with the <code>PLANTSEG_HOME</code> environment variable.</p> <pre><code>export PLANTSEG_HOME=\"/path/to/plantseg/home\"\n</code></pre>"},{"location":"chapters/plantseg_models/custom_datasets/","title":"Custom Datasets","text":"<p>Feature in Progress</p> <p>This feature is under development.</p>"},{"location":"chapters/plantseg_models/training/","title":"Training on New Data","text":"<p>For training new models we rely on the pytorch-3dunet. A similar configuration file can be used for training on new data and all the instructions can be found in the repo.</p>"},{"location":"chapters/plantseg_models/training/#adding-models","title":"Adding Models","text":"<ol> <li>Put these three files in one directory:<ol> <li>configuration file used for training: <code>config_train.yml</code></li> <li>snapshot of the best model across training: <code>best_checkpoint.pytorch</code></li> <li>snapshot of the last model saved during training: <code>last_checkpoint.pytorch</code></li> </ol> </li> <li>Click \"Add Custom Model\" in the GUI and follow the instruction</li> </ol>"},{"location":"chapters/plantseg_models/training/#alternative-old-method","title":"Alternative Old Method","text":"<p>When the network is trained, it is enough to create <code>~/.plantseg_models/MY_MODEL_NAME</code> directory and copy the following files into it:</p> <ul> <li>configuration file used for training: <code>config_train.yml</code></li> <li>snapshot of the best model across training: <code>best_checkpoint.pytorch</code></li> <li>snapshot of the last model saved during training: <code>last_checkpoint.pytorch</code></li> </ul> <p>The later two files are automatically generated during training and contain all neural networks parameters.</p> <p>Now you can simply use your model for prediction by setting the config.yaml key to <code>MY_MODEL_NAME</code>.</p> <p>If you want your model to be part of the open-source model zoo provided with this package, please contact us.</p>"},{"location":"chapters/python_api/","title":"Hierarchical Design of PlantSeg","text":"<p>PlantSeg is organized into three layers:</p> <ol> <li>Functionals (Python API): The foundational layer of PlantSeg, providing its core functionality. This layer can be accessed directly in Python scripts or Jupyter notebooks.</li> <li>Tasks: The intermediate layer of PlantSeg, which encapsulates the functionals to handle resource management and support distributed computing.</li> <li>Napari Widgets: The top layer of PlantSeg, which integrates tasks into user-friendly widgets for easy interaction within graphical interfaces.</li> </ol>"},{"location":"chapters/python_api/functionals/cnn_prediction/","title":"PlantSeg CNN Prediction","text":""},{"location":"chapters/python_api/functionals/cnn_prediction/#plantseg.functionals.prediction.prediction.unet_prediction","title":"<code>plantseg.functionals.prediction.prediction.unet_prediction(raw: np.ndarray, input_layout: ImageLayout, model_name: str | None, model_id: str | None, patch: tuple[int, int, int] | None = None, patch_halo: tuple[int, int, int] | None = None, single_batch_mode: bool = True, device: str = 'cuda', model_update: bool = False, disable_tqdm: bool = False, config_path: Path | None = None, model_weights_path: Path | None = None, tracker=None) -&gt; np.ndarray</code>","text":"<p>Generate prediction from raw data using a specified 3D U-Net model.</p> <p>This function handles both single and multi-channel outputs from the model, returning appropriately shaped arrays based on the output channel configuration.</p> <p>For Bioimage.IO Model Zoo models, weights are downloaded and loaded into <code>UNet3D</code> or <code>UNet2D</code> in <code>plantseg.training.model</code>, i.e. <code>bioimageio.core</code> is not used. <code>biio_prediction()</code> uses <code>bioimageio.core</code> for loading and running models.</p> <p>Parameters:</p> <ul> <li> <code>raw</code>               (<code>ndarray</code>)           \u2013            <p>Raw input data.</p> </li> <li> <code>Input_layout</code>               (<code>ImageLayout</code>)           \u2013            <p>The layout of the input data.</p> </li> <li> <code>model_name</code>               (<code>str | None</code>)           \u2013            <p>The name of the model to use.</p> </li> <li> <code>model_id</code>               (<code>str | None</code>)           \u2013            <p>The ID of the model from the BioImage.IO model zoo.</p> </li> <li> <code>patch</code>               (<code>tuple[int, int, int]</code>, default:                   <code>None</code> )           \u2013            <p>Patch size for prediction. Defaults to (80, 160, 160).</p> </li> <li> <code>patch_halo</code>               (<code>tuple[int, int, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Halo size around patches. Defaults to None.</p> </li> <li> <code>single_batch_mode</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use a single batch for prediction. Defaults to True.</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda'</code> )           \u2013            <p>The computation device ('cpu', 'cuda', etc.). Defaults to 'cuda'.</p> </li> <li> <code>model_update</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to update the model to the latest version. Defaults to False.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, disables the tqdm progress bar. Defaults to False.</p> </li> <li> <code>config_path</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model configuration file. Defaults to None.</p> </li> <li> <code>model_weights_path</code>               (<code>Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model weights file. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: The predicted boundaries as a 3D (Z, Y, X) or 4D (C, Z, Y, X) array, normalized between 0 and 1.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If neither <code>model_name</code>, <code>model_id</code>, nor <code>config_path</code> are provided.</p> </li> </ul> Source code in <code>plantseg/functionals/prediction/prediction.py</code> <pre><code>def unet_prediction(\n    raw: np.ndarray,\n    input_layout: ImageLayout,\n    model_name: str | None,\n    model_id: str | None,\n    patch: tuple[int, int, int] | None = None,\n    patch_halo: tuple[int, int, int] | None = None,\n    single_batch_mode: bool = True,\n    device: str = \"cuda\",\n    model_update: bool = False,\n    disable_tqdm: bool = False,\n    config_path: Path | None = None,\n    model_weights_path: Path | None = None,\n    tracker=None,\n) -&gt; np.ndarray:\n    \"\"\"Generate prediction from raw data using a specified 3D U-Net model.\n\n    This function handles both single and multi-channel outputs from the model,\n    returning appropriately shaped arrays based on the output channel configuration.\n\n    For Bioimage.IO Model Zoo models, weights are downloaded and loaded into `UNet3D` or `UNet2D`\n    in `plantseg.training.model`, i.e. `bioimageio.core` is not used. `biio_prediction()` uses\n    `bioimageio.core` for loading and running models.\n\n    Args:\n        raw (np.ndarray): Raw input data.\n        Input_layout (ImageLayout): The layout of the input data.\n        model_name (str | None): The name of the model to use.\n        model_id (str | None): The ID of the model from the BioImage.IO model zoo.\n        patch (tuple[int, int, int], optional): Patch size for prediction. Defaults to (80, 160, 160).\n        patch_halo (tuple[int, int, int] | None, optional): Halo size around patches. Defaults to None.\n        single_batch_mode (bool, optional): Whether to use a single batch for prediction. Defaults to True.\n        device (str, optional): The computation device ('cpu', 'cuda', etc.). Defaults to 'cuda'.\n        model_update (bool, optional): Whether to update the model to the latest version. Defaults to False.\n        disable_tqdm (bool, optional): If True, disables the tqdm progress bar. Defaults to False.\n        config_path (Path | None, optional): Path to the model configuration file. Defaults to None.\n        model_weights_path (Path | None, optional): Path to the model weights file. Defaults to None.\n\n    Returns:\n        np.ndarray: The predicted boundaries as a 3D (Z, Y, X) or 4D (C, Z, Y, X) array, normalized between 0 and 1.\n\n    Raises:\n        ValueError: If neither `model_name`, `model_id`, nor `config_path` are provided.\n    \"\"\"\n\n    if config_path is not None:  # Safari mode for custom models outside zoos\n        logger.info(\"Safari prediction: Running model from custom config path.\")\n        model, model_config, model_path = model_zoo.get_model_by_config_path(\n            config_path, model_weights_path\n        )\n    elif model_id is not None:  # BioImage.IO zoo mode\n        logger.info(\"BioImage.IO prediction: Running model from BioImage.IO model zoo.\")\n        model, model_config, model_path = model_zoo.get_model_by_id(model_id)\n    elif model_name is not None:  # PlantSeg zoo mode\n        logger.info(\"Zoo prediction: Running model from PlantSeg official zoo.\")\n        model, model_config, model_path = model_zoo.get_model_by_name(\n            model_name, model_update=model_update\n        )\n    else:\n        raise ValueError(\n            \"Either `model_name` or `model_id` or `model_path` must be provided.\"\n        )\n    state = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n\n    if \"model_state_dict\" in state:  # Model weights format may vary between versions\n        state = state[\"model_state_dict\"]\n    model.load_state_dict(state)\n\n    if patch_halo is None:\n        try:\n            logger.info(\"Computing theoretical minimum halo from model.\")\n            patch_halo = model_zoo.compute_3D_halo_for_pytorch3dunet(model)\n        except Exception:\n            logger.warning(\n                \"Could not compute halo from model. Using 0 halo size, you may experience edge artifacts.\"\n            )\n            patch_halo = (0, 0, 0)\n\n    if patch is None:\n        maximum_patch_shape = find_a_max_patch_shape(\n            model, model_config[\"in_channels\"], device\n        )\n        raw_shape = raw.shape if input_layout == \"ZYX\" else (1,) + raw.shape\n        assert len(raw_shape) == 3\n        patch, patch_halo = find_patch_and_halo_shapes(\n            raw_shape, maximum_patch_shape, patch_halo, both_sides=False\n        )\n\n    logger.info(\n        f\"For raw in shape {raw.shape}: set patch shape {patch}, set halo shape {patch_halo}\"\n    )\n\n    predictor = ArrayPredictor(\n        model=model,\n        in_channels=model_config[\"in_channels\"],\n        out_channels=model_config[\"out_channels\"],\n        device=device,\n        patch=patch,\n        patch_halo=patch_halo,\n        single_batch_mode=single_batch_mode,\n        headless=False,\n        verbose_logging=False,\n        disable_tqdm=disable_tqdm,\n        tracker=tracker,\n    )\n\n    if int(model_config[\"in_channels\"]) &gt; 1:  # if multi-channel input\n        raw = fix_layout_to_CZYX(raw, input_layout)\n        multichannel_input = True\n    else:\n        raw = fix_layout_to_ZYX(raw, input_layout)\n        multichannel_input = False\n    raw = raw.astype(\"float32\")\n    augs = get_test_augmentations(\n        raw\n    )  # using full raw to compute global normalization mean and std\n    stride = get_stride_shape(patch)\n    slice_builder = SliceBuilder(\n        raw, label_dataset=None, patch_shape=patch, stride_shape=stride\n    )\n    test_dataset = ArrayDataset(\n        raw,\n        slice_builder,\n        augs,\n        halo_shape=patch_halo,\n        multichannel=multichannel_input,\n        verbose_logging=False,\n    )\n\n    pmaps = predictor(test_dataset)  # pmaps either (C, Z, Y, X) or (C, Y, X)\n    return pmaps\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/","title":"Data Processing","text":"<p>Basic data processing functions are provided in the <code>dataprocessing</code> module. These functions are used to preprocess data before training a model, or to post-process the output of a model.</p>"},{"location":"chapters/python_api/functionals/data_processing/#generic-functions","title":"Generic Functions","text":""},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.normalize_01","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.normalize_01(data: np.ndarray, eps=1e-12) -&gt; np.ndarray</code>","text":"<p>Normalize a numpy array between 0 and 1 and converts it to float32.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>Input numpy array</p> </li> <li> <code>eps</code>               (<code>float</code>, default:                   <code>1e-12</code> )           \u2013            <p>A small value added to the denominator for numerical stability</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>normalized_data</code> (              <code>ndarray</code> )          \u2013            <p>Normalized numpy array</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def normalize_01(data: np.ndarray, eps=1e-12) -&gt; np.ndarray:\n    \"\"\"\n    Normalize a numpy array between 0 and 1 and converts it to float32.\n\n    Args:\n        data (np.ndarray): Input numpy array\n        eps (float): A small value added to the denominator for numerical stability\n\n    Returns:\n        normalized_data (np.ndarray): Normalized numpy array\n    \"\"\"\n    return (data - np.min(data)) / (np.max(data) - np.min(data) + eps).astype(\"float32\")\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.scale_image_to_voxelsize","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.scale_image_to_voxelsize(image: np.ndarray, input_voxel_size: tuple[float, float, float], output_voxel_size: tuple[float, float, float], order: int = 0) -&gt; np.ndarray</code>","text":"<p>Scale an image from a given voxel size to another voxel size.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to scale</p> </li> <li> <code>input_voxel_size</code>               (<code>tuple[float, float, float]</code>)           \u2013            <p>Input voxel size</p> </li> <li> <code>output_voxel_size</code>               (<code>tuple[float, float, float]</code>)           \u2013            <p>Output voxel size</p> </li> <li> <code>order</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Interpolation order, must be 0 for segmentation and 1, 2 for images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>scaled_image</code> (              <code>ndarray</code> )          \u2013            <p>Scaled image as numpy array</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def scale_image_to_voxelsize(\n    image: np.ndarray,\n    input_voxel_size: tuple[float, float, float],\n    output_voxel_size: tuple[float, float, float],\n    order: int = 0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Scale an image from a given voxel size to another voxel size.\n\n    Args:\n        image (np.ndarray): Input image to scale\n        input_voxel_size (tuple[float, float, float]): Input voxel size\n        output_voxel_size (tuple[float, float, float]): Output voxel size\n        order (int): Interpolation order, must be 0 for segmentation and 1, 2 for images\n\n    Returns:\n        scaled_image (np.ndarray): Scaled image as numpy array\n    \"\"\"\n    factor = compute_scaling_factor(input_voxel_size, output_voxel_size)\n    return image_rescale(image, factor, order=order)\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.image_rescale","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.image_rescale(image: np.ndarray, factor: tuple[float, float, float], order: int) -&gt; np.ndarray</code>","text":"<p>Scale an image by a given factor in each dimension</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to scale</p> </li> <li> <code>factor</code>               (<code>tuple[float, float, float]</code>)           \u2013            <p>Scaling factor in each dimension</p> </li> <li> <code>order</code>               (<code>int</code>)           \u2013            <p>Interpolation order, must be 0 for segmentation and 1, 2 for images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>scaled_image</code> (              <code>ndarray</code> )          \u2013            <p>Scaled image as numpy array</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def image_rescale(\n    image: np.ndarray, factor: tuple[float, float, float], order: int\n) -&gt; np.ndarray:\n    \"\"\"\n    Scale an image by a given factor in each dimension\n\n    Args:\n        image (np.ndarray): Input image to scale\n        factor (tuple[float, float, float]): Scaling factor in each dimension\n        order (int): Interpolation order, must be 0 for segmentation and 1, 2 for images\n\n    Returns:\n        scaled_image (np.ndarray): Scaled image as numpy array\n    \"\"\"\n    if np.array_equal(factor, [1.0, 1.0, 1.0]):\n        return image\n    else:\n        return zoom(image, zoom=factor, order=order)\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.image_median","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.image_median(image: np.ndarray, radius: int) -&gt; np.ndarray</code>","text":"<p>Apply median smoothing on an image with a given radius.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to apply median smoothing.</p> </li> <li> <code>radius</code>               (<code>int</code>)           \u2013            <p>Radius of the median filter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Median smoothed image.</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def image_median(image: np.ndarray, radius: int) -&gt; np.ndarray:\n    \"\"\"\n    Apply median smoothing on an image with a given radius.\n\n    Args:\n        image (np.ndarray): Input image to apply median smoothing.\n        radius (int): Radius of the median filter.\n\n    Returns:\n        np.ndarray: Median smoothed image.\n    \"\"\"\n    if radius &lt;= 0:\n        raise ValueError(\"Radius must be a positive integer.\")\n\n    if image.ndim == 2:\n        # 2D image\n        return median(image, disk(radius))\n    elif image.ndim == 3:\n        if image.shape[0] == 1:\n            # Single slice (ZYX or YX) case\n            return median(image[0], disk(radius)).reshape(image.shape)\n        else:\n            # 3D image\n            return median(image, ball(radius))\n    else:\n        raise ValueError(\n            \"Unsupported image dimensionality. Image must be either 2D or 3D.\"\n        )\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.image_gaussian_smoothing","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.image_gaussian_smoothing(image: np.ndarray, sigma: float) -&gt; np.ndarray</code>","text":"<p>Apply gaussian smoothing on an image with a given sigma.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to apply gaussian smoothing</p> </li> <li> <code>sigma</code>               (<code>float</code>)           \u2013            <p>Sigma value for gaussian smoothing</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>smoothed_image</code> (              <code>ndarray</code> )          \u2013            <p>Gaussian smoothed image as numpy array</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def image_gaussian_smoothing(image: np.ndarray, sigma: float) -&gt; np.ndarray:\n    \"\"\"\n    Apply gaussian smoothing on an image with a given sigma.\n\n    Args:\n        image (np.ndarray): Input image to apply gaussian smoothing\n        sigma (float): Sigma value for gaussian smoothing\n\n    Returns:\n        smoothed_image (np.ndarray): Gaussian smoothed image as numpy array\n    \"\"\"\n    image = image.astype(\"float32\")\n    max_sigma = (np.array(image.shape) - 1) / 3\n    sigma_array = np.minimum(max_sigma, np.ones(max_sigma.ndim) * sigma)\n    return gaussianSmoothing(image, sigma_array)\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.image_crop","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.image_crop(image: np.ndarray, crop_str: str) -&gt; np.ndarray</code>","text":"<p>Crop an image from a crop string like [:, 10:30:, 10:20]</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ndarray</code>)           \u2013            <p>Input image to crop</p> </li> <li> <code>crop_str</code>               (<code>str</code>)           \u2013            <p>Crop string</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>cropped_image</code> (              <code>ndarray</code> )          \u2013            <p>Cropped image as numpy array</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def image_crop(image: np.ndarray, crop_str: str) -&gt; np.ndarray:\n    \"\"\"\n    Crop an image from a crop string like [:, 10:30:, 10:20]\n\n    Args:\n        image (np.ndarray): Input image to crop\n        crop_str (str): Crop string\n\n    Returns:\n        cropped_image (np.ndarray): Cropped image as numpy array\n    \"\"\"\n    crop_str = crop_str.replace(\"[\", \"\").replace(\"]\", \"\")\n    slices = tuple(\n        (\n            slice(*(int(i) if i else None for i in part.strip().split(\":\")))\n            if \":\" in part\n            else int(part.strip())\n        )\n        for part in crop_str.split(\",\")\n    )\n    return image[slices]\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.dataprocessing.process_images","title":"<code>plantseg.functionals.dataprocessing.dataprocessing.process_images(image1: np.ndarray, image2: np.ndarray, operation: ImagePairOperation, normalize_input: bool = False, clip_output: bool = False, normalize_output: bool = True) -&gt; np.ndarray</code>","text":"<p>General function for performing image operations with optional preprocessing and post-processing.</p> <p>Parameters:</p> <ul> <li> <code>image1</code>               (<code>ndarray</code>)           \u2013            <p>First input image.</p> </li> <li> <code>image2</code>               (<code>ndarray</code>)           \u2013            <p>Second input image.</p> </li> <li> <code>operation</code>               (<code>str</code>)           \u2013            <p>Operation to perform ('add', 'multiply', 'subtract', 'divide', 'max').</p> </li> <li> <code>normalize_input</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to normalize the input images to the range [0, 1]. Default is False.</p> </li> <li> <code>clip_output</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to clip the resulting image values to the range [0, 1]. Default is False.</p> </li> <li> <code>normalize_output</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to normalize the output image to the range [0, 1]. Default is True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: The resulting image after performing the operation.</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/dataprocessing.py</code> <pre><code>def process_images(\n    image1: np.ndarray,\n    image2: np.ndarray,\n    operation: ImagePairOperation,\n    normalize_input: bool = False,\n    clip_output: bool = False,\n    normalize_output: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"\n    General function for performing image operations with optional preprocessing and post-processing.\n\n    Args:\n        image1 (np.ndarray): First input image.\n        image2 (np.ndarray): Second input image.\n        operation (str): Operation to perform ('add', 'multiply', 'subtract', 'divide', 'max').\n        normalize_input (bool): Whether to normalize the input images to the range [0, 1]. Default is False.\n        clip_output (bool): Whether to clip the resulting image values to the range [0, 1]. Default is False.\n        normalize_output (bool): Whether to normalize the output image to the range [0, 1]. Default is True.\n\n    Returns:\n        np.ndarray: The resulting image after performing the operation.\n    \"\"\"\n    # Preprocessing: Normalize input images if specified\n    if normalize_input:\n        image1, image2 = normalize_01(image1), normalize_01(image2)\n\n    # Perform the specified operation\n    if operation == \"add\":\n        result = image1 + image2\n    elif operation == \"multiply\":\n        result = image1 * image2\n    elif operation == \"subtract\":\n        result = image1 - image2\n    elif operation == \"divide\":\n        result = image1 / image2\n    elif operation == \"max\":\n        result = np.maximum(image1, image2)\n    else:\n        raise ValueError(f\"Unsupported operation: {operation}\")\n\n    # Post-processing: Clip and/or normalize output if specified\n    if clip_output:\n        result = np.clip(result, 0, 1)\n    if normalize_output:\n        result = normalize_01(result)\n\n    return result\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#segmentation-functions","title":"Segmentation Functions","text":""},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.labelprocessing.relabel_segmentation","title":"<code>plantseg.functionals.dataprocessing.labelprocessing.relabel_segmentation(segmentation_image: np.ndarray, background: int | None = None) -&gt; np.ndarray</code>","text":"<p>Relabels contiguously a segmentation image, non-touching instances with same id will be relabeled differently. To be noted that measure.label is different from ndimage.label.</p> <p>1-connectivity     2-connectivity     diagonal connection close-up</p> <pre><code> [ ]           [ ]  [ ]  [ ]             [ ]\n  |               \\  |  /                 |  &lt;- hop 2\n</code></pre> <p>[ ]--[x]--[ ]      [ ]--[x]--[ ]        [x]--[ ]       |               /  |  \\             hop 1      [ ]           [ ]  [ ]  [ ]</p> <p>Parameters:</p> <ul> <li> <code>segmentation_image</code>               (<code>ndarray</code>)           \u2013            <p>A 2D or 3D segmentation image where connected components represent different instances.</p> </li> <li> <code>background</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Label of the background. If None, the function will assume the background                                label is 0. Default is None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: A relabeled segmentation image where each connected component is assigned a unique integer label.</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/labelprocessing.py</code> <pre><code>def relabel_segmentation(\n    segmentation_image: np.ndarray, background: int | None = None\n) -&gt; np.ndarray:\n    r\"\"\"\n    Relabels contiguously a segmentation image, non-touching instances with same id will be relabeled differently.\n    To be noted that measure.label is different from ndimage.label.\n\n    1-connectivity     2-connectivity     diagonal connection close-up\n\n         [ ]           [ ]  [ ]  [ ]             [ ]\n          |               \\  |  /                 |  &lt;- hop 2\n    [ ]--[x]--[ ]      [ ]--[x]--[ ]        [x]--[ ]\n          |               /  |  \\             hop 1\n         [ ]           [ ]  [ ]  [ ]\n\n    Args:\n        segmentation_image (np.ndarray): A 2D or 3D segmentation image where connected components represent different instances.\n        background (int | None, optional): Label of the background. If None, the function will assume the background\n                                           label is 0. Default is None.\n\n    Returns:\n        np.ndarray: A relabeled segmentation image where each connected component is assigned a unique integer label.\n    \"\"\"\n    relabeled_segmentation = measure.label(\n        segmentation_image,\n        background=background,\n        return_num=False,\n        connectivity=None,\n    )\n    assert isinstance(relabeled_segmentation, np.ndarray)\n    return relabeled_segmentation\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.labelprocessing.set_background_to_value","title":"<code>plantseg.functionals.dataprocessing.labelprocessing.set_background_to_value(segmentation_image: np.ndarray, value: int = 0) -&gt; np.ndarray</code>","text":"<p>Sets all occurrences of the background (label 0) in the segmentation image to a new value.</p> <p>Parameters:</p> <ul> <li> <code>segmentation_image</code>               (<code>ndarray</code>)           \u2013            <p>A 2D or 3D numpy array representing an instance segmentation.</p> </li> <li> <code>value</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The value to assign to the background. Default is 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: A segmentation image where all background pixels (originally 0) are set to <code>value</code>.</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/labelprocessing.py</code> <pre><code>def set_background_to_value(\n    segmentation_image: np.ndarray, value: int = 0\n) -&gt; np.ndarray:\n    \"\"\"\n    Sets all occurrences of the background (label 0) in the segmentation image to a new value.\n\n    Args:\n        segmentation_image (np.ndarray): A 2D or 3D numpy array representing an instance segmentation.\n        value (int, optional): The value to assign to the background. Default is 0.\n\n    Returns:\n        np.ndarray: A segmentation image where all background pixels (originally 0) are set to `value`.\n    \"\"\"\n    return np.where(segmentation_image == 0, value, segmentation_image)\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#advanced-functions","title":"Advanced Functions","text":""},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.advanced_dataprocessing.fix_over_under_segmentation_from_nuclei","title":"<code>plantseg.functionals.dataprocessing.advanced_dataprocessing.fix_over_under_segmentation_from_nuclei(cell_seg: np.ndarray, nuclei_seg: np.ndarray, threshold_merge: float, threshold_split: float, quantile_min: float, quantile_max: float, boundary: np.ndarray | None = None) -&gt; np.ndarray</code>","text":"<p>Correct over-segmentation and under-segmentation of cells based on nuclei information.</p> <p>This function uses information from nuclei segmentation to refine cell segmentation by first identifying over-segmented cells (cells mistakenly split into multiple segments) and merging them. It then corrects under-segmented cells (multiple nuclei within a single cell) by splitting them based on nuclei position and optional boundary information.</p> <p>Parameters:</p> <ul> <li> <code>cell_seg</code>               (<code>ndarray</code>)           \u2013            <p>A 2D or 3D array of segmented cells, where each integer represents a unique cell.</p> </li> <li> <code>nuclei_seg</code>               (<code>ndarray</code>)           \u2013            <p>A 2D or 3D array of segmented nuclei, matching the shape of <code>cell_seg</code>. Used to guide merging and splitting.</p> </li> <li> <code>threshold_merge</code>               (<code>float</code>)           \u2013            <p>A value between 0 and 1. Cells with less than this fraction of nuclei overlap are considered over-segmented and will be merged. Default is 0.33.</p> </li> <li> <code>threshold_split</code>               (<code>float</code>)           \u2013            <p>A value between 0 and 1. Cells with more than this fraction of nuclei overlap are considered under-segmented and will be split. Default is 0.66.</p> </li> <li> <code>quantile_min</code>               (<code>float</code>)           \u2013            <p>The lower size limit for nuclei, as a fraction (0-1). Nuclei smaller than this quantile are ignored. Default is 0.3.</p> </li> <li> <code>quantile_max</code>               (<code>float</code>)           \u2013            <p>The upper size limit for nuclei, as a fraction (0-1). Nuclei larger than this quantile are ignored. Default is 0.99.</p> </li> <li> <code>boundary</code>               (<code>ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional boundary map of the same shape as <code>cell_seg</code>. High values indicate cell boundaries and help refine splitting. If None, all regions are treated equally.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Corrected cell segmentation array.</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/advanced_dataprocessing.py</code> <pre><code>def fix_over_under_segmentation_from_nuclei(\n    cell_seg: np.ndarray,\n    nuclei_seg: np.ndarray,\n    threshold_merge: float,\n    threshold_split: float,\n    quantile_min: float,\n    quantile_max: float,\n    boundary: np.ndarray | None = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Correct over-segmentation and under-segmentation of cells based on nuclei information.\n\n    This function uses information from nuclei segmentation to refine cell segmentation by first identifying\n    over-segmented cells (cells mistakenly split into multiple segments) and merging them. It then corrects\n    under-segmented cells (multiple nuclei within a single cell) by splitting them based on nuclei position\n    and optional boundary information.\n\n    Args:\n        cell_seg (np.ndarray): A 2D or 3D array of segmented cells, where each integer represents a unique cell.\n        nuclei_seg (np.ndarray): A 2D or 3D array of segmented nuclei, matching the shape of `cell_seg`.\n            Used to guide merging and splitting.\n        threshold_merge (float, optional): A value between 0 and 1. Cells with less than this fraction of nuclei overlap\n            are considered over-segmented and will be merged. Default is 0.33.\n        threshold_split (float, optional): A value between 0 and 1. Cells with more than this fraction of nuclei overlap\n            are considered under-segmented and will be split. Default is 0.66.\n        quantile_min (float, optional): The lower size limit for nuclei, as a fraction (0-1). Nuclei smaller than this\n            quantile are ignored. Default is 0.3.\n        quantile_max (float, optional): The upper size limit for nuclei, as a fraction (0-1). Nuclei larger than this\n            quantile are ignored. Default is 0.99.\n        boundary (np.ndarray | None, optional): Optional boundary map of the same shape as `cell_seg`. High values\n            indicate cell boundaries and help refine splitting. If None, all regions are treated equally.\n\n    Returns:\n        np.ndarray: Corrected cell segmentation array.\n    \"\"\"\n    # Find overlaps between cells and nuclei\n    cell_counts, nuclei_counts, cell_nuclei_counts = numba_find_overlaps(\n        cell_seg, nuclei_seg\n    )\n\n    # Identify over-segmentation and correct it\n    nuclei_assignments = find_potential_over_seg(\n        nuclei_counts, cell_nuclei_counts, threshold=threshold_merge\n    )\n    corrected_seg = fix_over_segmentation(cell_seg, nuclei_assignments)\n\n    # Identify under-segmentation and correct it\n    cell_counts, nuclei_counts, cell_nuclei_counts = numba_find_overlaps(\n        corrected_seg, nuclei_seg\n    )\n    cell_assignments = find_potential_under_seg(\n        nuclei_counts,\n        cell_counts,\n        cell_nuclei_counts,\n        threshold=threshold_split,\n        quantiles_clip=(quantile_min, quantile_max),\n    )\n\n    boundary_pmap = np.ones_like(cell_seg) if boundary is None else boundary\n    return fix_under_segmentation(\n        corrected_seg, nuclei_seg, boundary_pmap, cell_assignments, cell_idx=None\n    )\n</code></pre>"},{"location":"chapters/python_api/functionals/data_processing/#plantseg.functionals.dataprocessing.advanced_dataprocessing.remove_false_positives_by_foreground_probability","title":"<code>plantseg.functionals.dataprocessing.advanced_dataprocessing.remove_false_positives_by_foreground_probability(segmentation: np.ndarray, foreground: np.ndarray, threshold: float) -&gt; np.ndarray</code>","text":"<p>Removes false positive regions in a segmentation based on a foreground probability map.</p> <ol> <li>Labels are not preserved.</li> <li>If the mean(an instance * its own probability region) &lt; threshold, it is removed.</li> </ol> <p>Parameters:</p> <ul> <li> <code>segmentation</code>               (<code>ndarray</code>)           \u2013            <p>Segmentation array where each unique non-zero value indicates a distinct region.</p> </li> <li> <code>foreground</code>               (<code>ndarray</code>)           \u2013            <p>Foreground probability map of the same shape as <code>segmentation</code>.</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>Probability threshold below which regions are considered false positives.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Segmentation array with false positives removed.</p> </li> </ul> Source code in <code>plantseg/functionals/dataprocessing/advanced_dataprocessing.py</code> <pre><code>def remove_false_positives_by_foreground_probability(\n    segmentation: np.ndarray,\n    foreground: np.ndarray,\n    threshold: float,\n) -&gt; np.ndarray:\n    \"\"\"\n    Removes false positive regions in a segmentation based on a foreground probability map.\n\n    1. Labels are not preserved.\n    2. If the mean(an instance * its own probability region) &lt; threshold, it is removed.\n\n    Args:\n        segmentation (np.ndarray): Segmentation array where each unique non-zero value indicates a distinct region.\n        foreground (np.ndarray): Foreground probability map of the same shape as `segmentation`.\n        threshold (float): Probability threshold below which regions are considered false positives.\n\n    Returns:\n        np.ndarray: Segmentation array with false positives removed.\n    \"\"\"\n    # TODO: make a channel for removed regions for easier inspection\n    # TODO: use `relabel_sequential` to recover the original labels\n\n    if segmentation.shape != foreground.shape:\n        raise ValueError(\"Segmentation and probability map must have the same shape.\")\n    if foreground.max() &gt; 1:\n        raise ValueError(\"Foreground must be a probability map with values in [0, 1].\")\n\n    instances, _, _ = relabel_sequential(\n        segmentation\n    )  # The label 0 is assumed to denote the bg and is never remapped.\n    regions = regionprops(instances)  # Labels with value 0 are ignored.\n\n    pixel_count = np.zeros(len(regions) + 1)\n    pixel_value = np.zeros(len(regions) + 1)\n    pixel_count[0] = (\n        1  # Avoid division by zero: pixel_count[0] and pixel_value[0] are fixed throughout.\n    )\n\n    for region in tqdm.tqdm(regions):\n        bbox = region.bbox\n        if instances.ndim == 3:\n            slices = (\n                slice(bbox[0], bbox[3]),\n                slice(bbox[1], bbox[4]),\n                slice(bbox[2], bbox[5]),\n            )\n        else:\n            slices = (slice(bbox[0], bbox[2]), slice(bbox[1], bbox[3]))\n\n        region_mask = instances[slices] == region.label\n        prob = foreground[slices]\n\n        pixel_count[region.label] = region.area\n        pixel_value[region.label] = (region_mask * prob).sum()\n\n    likelihood = pixel_value / pixel_count\n    to_remove = likelihood &lt; threshold\n\n    instances[np.isin(instances, np.nonzero(to_remove)[0])] = 0\n    instances, _, _ = relabel_sequential(instances)\n    return instances\n</code></pre>"},{"location":"chapters/python_api/functionals/io/","title":"Input/Output","text":"<p>All the input/output operations are handled by the <code>plantseg.io</code> module. This module provides functions to read and write data in different formats. The supported formats are <code>tiff</code>, <code>h5</code>, and <code>zarr</code>, <code>jpeg</code>, <code>png</code>.</p>"},{"location":"chapters/python_api/functionals/io/#reading","title":"Reading","text":""},{"location":"chapters/python_api/functionals/io/#plantseg.io.smart_load","title":"<code>plantseg.io.smart_load(path: Path, key: str | None = None, default=load_tiff) -&gt; np.ndarray</code>","text":"<p>Load a dataset from a file. The loader is chosen based on the file extension. Supported formats are: tiff, h5, zarr, and PIL images. If the format is not supported, a default loader can be provided (default: load_tiff).</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>path to the file to load.</p> </li> <li> <code>key</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>key of the dataset to load (if h5 or zarr).</p> </li> <li> <code>default</code>               (<code>callable</code>, default:                   <code>load_tiff</code> )           \u2013            <p>default loader if the type is not understood.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>stack</code> (              <code>ndarray</code> )          \u2013            <p>numpy array with the image data.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = smart_load('path/to/file.tif')\n&gt;&gt;&gt; data = smart_load('path/to/file.h5', key='raw')\n</code></pre> Source code in <code>plantseg/io/io.py</code> <pre><code>def smart_load(path: Path, key: str | None = None, default=load_tiff) -&gt; np.ndarray:\n    \"\"\"\n    Load a dataset from a file. The loader is chosen based on the file extension.\n    Supported formats are: tiff, h5, zarr, and PIL images.\n    If the format is not supported, a default loader can be provided (default: load_tiff).\n\n    Args:\n        path (Path): path to the file to load.\n        key (str): key of the dataset to load (if h5 or zarr).\n        default (callable): default loader if the type is not understood.\n\n    Returns:\n        stack (np.ndarray): numpy array with the image data.\n\n    Examples:\n        &gt;&gt;&gt; data = smart_load('path/to/file.tif')\n        &gt;&gt;&gt; data = smart_load('path/to/file.h5', key='raw')\n\n    \"\"\"\n    ext = (path.suffix).lower()\n    if key == \"\":\n        key = None\n\n    if ext in H5_EXTENSIONS:\n        return load_h5(path, key)\n\n    elif ext in TIFF_EXTENSIONS:\n        return load_tiff(path)\n\n    elif ext in PIL_EXTENSIONS:\n        return load_pil(path)\n\n    elif ext in ZARR_EXTENSIONS:\n        return load_zarr(path, key)\n\n    else:\n        logger.warning(f\"No default found for {ext}, reverting to default loader.\")\n        return default(path)\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#writing","title":"Writing","text":""},{"location":"chapters/python_api/functionals/io/#plantseg.io.create_tiff","title":"<code>plantseg.io.create_tiff(path: Path, stack: np.ndarray, voxel_size: VoxelSize, layout: str = 'ZYX') -&gt; None</code>","text":"<p>Create a tiff file from a numpy array</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>path to save the tiff file</p> </li> <li> <code>stack</code>               (<code>ndarray</code>)           \u2013            <p>numpy array to save as tiff</p> </li> <li> <code>voxel_size</code>               (<code>list or tuple</code>)           \u2013            <p>tuple of the voxel size</p> </li> <li> <code>voxel_size_unit</code>               (<code>str</code>)           \u2013            <p>units of the voxel size</p> </li> </ul> Source code in <code>plantseg/io/tiff.py</code> <pre><code>def create_tiff(\n    path: Path, stack: np.ndarray, voxel_size: VoxelSize, layout: str = \"ZYX\"\n) -&gt; None:\n    \"\"\"\n    Create a tiff file from a numpy array\n\n    Args:\n        path (Path): path to save the tiff file\n        stack (np.ndarray): numpy array to save as tiff\n        voxel_size (list or tuple): tuple of the voxel size\n        voxel_size_unit (str): units of the voxel size\n\n    \"\"\"\n    # taken from: https://pypi.org/project/tifffile docs\n    # dimensions in TZCYXS order\n    if layout == \"ZYX\":\n        assert stack.ndim == 3, \"Stack dimensions must be in ZYX order\"\n        z, y, x = stack.shape\n        stack = stack.reshape(1, z, 1, y, x, 1)\n\n    elif layout == \"YX\":\n        assert stack.ndim == 2, \"Stack dimensions must be in YX order\"\n        y, x = stack.shape\n        stack = stack.reshape(1, 1, 1, y, x, 1)\n\n    elif layout == \"CYX\":\n        assert stack.ndim == 3, \"Stack dimensions must be in CYX order\"\n        c, y, x = stack.shape\n        stack = stack.reshape(1, 1, c, y, x, 1)\n\n    elif layout == \"ZCYX\":\n        assert stack.ndim == 4, \"Stack dimensions must be in ZCYX order\"\n        z, c, y, x = stack.shape\n        stack = stack.reshape(1, z, c, y, x, 1)\n\n    elif layout == \"CZYX\":\n        assert stack.ndim == 4, \"Stack dimensions must be in CZYX order\"\n        c, z, y, x = stack.shape\n        stack = stack.reshape(1, z, c, y, x, 1)\n\n    else:\n        raise ValueError(f\"Layout {layout} not supported\")\n\n    if voxel_size.voxels_size is not None:\n        assert len(voxel_size.voxels_size) == 3, (\n            \"Voxel size must have 3 elements (z, y, x)\"\n        )\n        spacing, y, x = voxel_size.voxels_size\n    else:\n        spacing, y, x = (1.0, 1.0, 1.0)\n\n    resolution = (1.0 / x, 1.0 / y)\n    # Save output results as tiff\n    tifffile.imwrite(\n        path,\n        data=stack,\n        dtype=stack.dtype,\n        imagej=True,\n        resolution=resolution,\n        metadata={\"axes\": \"TZCYXS\", \"spacing\": spacing, \"unit\": voxel_size.unit},\n        compression=\"zlib\",\n    )\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#plantseg.io.create_h5","title":"<code>plantseg.io.create_h5(path: Path, stack: np.ndarray, key: str, voxel_size: Optional[VoxelSize] = None, mode: str = 'a') -&gt; None</code>","text":"<p>Create a dataset inside a h5 file from a numpy array.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>path to the h5 file</p> </li> <li> <code>stack</code>               (<code>ndarray</code>)           \u2013            <p>numpy array to save as dataset in the h5 file.</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>key of the dataset in the h5 file.</p> </li> <li> <code>voxel_size</code>               (<code>VoxelSize</code>, default:                   <code>None</code> )           \u2013            <p>voxel size of the dataset.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'a'</code> )           \u2013            <p>mode to open the h5 file ['w', 'a'].</p> </li> </ul> Source code in <code>plantseg/io/h5.py</code> <pre><code>def create_h5(\n    path: Path,\n    stack: np.ndarray,\n    key: str,\n    voxel_size: Optional[VoxelSize] = None,\n    mode: str = \"a\",\n) -&gt; None:\n    \"\"\"\n    Create a dataset inside a h5 file from a numpy array.\n\n    Args:\n        path (Path): path to the h5 file\n        stack (np.ndarray): numpy array to save as dataset in the h5 file.\n        key (str): key of the dataset in the h5 file.\n        voxel_size (VoxelSize): voxel size of the dataset.\n        mode (str): mode to open the h5 file ['w', 'a'].\n\n    \"\"\"\n\n    if key is None:\n        raise ValueError(\"Key is required to create a dataset in a h5 file.\")\n\n    if key == \"\":\n        raise ValueError(\"Key cannot be empty to create a dataset in a h5 file.\")\n\n    with h5py.File(path, mode) as f:\n        if key in f:\n            del f[key]\n        f.create_dataset(key, data=stack, compression=\"gzip\")\n        # save voxel_size\n        if voxel_size is not None and voxel_size.voxels_size is not None:\n            f[key].attrs[\"element_size_um\"] = voxel_size.voxels_size\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#plantseg.io.create_zarr","title":"<code>plantseg.io.create_zarr(path: Path, stack: np.ndarray, key: str, voxel_size: VoxelSize, mode: str = 'a') -&gt; None</code>","text":"<p>Create a Zarr array from a NumPy array.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the Zarr file.</p> </li> <li> <code>stack</code>               (<code>ndarray</code>)           \u2013            <p>The NumPy array to save as a dataset.</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>The internal key of the desired dataset.</p> </li> <li> <code>voxel_size</code>               (<code>VoxelSize</code>)           \u2013            <p>The voxel size of the dataset.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'a'</code> )           \u2013            <p>The mode to open the Zarr file ['w', 'a'].</p> </li> </ul> Source code in <code>plantseg/io/zarr.py</code> <pre><code>def create_zarr(\n    path: Path,\n    stack: np.ndarray,\n    key: str,\n    voxel_size: VoxelSize,\n    mode: str = \"a\",\n) -&gt; None:\n    \"\"\"\n    Create a Zarr array from a NumPy array.\n\n    Args:\n        path (Path): The path to the Zarr file.\n        stack (np.ndarray): The NumPy array to save as a dataset.\n        key (str): The internal key of the desired dataset.\n        voxel_size (VoxelSize): The voxel size of the dataset.\n        mode (str): The mode to open the Zarr file ['w', 'a'].\n\n    \"\"\"\n\n    if key is None:\n        raise ValueError(\"Key cannot be None.\")\n\n    if key == \"\":\n        raise ValueError(\"Key cannot be empty.\")\n\n    zarr_file = zarr.open_group(path, mode)\n    zarr_file.create_dataset(key, data=stack, compression=\"gzip\", overwrite=True)\n    zarr_file[key].attrs[\"element_size_um\"] = voxel_size.voxels_size\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#tiff-utilities","title":"Tiff Utilities","text":""},{"location":"chapters/python_api/functionals/io/#plantseg.io.tiff.read_tiff_voxel_size","title":"<code>plantseg.io.tiff.read_tiff_voxel_size(file_path: Path) -&gt; VoxelSize</code>","text":"<p>Returns the voxels size and the voxel units for imagej and ome style tiff (if absent returns [1, 1, 1], um)</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>Path</code>)           \u2013            <p>path to the tiff file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>VoxelSize</code> (              <code>VoxelSize</code> )          \u2013            <p>voxel size and unit</p> </li> </ul> Source code in <code>plantseg/io/tiff.py</code> <pre><code>def read_tiff_voxel_size(file_path: Path) -&gt; VoxelSize:\n    \"\"\"\n    Returns the voxels size and the voxel units for imagej and ome style tiff (if absent returns [1, 1, 1], um)\n\n    Args:\n        file_path (Path): path to the tiff file\n\n    Returns:\n        VoxelSize: voxel size and unit\n\n    \"\"\"\n    with tifffile.TiffFile(file_path) as tiff:\n        if tiff.imagej_metadata is not None:\n            return _read_imagej_meta(tiff)\n\n        elif tiff.ome_metadata is not None:\n            return _read_ome_meta(tiff)\n\n        warnings.warn(\"No metadata found.\")\n        return VoxelSize()\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#h5-utilities","title":"H5 Utilities","text":""},{"location":"chapters/python_api/functionals/io/#plantseg.io.h5.list_h5_keys","title":"<code>plantseg.io.h5.list_h5_keys(path: Path) -&gt; list[str]</code>","text":"<p>List all keys in a h5 file</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>path to the h5 file (Path object)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>keys</code> (              <code>list[str]</code> )          \u2013            <p>A list of keys in the h5 file.</p> </li> </ul> Source code in <code>plantseg/io/h5.py</code> <pre><code>def list_h5_keys(path: Path) -&gt; list[str]:\n    \"\"\"\n    List all keys in a h5 file\n\n    Args:\n        path (Path): path to the h5 file (Path object)\n\n    Returns:\n        keys (list[str]): A list of keys in the h5 file.\n\n    \"\"\"\n    _validate_h5_file(path)\n\n    def _recursive_find_keys(f, base=\"/\"):\n        _list_keys = []\n        for key, dataset in f.items():\n            if isinstance(dataset, h5py.Group):\n                new_base = f\"{base}{key}/\"\n                _list_keys += _recursive_find_keys(dataset, new_base)\n\n            elif isinstance(dataset, h5py.Dataset):\n                _list_keys.append(f\"{base}{key}\")\n        return _list_keys\n\n    with h5py.File(path, \"r\") as h5_f:\n        return _recursive_find_keys(h5_f)\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#plantseg.io.h5.del_h5_key","title":"<code>plantseg.io.h5.del_h5_key(path: Path, key: str, mode: str = 'a') -&gt; None</code>","text":"<p>helper function to delete a dataset from a h5file</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>path to the h5 file (Path object)</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>key of the dataset to delete</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'a'</code> )           \u2013            <p>mode to open the h5 file ['r', 'r+']</p> </li> </ul> Source code in <code>plantseg/io/h5.py</code> <pre><code>def del_h5_key(path: Path, key: str, mode: str = \"a\") -&gt; None:\n    \"\"\"\n    helper function to delete a dataset from a h5file\n\n    Args:\n        path (Path): path to the h5 file (Path object)\n        key (str): key of the dataset to delete\n        mode (str): mode to open the h5 file ['r', 'r+']\n\n    \"\"\"\n    _validate_h5_file(path)\n    with h5py.File(path, mode) as f:\n        if key in f:\n            del f[key]\n            f.close()\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#plantseg.io.h5.rename_h5_key","title":"<code>plantseg.io.h5.rename_h5_key(path: Path, old_key: str, new_key: str, mode='r+') -&gt; None</code>","text":"<p>Rename the 'old_key' dataset to 'new_key'</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>path to the h5 file (Path object)</p> </li> <li> <code>old_key</code>               (<code>str</code>)           \u2013            <p>old key name</p> </li> <li> <code>new_key</code>               (<code>str</code>)           \u2013            <p>new key name</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'r+'</code> )           \u2013            <p>mode to open the h5 file ['r', 'r+']</p> </li> </ul> Source code in <code>plantseg/io/h5.py</code> <pre><code>def rename_h5_key(path: Path, old_key: str, new_key: str, mode=\"r+\") -&gt; None:\n    \"\"\"\n    Rename the 'old_key' dataset to 'new_key'\n\n    Args:\n        path (Path): path to the h5 file (Path object)\n        old_key (str): old key name\n        new_key (str): new key name\n        mode (str): mode to open the h5 file ['r', 'r+']\n\n    \"\"\"\n    _validate_h5_file(path)\n    with h5py.File(path, mode) as f:\n        if old_key in f:\n            f[new_key] = f[old_key]\n            del f[old_key]\n            f.close()\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#zarr-utilities","title":"Zarr Utilities","text":""},{"location":"chapters/python_api/functionals/io/#plantseg.io.zarr.list_zarr_keys","title":"<code>plantseg.io.zarr.list_zarr_keys(path: Path) -&gt; list[str]</code>","text":"<p>List all keys in a Zarr file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the Zarr file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of keys in the Zarr file.</p> </li> </ul> Source code in <code>plantseg/io/zarr.py</code> <pre><code>def list_zarr_keys(path: Path) -&gt; list[str]:\n    \"\"\"\n    List all keys in a Zarr file.\n\n    Args:\n        path (Path): The path to the Zarr file.\n\n    Returns:\n        list[str]: A list of keys in the Zarr file.\n    \"\"\"\n\n    def _recursive_find_keys(\n        zarr_group: zarr.Group, base: Path = Path(\"\")\n    ) -&gt; list[str]:\n        _list_keys = []\n        for key, dataset in zarr_group.items():\n            if isinstance(dataset, zarr.Group):\n                new_base = base / key\n                _list_keys.extend(_recursive_find_keys(dataset, new_base))\n            elif isinstance(dataset, zarr.Array):\n                _list_keys.append(str(base / key))\n        return _list_keys\n\n    zarr_file = zarr.open_group(path, \"r\")\n    return _recursive_find_keys(zarr_file)\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#plantseg.io.zarr.del_zarr_key","title":"<code>plantseg.io.zarr.del_zarr_key(path: Path, key: str, mode: str = 'a') -&gt; None</code>","text":"<p>Delete a dataset from a Zarr file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the Zarr file.</p> </li> <li> <code>key</code>               (<code>str</code>)           \u2013            <p>The internal key of the dataset to be deleted.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'a'</code> )           \u2013            <p>The mode to open the Zarr file ['w', 'a'].</p> </li> </ul> Source code in <code>plantseg/io/zarr.py</code> <pre><code>def del_zarr_key(path: Path, key: str, mode: str = \"a\") -&gt; None:\n    \"\"\"\n    Delete a dataset from a Zarr file.\n\n    Args:\n        path (Path): The path to the Zarr file.\n        key (str): The internal key of the dataset to be deleted.\n        mode (str): The mode to open the Zarr file ['w', 'a'].\n\n    \"\"\"\n    zarr_file = zarr.open_group(path, mode)\n    if key in zarr_file:\n        del zarr_file[key]\n</code></pre>"},{"location":"chapters/python_api/functionals/io/#plantseg.io.zarr.rename_zarr_key","title":"<code>plantseg.io.zarr.rename_zarr_key(path: Path, old_key: str, new_key: str, mode: str = 'r+') -&gt; None</code>","text":"<p>Rename a dataset in a Zarr file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path</code>)           \u2013            <p>The path to the Zarr file.</p> </li> <li> <code>old_key</code>               (<code>str</code>)           \u2013            <p>The current key of the dataset.</p> </li> <li> <code>new_key</code>               (<code>str</code>)           \u2013            <p>The new key for the dataset.</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'r+'</code> )           \u2013            <p>The mode to open the Zarr file ['r+'].</p> </li> </ul> Source code in <code>plantseg/io/zarr.py</code> <pre><code>def rename_zarr_key(path: Path, old_key: str, new_key: str, mode: str = \"r+\") -&gt; None:\n    \"\"\"\n    Rename a dataset in a Zarr file.\n\n    Args:\n        path (Path): The path to the Zarr file.\n        old_key (str): The current key of the dataset.\n        new_key (str): The new key for the dataset.\n        mode (str): The mode to open the Zarr file ['r+'].\n\n    \"\"\"\n    zarr_file = zarr.open_group(path, mode)\n    if old_key in zarr_file:\n        zarr_file[new_key] = zarr_file[old_key]\n        del zarr_file[old_key]\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/","title":"PlantSeg Segmentation","text":"<p>The PlantSeg segmentation module implements all segmentation routine in plantseg.</p>"},{"location":"chapters/python_api/functionals/segmentation/#dt-watershed","title":"DT Watershed","text":""},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.dt_watershed","title":"<code>plantseg.functionals.segmentation.dt_watershed(boundary_pmaps: np.ndarray, threshold: float = 0.5, sigma_seeds: float = 1.0, stacked: bool = False, sigma_weights: float = 2.0, min_size: int = 100, alpha: float = 1.0, pixel_pitch: Optional[tuple[int, ...]] = None, apply_nonmax_suppression: bool = False, n_threads: Optional[int] = None, mask: Optional[np.ndarray] = None) -&gt; np.ndarray</code>","text":"<p>Performs watershed segmentation using distance transforms on boundary probability maps.</p> <p>This function applies the distance transform watershed algorithm to the input boundary probability maps, either slice-by-slice or in original shape depending on the 'stacked' parameter. The watershed method is applied to the boundary probability maps with optional pre-processing like thresholding, smoothing, and masking.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>Input array of boundary probability maps, often obtained from deep learning models. Each pixel/voxel value represents the probability of being part of a boundary.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold value applied to the boundary probability map before computing the distance transform. Values below this threshold are considered background. Defaults to 0.5.</p> </li> <li> <code>sigma_seeds</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Standard deviation for Gaussian smoothing applied to the seed map (used for initializing the watershed regions). Higher values result in more smoothed seeds. Defaults to 1.0.</p> </li> <li> <code>stacked</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, performs watershed segmentation on each 2D slice of a 3D volume independently (slice-by-slice). If False, performs watershed segmentation in 3D for volumetric data or 2D for 2D input. Defaults to False.</p> </li> <li> <code>sigma_weights</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Standard deviation for Gaussian smoothing applied to the weight map. The weight map combines the distance transform and input map to guide the watershed. Larger values result in smoother weight maps. Defaults to 2.0.</p> </li> <li> <code>min_size</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Minimum size of the segmented regions to retain. Regions smaller than this size are removed. Defaults to 100.</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Blending factor to combine the input boundary probability maps and the distance transform when constructing the weight map. A higher alpha prioritizes the input maps. Defaults to 1.0.</p> </li> <li> <code>pixel_pitch</code>               (<code>Optional[tuple[int, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Voxel anisotropy factors (e.g., spacing along different axes) to use during the distance transform. If None, the distances are computed isotropically. For anisotropic volumes, this should match the voxel spacing. Defaults to None.</p> </li> <li> <code>apply_nonmax_suppression</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, applies non-maximum suppression to the detected seeds, reducing seed redundancy. This requires the Nifty library. Defaults to False.</p> </li> <li> <code>n_threads</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of threads to use for parallel processing in 2D mode (stacked mode). If None, the default number of threads will be used. Defaults to None.</p> </li> <li> <code>mask</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>A binary mask that excludes certain regions from segmentation. Only regions within the mask will be considered. If None, all regions are included. Must have the same shape as 'boundary_pmaps'. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: A labeled segmentation map where each region is assigned a unique label.</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def dt_watershed(\n    boundary_pmaps: np.ndarray,\n    threshold: float = 0.5,\n    sigma_seeds: float = 1.0,\n    stacked: bool = False,\n    sigma_weights: float = 2.0,\n    min_size: int = 100,\n    alpha: float = 1.0,\n    pixel_pitch: Optional[tuple[int, ...]] = None,\n    apply_nonmax_suppression: bool = False,\n    n_threads: Optional[int] = None,\n    mask: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"Performs watershed segmentation using distance transforms on boundary probability maps.\n\n    This function applies the distance transform watershed algorithm to the input boundary\n    probability maps, either slice-by-slice or in original shape depending on the 'stacked' parameter.\n    The watershed method is applied to the boundary probability maps with optional pre-processing\n    like thresholding, smoothing, and masking.\n\n    Args:\n        boundary_pmaps (np.ndarray): Input array of boundary probability maps, often obtained\n            from deep learning models. Each pixel/voxel value represents the probability of\n            being part of a boundary.\n        threshold (float, optional): Threshold value applied to the boundary probability map\n            before computing the distance transform. Values below this threshold are considered\n            background. Defaults to 0.5.\n        sigma_seeds (float, optional): Standard deviation for Gaussian smoothing applied to\n            the seed map (used for initializing the watershed regions). Higher values result\n            in more smoothed seeds. Defaults to 1.0.\n        stacked (bool, optional): If True, performs watershed segmentation on each 2D slice of\n            a 3D volume independently (slice-by-slice). If False, performs watershed segmentation\n            in 3D for volumetric data or 2D for 2D input. Defaults to False.\n        sigma_weights (float, optional): Standard deviation for Gaussian smoothing applied\n            to the weight map. The weight map combines the distance transform and input map\n            to guide the watershed. Larger values result in smoother weight maps. Defaults to 2.0.\n        min_size (int, optional): Minimum size of the segmented regions to retain. Regions\n            smaller than this size are removed. Defaults to 100.\n        alpha (float, optional): Blending factor to combine the input boundary probability maps\n            and the distance transform when constructing the weight map. A higher alpha\n            prioritizes the input maps. Defaults to 1.0.\n        pixel_pitch (Optional[tuple[int, ...]], optional): Voxel anisotropy factors (e.g., spacing\n            along different axes) to use during the distance transform. If None, the distances are\n            computed isotropically. For anisotropic volumes, this should match the voxel spacing.\n            Defaults to None.\n        apply_nonmax_suppression (bool, optional): If True, applies non-maximum suppression to\n            the detected seeds, reducing seed redundancy. This requires the Nifty library.\n            Defaults to False.\n        n_threads (Optional[int], optional): Number of threads to use for parallel processing in\n            2D mode (stacked mode). If None, the default number of threads will be used.\n            Defaults to None.\n        mask (Optional[np.ndarray], optional): A binary mask that excludes certain regions from\n            segmentation. Only regions within the mask will be considered. If None, all regions\n            are included. Must have the same shape as 'boundary_pmaps'. Defaults to None.\n\n    Returns:\n        np.ndarray: A labeled segmentation map where each region is assigned a unique label.\n\n    \"\"\"\n    # Prepare the keyword arguments for the watershed function\n    boundary_pmaps = boundary_pmaps.astype(\"float32\")\n    ws_kwargs = {\n        \"threshold\": threshold,\n        \"sigma_seeds\": sigma_seeds,\n        \"sigma_weights\": sigma_weights,\n        \"min_size\": min_size,\n        \"alpha\": alpha,\n        \"pixel_pitch\": pixel_pitch,\n        \"apply_nonmax_suppression\": apply_nonmax_suppression,\n        \"mask\": mask,\n    }\n    if stacked:\n        # Apply watershed slice by slice (for 3D data)\n        segmentation, _ = stacked_watershed(\n            boundary_pmaps,\n            ws_function=distance_transform_watershed,\n            n_threads=n_threads,\n            **ws_kwargs,\n        )\n    else:\n        # Apply watershed in 3D for 3D data or in 2D for 2D data\n        segmentation, _ = distance_transform_watershed(boundary_pmaps, **ws_kwargs)\n\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/#gasp","title":"GASP","text":""},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.gasp","title":"<code>plantseg.functionals.segmentation.gasp(boundary_pmaps: np.ndarray, superpixels: Optional[np.ndarray] = None, gasp_linkage_criteria: str = 'average', beta: float = 0.5, post_minsize: int = 100, n_threads: int = 6) -&gt; np.ndarray</code>","text":"<p>Perform segmentation using the GASP algorithm with affinity maps.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>Cell boundary prediction.</p> </li> <li> <code>superpixels</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>Superpixel segmentation. If None, GASP will be run from the pixels. Default is None.</p> </li> <li> <code>gasp_linkage_criteria</code>               (<code>str</code>, default:                   <code>'average'</code> )           \u2013            <p>Linkage criteria for GASP. Default is 'average'.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Beta parameter for GASP. Small values steer towards under-segmentation, while high values bias towards over-segmentation. Default is 0.5.</p> </li> <li> <code>post_minsize</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Minimum size of the segments after GASP. Default is 100.</p> </li> <li> <code>n_threads</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Number of threads used for GASP. Default is 6.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: GASP output segmentation.</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def gasp(\n    boundary_pmaps: np.ndarray,\n    superpixels: Optional[np.ndarray] = None,\n    gasp_linkage_criteria: str = \"average\",\n    beta: float = 0.5,\n    post_minsize: int = 100,\n    n_threads: int = 6,\n) -&gt; np.ndarray:\n    \"\"\"\n    Perform segmentation using the GASP algorithm with affinity maps.\n\n    Args:\n        boundary_pmaps (np.ndarray): Cell boundary prediction.\n        superpixels (Optional[np.ndarray]): Superpixel segmentation. If None, GASP will be run from the pixels. Default is None.\n        gasp_linkage_criteria (str): Linkage criteria for GASP. Default is 'average'.\n        beta (float): Beta parameter for GASP. Small values steer towards under-segmentation, while high values bias towards over-segmentation. Default is 0.5.\n        post_minsize (int): Minimum size of the segments after GASP. Default is 100.\n        n_threads (int): Number of threads used for GASP. Default is 6.\n\n    Returns:\n        np.ndarray: GASP output segmentation.\n    \"\"\"\n    remove_singleton = False\n    if superpixels is not None:\n        assert boundary_pmaps.shape == superpixels.shape, (\n            \"Shape mismatch between boundary_pmaps and superpixels.\"\n        )\n        if superpixels.ndim == 2:  # Ensure superpixels is 3D if provided\n            superpixels = superpixels[None, ...]\n            boundary_pmaps = boundary_pmaps[None, ...]\n            remove_singleton = True\n\n    # Prepare the arguments for running GASP\n    run_GASP_kwargs = {\n        \"linkage_criteria\": gasp_linkage_criteria,\n        \"add_cannot_link_constraints\": False,\n        \"use_efficient_implementations\": False,\n    }\n\n    # Interpret boundary_pmaps as affinities and prepare for GASP\n    boundary_pmaps = boundary_pmaps.astype(\"float32\")\n    affinities = np.stack([boundary_pmaps] * 3, axis=0)\n\n    offsets = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n    # Shift is required to correct aligned affinities\n    affinities = shift_affinities(affinities, offsets=offsets)\n\n    # invert affinities\n    affinities = 1 - affinities\n\n    # Initialize and run GASP\n    gasp_instance = GaspFromAffinities(\n        offsets,\n        superpixel_generator=None\n        if superpixels is None\n        else (lambda *args, **kwargs: superpixels),\n        run_GASP_kwargs=run_GASP_kwargs,\n        n_threads=n_threads,\n        beta_bias=beta,\n    )\n    segmentation, _ = gasp_instance(affinities)\n\n    # Apply size filtering if specified\n    if post_minsize &gt; 0:\n        segmentation, _ = apply_size_filter(\n            segmentation.astype(\"uint32\"), boundary_pmaps, post_minsize\n        )\n\n    if remove_singleton:\n        segmentation = segmentation[0]\n\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/#multicut","title":"Multicut","text":""},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.multicut","title":"<code>plantseg.functionals.segmentation.multicut(boundary_pmaps: np.ndarray, superpixels: np.ndarray, beta: float = 0.5, post_minsize: int = 50) -&gt; np.ndarray</code>","text":"<p>Multicut segmentation from boundary prediction.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>cell boundary prediction, 3D array of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>superpixels</code>               (<code>ndarray</code>)           \u2013            <p>superpixel segmentation. Must have the same shape as boundary_pmaps.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter for the Multicut. A small value will steer the segmentation towards under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)</p> </li> <li> <code>post_minsize</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>minimal size of the segments after Multicut. (default: 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>Multicut output segmentation</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def multicut(\n    boundary_pmaps: np.ndarray,\n    superpixels: np.ndarray,\n    beta: float = 0.5,\n    post_minsize: int = 50,\n) -&gt; np.ndarray:\n    \"\"\"\n    Multicut segmentation from boundary prediction.\n\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary prediction, 3D array of shape (Z, Y, X) with values between 0 and 1.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n        beta (float): beta parameter for the Multicut. A small value will steer the segmentation towards\n            under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after Multicut. (default: 100)\n\n    Returns:\n        segmentation (np.ndarray): Multicut output segmentation\n    \"\"\"\n\n    rag = compute_rag(superpixels)\n\n    # Prob -&gt; edge costs\n    boundary_pmaps = boundary_pmaps.astype(\"float32\")\n    costs = compute_mc_costs(boundary_pmaps, rag, beta=beta)\n\n    # Creating graph\n    graph = nifty.graph.undirectedGraph(rag.numberOfNodes)\n    graph.insertEdges(rag.uvIds())\n\n    # Solving Multicut\n    node_labels = multicut_kernighan_lin(graph, costs)\n    segmentation = nifty.tools.take(node_labels, superpixels)\n\n    # run size threshold\n    if post_minsize &gt; 0:\n        segmentation, _ = apply_size_filter(\n            segmentation.astype(\"uint32\"), boundary_pmaps, post_minsize\n        )\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/#mutex-watershed","title":"Mutex Watershed","text":""},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.mutex_ws","title":"<code>plantseg.functionals.segmentation.mutex_ws(boundary_pmaps: np.ndarray, superpixels: Optional[np.ndarray] = None, beta: float = 0.5, post_minsize: int = 100, n_threads: int = 6) -&gt; np.ndarray</code>","text":"<p>Wrapper around gasp with mutex_watershed as linkage criteria.</p> <p>Args:magicgui     boundary_pmaps (np.ndarray): cell boundary prediction. 3D array of shape (Z, Y, X) with values between 0 and 1.     superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.         If None, GASP will be run from the pixels. (default: None)     beta (float): beta parameter for GASP. A small value will steer the segmentation towards under-segmentation.         While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)     post_minsize (int): minimal size of the segments after GASP. (default: 100)     n_threads (int): number of threads used for GASP. (default: 6)</p> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>MutexWS output segmentation</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def mutex_ws(\n    boundary_pmaps: np.ndarray,\n    superpixels: Optional[np.ndarray] = None,\n    beta: float = 0.5,\n    post_minsize: int = 100,\n    n_threads: int = 6,\n) -&gt; np.ndarray:\n    \"\"\"\n    Wrapper around gasp with mutex_watershed as linkage criteria.\n\n    Args:magicgui\n        boundary_pmaps (np.ndarray): cell boundary prediction. 3D array of shape (Z, Y, X) with values between 0 and 1.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n            If None, GASP will be run from the pixels. (default: None)\n        beta (float): beta parameter for GASP. A small value will steer the segmentation towards under-segmentation.\n            While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after GASP. (default: 100)\n        n_threads (int): number of threads used for GASP. (default: 6)\n\n    Returns:\n        segmentation (np.ndarray): MutexWS output segmentation\n\n    \"\"\"\n    return gasp(\n        boundary_pmaps=boundary_pmaps,\n        superpixels=superpixels,\n        gasp_linkage_criteria=\"mutex_watershed\",\n        beta=beta,\n        post_minsize=post_minsize,\n        n_threads=n_threads,\n    )\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/#lifted-multicut","title":"Lifted Multicut","text":""},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.segmentation.lifted_multicut_from_nuclei_pmaps","title":"<code>plantseg.functionals.segmentation.segmentation.lifted_multicut_from_nuclei_pmaps(boundary_pmaps: np.ndarray, nuclei_pmaps: np.ndarray, superpixels: np.ndarray, beta: float = 0.5, post_minsize: int = 50) -&gt; np.ndarray</code>","text":"<p>Lifted Multicut segmentation from boundary prediction and nuclei prediction.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>cell boundary prediction, 3D array of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>nuclei_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>nuclei prediction. Must have the same shape as boundary_pmaps and with values between 0 and 1.</p> </li> <li> <code>superpixels</code>               (<code>ndarray</code>)           \u2013            <p>superpixel segmentation. Must have the same shape as boundary_pmaps.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter for the Multicut. A small value will steer the segmentation towards under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)</p> </li> <li> <code>post_minsize</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>minimal size of the segments after Multicut. (default: 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>Multicut output segmentation</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def lifted_multicut_from_nuclei_pmaps(\n    boundary_pmaps: np.ndarray,\n    nuclei_pmaps: np.ndarray,\n    superpixels: np.ndarray,\n    beta: float = 0.5,\n    post_minsize: int = 50,\n) -&gt; np.ndarray:\n    \"\"\"\n    Lifted Multicut segmentation from boundary prediction and nuclei prediction.\n\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary prediction, 3D array of shape (Z, Y, X) with values between 0 and 1.\n        nuclei_pmaps (np.ndarray): nuclei prediction. Must have the same shape as boundary_pmaps and\n            with values between 0 and 1.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n        beta (float): beta parameter for the Multicut. A small value will steer the segmentation towards\n            under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after Multicut. (default: 100)\n\n    Returns:\n        segmentation (np.ndarray): Multicut output segmentation\n    \"\"\"\n    if nuclei_pmaps.max() &gt; 1 or nuclei_pmaps.min() &lt; 0:\n        raise ValueError(\"nuclei_pmaps should be between 0 and 1\")\n\n    # compute the region adjacency graph\n    rag = compute_rag(superpixels)\n\n    # compute multi cut edges costs\n    boundary_pmaps = boundary_pmaps.astype(\"float32\")\n    costs = compute_mc_costs(boundary_pmaps, rag, beta)\n\n    # assert nuclei pmaps are floats\n    nuclei_pmaps = nuclei_pmaps.astype(\"float32\")\n    input_maps = [nuclei_pmaps]\n    assignment_threshold = 0.9\n\n    # compute lifted multicut features from boundary pmaps\n    lifted_uvs, lifted_costs = lifted_problem_from_probabilities(\n        rag,\n        superpixels.astype(\"uint32\"),\n        input_maps,\n        assignment_threshold,\n        graph_depth=4,\n    )\n\n    # solve the full lifted problem using the kernighan lin approximation introduced in\n    # http://openaccess.thecvf.com/content_iccv_2015/html/Keuper_Efficient_Decomposition_of_ICCV_2015_paper.html\n    node_labels = lmc.lifted_multicut_kernighan_lin(\n        rag, costs, lifted_uvs, lifted_costs\n    )\n    segmentation = project_node_labels_to_pixels(rag, node_labels)\n\n    # run size threshold\n    if post_minsize &gt; 0:\n        segmentation, _ = apply_size_filter(\n            segmentation.astype(\"uint32\"), boundary_pmaps, post_minsize\n        )\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.lifted_multicut_from_nuclei_segmentation","title":"<code>plantseg.functionals.segmentation.lifted_multicut_from_nuclei_segmentation(boundary_pmaps: np.ndarray, nuclei_seg: np.ndarray, superpixels: np.ndarray, beta: float = 0.5, post_minsize: int = 50) -&gt; np.ndarray</code>","text":"<p>Lifted Multicut segmentation from boundary prediction and nuclei segmentation.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>cell boundary prediction, 3D array of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>nuclei_seg</code>               (<code>ndarray</code>)           \u2013            <p>Nuclei segmentation. Must have the same shape as boundary_pmaps.</p> </li> <li> <code>superpixels</code>               (<code>ndarray</code>)           \u2013            <p>superpixel segmentation. Must have the same shape as boundary_pmaps.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter for the Multicut. A small value will steer the segmentation towards under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)</p> </li> <li> <code>post_minsize</code>               (<code>int</code>, default:                   <code>50</code> )           \u2013            <p>minimal size of the segments after Multicut. (default: 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>Multicut output segmentation</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def lifted_multicut_from_nuclei_segmentation(\n    boundary_pmaps: np.ndarray,\n    nuclei_seg: np.ndarray,\n    superpixels: np.ndarray,\n    beta: float = 0.5,\n    post_minsize: int = 50,\n) -&gt; np.ndarray:\n    \"\"\"\n    Lifted Multicut segmentation from boundary prediction and nuclei segmentation.\n\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary prediction, 3D array of shape (Z, Y, X) with values between 0 and 1.\n        nuclei_seg (np.ndarray): Nuclei segmentation. Must have the same shape as boundary_pmaps.\n        superpixels (np.ndarray): superpixel segmentation. Must have the same shape as boundary_pmaps.\n        beta (float): beta parameter for the Multicut. A small value will steer the segmentation towards\n            under-segmentation. While a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_minsize (int): minimal size of the segments after Multicut. (default: 100)\n\n    Returns:\n        segmentation (np.ndarray): Multicut output segmentation\n    \"\"\"\n    # compute the region adjacency graph\n    rag = compute_rag(superpixels)\n\n    # compute multi cut edges costs\n    boundary_pmaps = boundary_pmaps.astype(\"float32\")\n    costs = compute_mc_costs(boundary_pmaps, rag, beta)\n    max_cost = np.abs(np.max(costs))\n    lifted_uvs, lifted_costs = lifted_problem_from_segmentation(\n        rag,\n        superpixels,\n        nuclei_seg,\n        overlap_threshold=0.2,\n        graph_depth=4,\n        same_segment_cost=5 * max_cost,\n        different_segment_cost=-5 * max_cost,\n    )\n\n    # solve the full lifted problem using the kernighan lin approximation introduced in\n    # http://openaccess.thecvf.com/content_iccv_2015/html/Keuper_Efficient_Decomposition_of_ICCV_2015_paper.html\n    lifted_costs = lifted_costs.astype(\"float64\")\n    node_labels = lmc.lifted_multicut_kernighan_lin(\n        rag, costs, lifted_uvs, lifted_costs\n    )\n    segmentation = project_node_labels_to_pixels(rag, node_labels)\n\n    # run size threshold\n    if post_minsize &gt; 0:\n        segmentation, _ = apply_size_filter(\n            segmentation.astype(\"uint32\"), boundary_pmaps, post_minsize\n        )\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/functionals/segmentation/#simple-itk-watershed","title":"Simple ITK Watershed","text":""},{"location":"chapters/python_api/functionals/segmentation/#plantseg.functionals.segmentation.simple_itk_watershed","title":"<code>plantseg.functionals.segmentation.simple_itk_watershed(boundary_pmaps: np.ndarray, threshold: float = 0.5, sigma: float = 1.0, minsize: int = 100) -&gt; np.ndarray</code>","text":"<p>Simple itk watershed segmentation.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmaps</code>               (<code>ndarray</code>)           \u2013            <p>cell boundary prediction. 3D array of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>threshold for the watershed segmentation. (default: 0.5)</p> </li> <li> <code>sigma</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>sigma for the gaussian smoothing. (default: 1.0)</p> </li> <li> <code>minsize</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>minimal size of the segments after segmentation. (default: 100)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>segmentation</code> (              <code>ndarray</code> )          \u2013            <p>watershed output segmentation (using SimpleITK)</p> </li> </ul> Source code in <code>plantseg/functionals/segmentation/segmentation.py</code> <pre><code>def simple_itk_watershed(\n    boundary_pmaps: np.ndarray,\n    threshold: float = 0.5,\n    sigma: float = 1.0,\n    minsize: int = 100,\n) -&gt; np.ndarray:\n    \"\"\"\n    Simple itk watershed segmentation.\n\n    Args:\n        boundary_pmaps (np.ndarray): cell boundary prediction. 3D array of shape (Z, Y, X) with values between 0 and 1.\n        threshold (float): threshold for the watershed segmentation. (default: 0.5)\n        sigma (float): sigma for the gaussian smoothing. (default: 1.0)\n        minsize (int): minimal size of the segments after segmentation. (default: 100)\n\n    Returns:\n        segmentation (np.ndarray): watershed output segmentation (using SimpleITK)\n\n    \"\"\"\n    if not SIMPLE_ITK_INSTALLED:\n        raise ValueError(\"please install sitk before running this process\")\n\n    if sigma &gt; 0:\n        # fix ws sigma length\n        # ws sigma cannot be shorter than pmaps dims\n        max_sigma = (np.array(boundary_pmaps.shape) - 1) / 3\n        ws_sigma = np.minimum(max_sigma, np.ones(max_sigma.ndim) * sigma)\n        boundary_pmaps = gaussianSmoothing(boundary_pmaps, ws_sigma)\n\n    # Itk watershed + size filtering\n    itk_pmaps = sitk.GetImageFromArray(boundary_pmaps)\n    itk_segmentation = sitk.MorphologicalWatershed(\n        itk_pmaps, threshold, markWatershedLine=False, fullyConnected=False\n    )\n    itk_segmentation = sitk.RelabelComponent(itk_segmentation, minsize)\n    segmentation = sitk.GetArrayFromImage(itk_segmentation).astype(np.uint16)\n    return segmentation\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/","title":"Data Processing Tasks","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#image-preprocessing-tasks","title":"Image Preprocessing Tasks","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#gaussian-smoothing-task","title":"Gaussian smoothing task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.gaussian_smoothing_task","title":"<code>plantseg.tasks.dataprocessing_tasks.gaussian_smoothing_task(image: PlantSegImage, sigma: float) -&gt; PlantSegImage</code>","text":"<p>Apply Gaussian smoothing to a PlantSegImage object.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image</p> </li> <li> <code>sigma</code>               (<code>float</code>)           \u2013            <p>standard deviation of the Gaussian kernel</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef gaussian_smoothing_task(image: PlantSegImage, sigma: float) -&gt; PlantSegImage:\n    \"\"\"\n    Apply Gaussian smoothing to a PlantSegImage object.\n\n    Args:\n        image (PlantSegImage): input image\n        sigma (float): standard deviation of the Gaussian kernel\n\n    \"\"\"\n    if image.is_multichannel:\n        raise ValueError(\"Gaussian smoothing is not supported for multichannel images.\")\n\n    data = image.get_data()\n    smoothed_data = image_gaussian_smoothing(data, sigma=sigma)\n    new_image = image.derive_new(smoothed_data, name=f\"{image.name}_smoothed\")\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#image-cropping-task","title":"Image cropping task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.image_cropping_task","title":"<code>plantseg.tasks.dataprocessing_tasks.image_cropping_task(image: PlantSegImage, rectangle=None, crop_z: tuple[int, int] = (0, 100)) -&gt; PlantSegImage</code>","text":"<p>Crop the image based on the given rectangle and z-slices.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>The image to be cropped.</p> </li> <li> <code>rectangle</code>               (<code>Optional</code>, default:                   <code>None</code> )           \u2013            <p>Rectangle defining the region to crop.</p> </li> <li> <code>crop_z</code>               (<code>tuple[int, int]</code>, default:                   <code>(0, 100)</code> )           \u2013            <p>Z-slice range for cropping.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PlantSegImage</code> (              <code>PlantSegImage</code> )          \u2013            <p>The cropped image.</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef image_cropping_task(\n    image: PlantSegImage, rectangle=None, crop_z: tuple[int, int] = (0, 100)\n) -&gt; PlantSegImage:\n    \"\"\"\n    Crop the image based on the given rectangle and z-slices.\n\n    Args:\n        image (PlantSegImage): The image to be cropped.\n        rectangle (Optional): Rectangle defining the region to crop.\n        crop_z (tuple[int, int]): Z-slice range for cropping.\n\n    Returns:\n        PlantSegImage: The cropped image.\n    \"\"\"\n    data = image.get_data()\n\n    # Compute crop slices\n    if image.dimensionality == ImageDimensionality.TWO:\n        crop_slices = _compute_slices_2d(rectangle, data.shape)\n    else:\n        crop_slices = _compute_slices_3d(rectangle, crop_z, data.shape)\n\n    # Perform cropping on the data\n    cropped_data = _cropping(data, crop_slices)\n\n    # Create and return a new PlantSegImage object from the cropped data\n    cropped_image = image.derive_new(cropped_data, name=f\"{image.name}_cropped\")\n\n    return cropped_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#image-rescale-to-shape-task","title":"Image rescale to shape task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.image_rescale_to_shape_task","title":"<code>plantseg.tasks.dataprocessing_tasks.image_rescale_to_shape_task(image: PlantSegImage, new_shape: tuple[int, ...], order: int = 0) -&gt; PlantSegImage</code>","text":"<p>Rescale an image to a new shape.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image</p> </li> <li> <code>new_shape</code>               (<code>tuple[int, ...]</code>)           \u2013            <p>new shape of the image</p> </li> <li> <code>order</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>order of the interpolation</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef image_rescale_to_shape_task(\n    image: PlantSegImage, new_shape: tuple[int, ...], order: int = 0\n) -&gt; PlantSegImage:\n    \"\"\"Rescale an image to a new shape.\n\n    Args:\n        image (PlantSegImage): input image\n        new_shape (tuple[int, ...]): new shape of the image\n        order (int): order of the interpolation\n    \"\"\"\n    if image.image_layout == ImageLayout.YX:\n        scaling_factor = (new_shape[1] / image.shape[0], new_shape[2] / image.shape[1])\n        spatial_scaling_factor = (1.0, scaling_factor[0], scaling_factor[1])\n    elif image.image_layout == ImageLayout.ZYX:\n        scaling_factor = (\n            new_shape[0] / image.shape[0],\n            new_shape[1] / image.shape[1],\n            new_shape[2] / image.shape[2],\n        )\n        spatial_scaling_factor = scaling_factor\n    elif image.image_layout == ImageLayout.CYX:\n        scaling_factor = (\n            1.0,\n            new_shape[1] / image.shape[1],\n            new_shape[2] / image.shape[2],\n        )\n        spatial_scaling_factor = (1.0, scaling_factor[1], scaling_factor[2])\n    elif image.image_layout == ImageLayout.CZYX:\n        scaling_factor = (\n            1.0,\n            new_shape[0] / image.shape[1],\n            new_shape[1] / image.shape[2],\n            new_shape[2] / image.shape[3],\n        )\n        spatial_scaling_factor = scaling_factor[1:]\n    elif image.image_layout == ImageLayout.ZCYX:\n        scaling_factor = (\n            new_shape[0] / image.shape[0],\n            1.0,\n            new_shape[1] / image.shape[2],\n            new_shape[2] / image.shape[3],\n        )\n        spatial_scaling_factor = (\n            scaling_factor[0],\n            scaling_factor[2],\n            scaling_factor[3],\n        )\n\n    out_data = image_rescale(image.get_data(), scaling_factor, order=order)\n\n    if image.has_valid_voxel_size():\n        out_voxel_size = image.voxel_size.voxelsize_from_factor(spatial_scaling_factor)\n    else:\n        out_voxel_size = VoxelSize()\n\n    new_image = image.derive_new(\n        out_data, name=f\"{image.name}_reshaped\", voxel_size=out_voxel_size\n    )\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#image-rescale-to-voxel-size-task","title":"Image rescale to voxel size task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.image_rescale_to_voxel_size_task","title":"<code>plantseg.tasks.dataprocessing_tasks.image_rescale_to_voxel_size_task(image: PlantSegImage, new_voxel_size: VoxelSize, order: int = 0) -&gt; PlantSegImage</code>","text":"<p>Rescale an image to a new voxel size.</p> <p>If the voxel size is not defined in the input image, use the set voxel size task to set the voxel size.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image</p> </li> <li> <code>new_voxel_size</code>               (<code>VoxelSize</code>)           \u2013            <p>new voxel size</p> </li> <li> <code>order</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>order of the interpolation</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef image_rescale_to_voxel_size_task(\n    image: PlantSegImage, new_voxel_size: VoxelSize, order: int = 0\n) -&gt; PlantSegImage:\n    \"\"\"Rescale an image to a new voxel size.\n\n    If the voxel size is not defined in the input image, use the set voxel size task to set the voxel size.\n\n    Args:\n        image (PlantSegImage): input image\n        new_voxel_size (VoxelSize): new voxel size\n        order (int): order of the interpolation\n\n    \"\"\"\n    spatial_scaling_factor = image.voxel_size.scalefactor_from_voxelsize(new_voxel_size)\n\n    if image.image_layout == ImageLayout.YX:\n        scaling_factor = (spatial_scaling_factor[1], spatial_scaling_factor[2])\n    elif image.image_layout == ImageLayout.CYX:\n        scaling_factor = (1.0, spatial_scaling_factor[1], spatial_scaling_factor[2])\n    elif image.image_layout == ImageLayout.ZYX:\n        scaling_factor = spatial_scaling_factor\n    elif image.image_layout == ImageLayout.CZYX:\n        scaling_factor = (1.0, *spatial_scaling_factor)\n    elif image.image_layout == ImageLayout.ZCYX:\n        scaling_factor = (spatial_scaling_factor[0], 1.0, *spatial_scaling_factor[1:])\n\n    out_data = image_rescale(image.get_data(), scaling_factor, order=order)\n    new_image = image.derive_new(\n        out_data, name=f\"{image.name}_rescaled\", voxel_size=new_voxel_size\n    )\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#set-image-voxel-size-task","title":"Set image voxel size task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.set_voxel_size_task","title":"<code>plantseg.tasks.dataprocessing_tasks.set_voxel_size_task(image: PlantSegImage, voxel_size: tuple[float, float, float]) -&gt; PlantSegImage</code>","text":"<p>Set the voxel size of an image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image</p> </li> <li> <code>voxel_size</code>               (<code>tuple[float, float, float]</code>)           \u2013            <p>new voxel size</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef set_voxel_size_task(\n    image: PlantSegImage, voxel_size: tuple[float, float, float]\n) -&gt; PlantSegImage:\n    \"\"\"Set the voxel size of an image.\n\n    Args:\n        image (PlantSegImage): input image\n        voxel_size (tuple[float, float, float]): new voxel size\n\n    \"\"\"\n    new_voxel_size = VoxelSize(voxels_size=voxel_size)\n    new_image = image.derive_new(\n        image._data,\n        name=f\"{image.name}_set_voxel_size\",\n        voxel_size=new_voxel_size,\n        original_voxel_size=new_voxel_size,\n    )\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#image-pair-operation-task","title":"Image pair operation task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.image_pair_operation_task","title":"<code>plantseg.tasks.dataprocessing_tasks.image_pair_operation_task(image1: PlantSegImage, image2: PlantSegImage, operation: ImagePairOperation, normalize_input: bool = False, clip_output: bool = False, normalize_output: bool = False) -&gt; PlantSegImage</code>","text":"<p>Task to perform an operation on two images.</p> <p>Parameters:</p> <ul> <li> <code>image1</code>               (<code>PlantSegImage</code>)           \u2013            <p>First image to process.</p> </li> <li> <code>Image2</code>               (<code>PlantSegImage</code>)           \u2013            <p>Second image to process.</p> </li> <li> <code>operation</code>               (<code>str</code>)           \u2013            <p>Operation to perform on the images.</p> </li> <li> <code>normalize_input</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Normalize input images before processing.</p> </li> <li> <code>clip_output</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Clip output values to the range [0, 1].</p> </li> <li> <code>normalize_output</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Normalize output values to the range [0, 1].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PlantSegImage</code> (              <code>PlantSegImage</code> )          \u2013            <p>New image resulting from the operation.</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef image_pair_operation_task(\n    image1: PlantSegImage,\n    image2: PlantSegImage,\n    operation: ImagePairOperation,\n    normalize_input: bool = False,\n    clip_output: bool = False,\n    normalize_output: bool = False,\n) -&gt; PlantSegImage:\n    \"\"\"\n    Task to perform an operation on two images.\n\n    Args:\n        image1 (PlantSegImage): First image to process.\n        Image2 (PlantSegImage): Second image to process.\n        operation (str): Operation to perform on the images.\n        normalize_input (bool): Normalize input images before processing.\n        clip_output (bool): Clip output values to the range [0, 1].\n        normalize_output (bool): Normalize output values to the range [0, 1].\n\n    Returns:\n        PlantSegImage: New image resulting from the operation.\n    \"\"\"\n    result = process_images(\n        image1.get_data(),\n        image2.get_data(),\n        operation=operation,\n        normalize_input=normalize_input,\n        clip_output=clip_output,\n        normalize_output=normalize_output,\n    )\n    new_image = image1.derive_new(\n        result, name=f\"{image1.name}_{operation}_{image2.name}\"\n    )\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#label-postprocessing-tasks","title":"Label Postprocessing Tasks","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#remove-false-positives-task","title":"Remove false positives task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.remove_false_positives_by_foreground_probability_task","title":"<code>plantseg.tasks.dataprocessing_tasks.remove_false_positives_by_foreground_probability_task(segmentation: PlantSegImage, foreground: PlantSegImage, threshold: float) -&gt; PlantSegImage</code>","text":"<p>Remove false positives from a segmentation based on the foreground probability.</p> <p>Parameters:</p> <ul> <li> <code>segmentation</code>               (<code>PlantSegImage</code>)           \u2013            <p>input segmentation</p> </li> <li> <code>foreground</code>               (<code>PlantSegImage</code>)           \u2013            <p>input foreground probability</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>threshold value</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef remove_false_positives_by_foreground_probability_task(\n    segmentation: PlantSegImage, foreground: PlantSegImage, threshold: float\n) -&gt; PlantSegImage:\n    \"\"\"Remove false positives from a segmentation based on the foreground probability.\n\n    Args:\n        segmentation (PlantSegImage): input segmentation\n        foreground (PlantSegImage): input foreground probability\n        threshold (float): threshold value\n\n    \"\"\"\n    if segmentation.shape != foreground.shape:\n        raise ValueError(\n            \"Segmentation and foreground probability must have the same shape.\"\n        )\n\n    out_data = remove_false_positives_by_foreground_probability(\n        segmentation.get_data(), foreground.get_data(), threshold\n    )\n    new_image = segmentation.derive_new(\n        out_data, name=f\"{segmentation.name}_fg_filtered\"\n    )\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#fix-overunder-segmentation-task","title":"Fix Over/Under segmentation task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.fix_over_under_segmentation_from_nuclei_task","title":"<code>plantseg.tasks.dataprocessing_tasks.fix_over_under_segmentation_from_nuclei_task(cell_seg: PlantSegImage, nuclei_seg: PlantSegImage, threshold_merge: float, threshold_split: float, quantile_min: float, quantile_max: float, boundary: PlantSegImage | None = None) -&gt; PlantSegImage</code>","text":"<p>Task to fix over- and under-segmentation of cells based on nuclear segmentation.</p> <p>Parameters:</p> <ul> <li> <code>cell_seg</code>               (<code>PlantSegImage</code>)           \u2013            <p>Input cell segmentation as a PlantSegImage object.</p> </li> <li> <code>nuclei_seg</code>               (<code>PlantSegImage</code>)           \u2013            <p>Input nuclear segmentation as a PlantSegImage object.</p> </li> <li> <code>threshold_merge</code>               (<code>float</code>)           \u2013            <p>Threshold for merging cells, as a fraction (0-1).</p> </li> <li> <code>threshold_split</code>               (<code>float</code>)           \u2013            <p>Threshold for splitting cells, as a fraction (0-1).</p> </li> <li> <code>quantile_min</code>               (<code>float</code>)           \u2013            <p>Minimum quantile for filtering nuclei sizes, as a fraction (0-1).</p> </li> <li> <code>quantile_max</code>               (<code>float</code>)           \u2013            <p>Maximum quantile for filtering nuclei sizes, as a fraction (0-1).</p> </li> <li> <code>boundary</code>               (<code>PlantSegImage | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional boundary probability map for segmentation refinement.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PlantSegImage</code> (              <code>PlantSegImage</code> )          \u2013            <p>Corrected cell segmentation as a PlantSegImage object.</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef fix_over_under_segmentation_from_nuclei_task(\n    cell_seg: PlantSegImage,\n    nuclei_seg: PlantSegImage,\n    threshold_merge: float,\n    threshold_split: float,\n    quantile_min: float,\n    quantile_max: float,\n    boundary: PlantSegImage | None = None,\n) -&gt; PlantSegImage:\n    \"\"\"\n    Task to fix over- and under-segmentation of cells based on nuclear segmentation.\n\n    Args:\n        cell_seg (PlantSegImage): Input cell segmentation as a PlantSegImage object.\n        nuclei_seg (PlantSegImage): Input nuclear segmentation as a PlantSegImage object.\n        threshold_merge (float): Threshold for merging cells, as a fraction (0-1).\n        threshold_split (float): Threshold for splitting cells, as a fraction (0-1).\n        quantile_min (float): Minimum quantile for filtering nuclei sizes, as a fraction (0-1).\n        quantile_max (float): Maximum quantile for filtering nuclei sizes, as a fraction (0-1).\n        boundary (PlantSegImage | None, optional): Optional boundary probability map for segmentation refinement.\n\n    Returns:\n        PlantSegImage: Corrected cell segmentation as a PlantSegImage object.\n    \"\"\"\n    corrected_data = fix_over_under_segmentation_from_nuclei(\n        cell_seg.get_data(),\n        nuclei_seg.get_data(),\n        threshold_merge=threshold_merge,\n        threshold_split=threshold_split,\n        quantile_min=quantile_min,\n        quantile_max=quantile_max,\n        boundary=boundary.get_data() if boundary else None,\n    )\n    return cell_seg.derive_new(corrected_data, name=f\"{cell_seg.name}_nuc_fixed\")\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#set-biggest-object-as-background-task","title":"Set biggest object as background task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.set_biggest_instance_to_zero_task","title":"<code>plantseg.tasks.dataprocessing_tasks.set_biggest_instance_to_zero_task(image: PlantSegImage, instance_could_be_zero: bool = False) -&gt; PlantSegImage</code>","text":"<p>Task to set the largest segment in a segmentation image to zero.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>Segmentation image to process.</p> </li> <li> <code>instance_could_be_zero</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, 0 might be an instance label, add 1 to all labels before processing.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PlantSegImage</code> (              <code>PlantSegImage</code> )          \u2013            <p>New segmentation image with largest instance set to 0.</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef set_biggest_instance_to_zero_task(\n    image: PlantSegImage, instance_could_be_zero: bool = False\n) -&gt; PlantSegImage:\n    \"\"\"\n    Task to set the largest segment in a segmentation image to zero.\n\n    Args:\n        image (PlantSegImage): Segmentation image to process.\n        instance_could_be_zero (bool): If True, 0 might be an instance label, add 1 to all labels before processing.\n\n    Returns:\n        PlantSegImage: New segmentation image with largest instance set to 0.\n    \"\"\"\n    if not (\n        image.semantic_type == SemanticType.SEGMENTATION\n        or image.semantic_type == SemanticType.LABEL\n    ):\n        raise ValueError(\"Input image must be a segmentation or mask image.\")\n    data = image.get_data()\n    logger.info(\n        f\"Processing {image.name} with shape {data.shape} and max {data.max()}, min {data.min()}.\"\n    )\n    new_data = set_biggest_instance_to_zero(\n        data, instance_could_be_zero=instance_could_be_zero\n    )\n    new_image = image.derive_new(new_data, name=f\"{image.name}_bg0\")\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#relabel-task","title":"Relabel task","text":""},{"location":"chapters/python_api/tasks/dataprocessing_tasks/#plantseg.tasks.dataprocessing_tasks.relabel_segmentation_task","title":"<code>plantseg.tasks.dataprocessing_tasks.relabel_segmentation_task(image: PlantSegImage, background: int | None = None) -&gt; PlantSegImage</code>","text":"<p>Task to relabel a segmentation image contiguously, ensuring non-touching segments with the same ID are relabeled.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>Segmentation image to process.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PlantSegImage</code> (              <code>PlantSegImage</code> )          \u2013            <p>New segmentation image with relabeled instances.</p> </li> </ul> Source code in <code>plantseg/tasks/dataprocessing_tasks.py</code> <pre><code>@task_tracker\ndef relabel_segmentation_task(\n    image: PlantSegImage, background: int | None = None\n) -&gt; PlantSegImage:\n    \"\"\"\n    Task to relabel a segmentation image contiguously, ensuring non-touching segments with the same ID are relabeled.\n\n    Args:\n        image (PlantSegImage): Segmentation image to process.\n\n    Returns:\n        PlantSegImage: New segmentation image with relabeled instances.\n    \"\"\"\n    if not (\n        image.semantic_type == SemanticType.SEGMENTATION\n        or image.semantic_type == SemanticType.LABEL\n    ):\n        raise ValueError(\"Input image must be a segmentation or mask image.\")\n    data = image.get_data()\n    new_data = relabel_segmentation(data, background=background)\n    new_image = image.derive_new(new_data, name=f\"{image.name}_relabeled\")\n    return new_image\n</code></pre>"},{"location":"chapters/python_api/tasks/io_tasks/","title":"Import and export tasks","text":""},{"location":"chapters/python_api/tasks/io_tasks/#import-task","title":"Import task","text":""},{"location":"chapters/python_api/tasks/io_tasks/#plantseg.tasks.io_tasks.import_image_task","title":"<code>plantseg.tasks.io_tasks.import_image_task(input_path: Path, semantic_type: str, stack_layout: str, image_name: str | None = None, key: str | None = None, m_slicing: str | None = None) -&gt; PlantSegImage</code>","text":"<p>Task wrapper creating a PlantSegImage object from an image file.</p> <p>Parameters:</p> <ul> <li> <code>input_path</code>               (<code>Path</code>)           \u2013            <p>path to the image file</p> </li> <li> <code>semantic_type</code>               (<code>str</code>)           \u2013            <p>semantic type of the image (raw, segmentation, prediction)</p> </li> <li> <code>stack_layout</code>               (<code>str</code>)           \u2013            <p>stack layout of the image (3D, 2D, 2D_time)</p> </li> <li> <code>image_name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>name of the image, if None the name will be the same as the file name</p> </li> <li> <code>key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>key for the image (used only for h5 and zarr formats)</p> </li> <li> <code>m_slicing</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>m_slicing of the image (None, time, z, y, x)</p> </li> </ul> Source code in <code>plantseg/tasks/io_tasks.py</code> <pre><code>@task_tracker(\n    is_root=True,\n    list_inputs={\n        \"input_path\": RunTimeInputSchema(\n            description=\"Path to a file, or a directory containing files (all files will be imported) or list of paths.\",\n            required=True,\n            is_input_file=True,\n        ),\n    },\n)\ndef import_image_task(\n    input_path: Path,\n    semantic_type: str,\n    stack_layout: str,\n    image_name: str | None = None,\n    key: str | None = None,\n    m_slicing: str | None = None,\n) -&gt; PlantSegImage:\n    \"\"\"\n    Task wrapper creating a PlantSegImage object from an image file.\n\n    Args:\n        input_path (Path): path to the image file\n        semantic_type (str): semantic type of the image (raw, segmentation, prediction)\n        stack_layout (str): stack layout of the image (3D, 2D, 2D_time)\n        image_name (str): name of the image, if None the name will be the same as the file name\n        key (str | None): key for the image (used only for h5 and zarr formats)\n        m_slicing (str | None): m_slicing of the image (None, time, z, y, x)\n    \"\"\"\n\n    if image_name is None:\n        image_name = input_path.stem\n\n    return import_image(\n        path=input_path,\n        key=key,\n        image_name=image_name,\n        semantic_type=semantic_type,\n        stack_layout=stack_layout,\n        m_slicing=m_slicing,\n    )\n</code></pre>"},{"location":"chapters/python_api/tasks/io_tasks/#export-task","title":"Export task","text":""},{"location":"chapters/python_api/tasks/io_tasks/#plantseg.tasks.io_tasks.export_image_task","title":"<code>plantseg.tasks.io_tasks.export_image_task(image: PlantSegImage, export_directory: Path, name_pattern: str = '{file_name}_export', key: str | None = None, scale_to_origin: bool = True, export_format: str = 'tiff', data_type: str = 'uint16') -&gt; None</code>","text":"<p>Task wrapper for saving an PlantSegImage object to disk.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image to be saved to disk</p> </li> <li> <code>export_directory</code>               (<code>Path</code>)           \u2013            <p>output directory path where the image will be saved</p> </li> <li> <code>name_pattern</code>               (<code>str</code>, default:                   <code>'{file_name}_export'</code> )           \u2013            <p>output file name pattern, can contain the {image_name} or {file_name} tokens to be replaced in the final file name.</p> </li> <li> <code>key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>key for the image (used only for h5 and zarr formats).</p> </li> <li> <code>scale_to_origin</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>scale the voxel size to the original one</p> </li> <li> <code>export_format</code>               (<code>str</code>, default:                   <code>'tiff'</code> )           \u2013            <p>file format (tiff, h5, zarr)</p> </li> <li> <code>data_type</code>               (<code>str</code>, default:                   <code>'uint16'</code> )           \u2013            <p>data type to save the image.</p> </li> </ul> Source code in <code>plantseg/tasks/io_tasks.py</code> <pre><code>@task_tracker(\n    is_leaf=True,\n    list_inputs={\n        \"export_directory\": RunTimeInputSchema(\n            description=\"Output directory path where the image will be saved\",\n            required=True,\n        ),\n        \"name_pattern\": RunTimeInputSchema(\n            description=\"Output file name pattern. Can contain the special {image_name} or {file_name} tokens \",\n            required=False,\n        ),\n    },\n)\ndef export_image_task(\n    image: PlantSegImage,\n    export_directory: Path,\n    name_pattern: str = \"{file_name}_export\",\n    key: str | None = None,\n    scale_to_origin: bool = True,\n    export_format: str = \"tiff\",\n    data_type: str = \"uint16\",\n) -&gt; None:\n    \"\"\"\n    Task wrapper for saving an PlantSegImage object to disk.\n\n    Args:\n        image (PlantSegImage): input image to be saved to disk\n        export_directory (Path): output directory path where the image will be saved\n        name_pattern (str): output file name pattern, can contain the {image_name} or {file_name} tokens\n            to be replaced in the final file name.\n        key (str | None): key for the image (used only for h5 and zarr formats).\n        scale_to_origin (bool): scale the voxel size to the original one\n        export_format (str): file format (tiff, h5, zarr)\n        data_type (str): data type to save the image.\n    \"\"\"\n    save_image(\n        image=image,\n        export_directory=export_directory,\n        name_pattern=name_pattern,\n        key=key,\n        scale_to_origin=scale_to_origin,\n        export_format=export_format,\n        data_type=data_type,\n    )\n    return None\n</code></pre>"},{"location":"chapters/python_api/tasks/prediction_tasks/","title":"Neural network prediction tasks","text":""},{"location":"chapters/python_api/tasks/prediction_tasks/#unet-prediction-task","title":"UNet prediction task","text":""},{"location":"chapters/python_api/tasks/prediction_tasks/#plantseg.tasks.prediction_tasks.unet_prediction_task","title":"<code>plantseg.tasks.prediction_tasks.unet_prediction_task(image: PlantSegImage, model_name: str | None, model_id: str | None, suffix: str = '_prediction', patch: tuple[int, int, int] | None = None, patch_halo: tuple[int, int, int] | None = None, single_batch_mode: bool = True, device: str = 'cuda', model_update: bool = False, disable_tqdm: bool = False, config_path: Path | None = None, model_weights_path: Path | None = None, _tracker: Optional[PBar_Tracker] = None) -&gt; list[PlantSegImage]</code>","text":"<p>Apply a trained U-Net model to a PlantSegImage object.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image object</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>the name of the model to use</p> </li> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>the ID of the model to use</p> </li> <li> <code>suffix</code>               (<code>str</code>, default:                   <code>'_prediction'</code> )           \u2013            <p>suffix to append to the new image name</p> </li> <li> <code>patch</code>               (<code>tuple[int, int, int]</code>, default:                   <code>None</code> )           \u2013            <p>patch size for prediction</p> </li> <li> <code>single_batch_mode</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>whether to use a single batch for prediction</p> </li> <li> <code>device</code>               (<code>str</code>, default:                   <code>'cuda'</code> )           \u2013            <p>the computation device ('cpu', 'cuda', etc.)</p> </li> <li> <code>model_update</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to update the model to the latest version</p> </li> </ul> Source code in <code>plantseg/tasks/prediction_tasks.py</code> <pre><code>@task_tracker\ndef unet_prediction_task(\n    image: PlantSegImage,\n    model_name: str | None,\n    model_id: str | None,\n    suffix: str = \"_prediction\",\n    patch: tuple[int, int, int] | None = None,\n    patch_halo: tuple[int, int, int] | None = None,\n    single_batch_mode: bool = True,\n    device: str = \"cuda\",\n    model_update: bool = False,\n    disable_tqdm: bool = False,\n    config_path: Path | None = None,\n    model_weights_path: Path | None = None,\n    _tracker: Optional[\"PBar_Tracker\"] = None,\n) -&gt; list[PlantSegImage]:\n    \"\"\"\n    Apply a trained U-Net model to a PlantSegImage object.\n\n    Args:\n        image (PlantSegImage): input image object\n        model_name (str): the name of the model to use\n        model_id (str): the ID of the model to use\n        suffix (str): suffix to append to the new image name\n        patch (tuple[int, int, int]): patch size for prediction\n        single_batch_mode (bool): whether to use a single batch for prediction\n        device (str): the computation device ('cpu', 'cuda', etc.)\n        model_update (bool): whether to update the model to the latest version\n    \"\"\"\n    data = image.get_data()\n    input_layout = image.image_layout\n\n    pmaps = unet_prediction(\n        raw=data,\n        input_layout=input_layout.value,\n        model_name=model_name,\n        model_id=model_id,\n        patch=patch,\n        patch_halo=patch_halo,\n        single_batch_mode=single_batch_mode,\n        device=device,\n        model_update=model_update,\n        disable_tqdm=disable_tqdm,\n        config_path=config_path,\n        model_weights_path=model_weights_path,\n        tracker=_tracker,\n    )\n    assert pmaps.ndim == 4, f\"Expected 4D CZXY prediction, got {pmaps.ndim}D\"\n\n    new_images = []\n\n    for i, pmap in enumerate(pmaps):\n        # Input layout is always ZYX this loop\n        pmap = fix_layout(\n            pmap, input_layout=ImageLayout.ZYX.value, output_layout=input_layout.value\n        )\n        new_images.append(\n            image.derive_new(\n                pmap,\n                name=f\"{image.name}_{suffix}_{i}\",\n                semantic_type=SemanticType.PREDICTION,\n                image_layout=input_layout,\n            )\n        )\n\n    return new_images\n</code></pre>"},{"location":"chapters/python_api/tasks/segmentation_tasks/","title":"Segmentation tasks","text":""},{"location":"chapters/python_api/tasks/segmentation_tasks/#distance-transform-watershed-task","title":"Distance transform watershed task","text":""},{"location":"chapters/python_api/tasks/segmentation_tasks/#plantseg.tasks.segmentation_tasks.dt_watershed_task","title":"<code>plantseg.tasks.segmentation_tasks.dt_watershed_task(image: PlantSegImage, threshold: float = 0.5, sigma_seeds: float = 1.0, stacked: bool = False, sigma_weights: float = 2.0, min_size: int = 100, alpha: float = 1.0, pixel_pitch: tuple[int, ...] | None = None, apply_nonmax_suppression: bool = False, n_threads: int | None = None, is_nuclei_image: bool = False) -&gt; PlantSegImage</code>","text":"<p>Distance transform watershed segmentation task.</p> <p>This function applies the distance transform watershed algorithm to segment the input image. It handles both standard boundary probability maps and nuclei images, with options for various preprocessing and segmentation parameters.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>The input image to segment.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold value for the boundary probability maps. Defaults to 0.5.</p> </li> <li> <code>sigma_seeds</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Standard deviation for Gaussian smoothing applied to the seed map. Defaults to 1.0.</p> </li> <li> <code>stacked</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True and the image is 3D, processes the image slice-by-slice (2D). Defaults to False.</p> </li> <li> <code>sigma_weights</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Standard deviation for Gaussian smoothing applied to the weight map. Defaults to 2.0.</p> </li> <li> <code>min_size</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Minimum size of the segments to keep. Smaller segments will be removed. Defaults to 100.</p> </li> <li> <code>alpha</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Blending factor between the input image and the distance transform when computing the weight map. Defaults to 1.0.</p> </li> <li> <code>pixel_pitch</code>               (<code>tuple[int, ...] | None</code>, default:                   <code>None</code> )           \u2013            <p>Anisotropy factors for the distance transform. If None, isotropic distances are assumed. Defaults to None.</p> </li> <li> <code>apply_nonmax_suppression</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to apply non-maximum suppression to the seeds. Requires the Nifty library. Defaults to False.</p> </li> <li> <code>n_threads</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of threads to use for parallel processing in 2D mode. Defaults to None.</p> </li> <li> <code>is_nuclei_image</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, indicates that the input image is a nuclei image, and preprocessing is applied accordingly. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PlantSegImage</code> (              <code>PlantSegImage</code> )          \u2013            <p>The segmented image as a new <code>PlantSegImage</code> object.</p> </li> </ul> Source code in <code>plantseg/tasks/segmentation_tasks.py</code> <pre><code>@task_tracker\ndef dt_watershed_task(\n    image: PlantSegImage,\n    threshold: float = 0.5,\n    sigma_seeds: float = 1.0,\n    stacked: bool = False,\n    sigma_weights: float = 2.0,\n    min_size: int = 100,\n    alpha: float = 1.0,\n    pixel_pitch: tuple[int, ...] | None = None,\n    apply_nonmax_suppression: bool = False,\n    n_threads: int | None = None,\n    is_nuclei_image: bool = False,\n) -&gt; PlantSegImage:\n    \"\"\"Distance transform watershed segmentation task.\n\n    This function applies the distance transform watershed algorithm to segment the input image.\n    It handles both standard boundary probability maps and nuclei images, with options for\n    various preprocessing and segmentation parameters.\n\n    Args:\n        image (PlantSegImage): The input image to segment.\n        threshold (float, optional): Threshold value for the boundary probability maps.\n            Defaults to 0.5.\n        sigma_seeds (float, optional): Standard deviation for Gaussian smoothing applied to\n            the seed map. Defaults to 1.0.\n        stacked (bool, optional): If True and the image is 3D, processes the image\n            slice-by-slice (2D). Defaults to False.\n        sigma_weights (float, optional): Standard deviation for Gaussian smoothing applied to\n            the weight map. Defaults to 2.0.\n        min_size (int, optional): Minimum size of the segments to keep. Smaller segments\n            will be removed. Defaults to 100.\n        alpha (float, optional): Blending factor between the input image and the distance\n            transform when computing the weight map. Defaults to 1.0.\n        pixel_pitch (tuple[int, ...] | None, optional): Anisotropy factors for the distance\n            transform. If None, isotropic distances are assumed. Defaults to None.\n        apply_nonmax_suppression (bool, optional): Whether to apply non-maximum suppression\n            to the seeds. Requires the Nifty library. Defaults to False.\n        n_threads (int | None, optional): Number of threads to use for parallel processing\n            in 2D mode. Defaults to None.\n        is_nuclei_image (bool, optional): If True, indicates that the input image is a nuclei\n            image, and preprocessing is applied accordingly. Defaults to False.\n\n    Returns:\n        PlantSegImage: The segmented image as a new `PlantSegImage` object.\n    \"\"\"\n    if image.is_multichannel:\n        raise ValueError(\"Multichannel images are not supported for this task.\")\n\n    if image.semantic_type != SemanticType.PREDICTION:\n        logger.warning(\n            \"The input image is not a boundary probability map. The task will still attempt to run, but the results may not be as expected.\"\n        )\n\n    if image.image_layout == ImageLayout.YX and stacked:\n        logger.warning(\n            \"Stack, or 'per slice' is only for 3D images (ZYX). The stack option will be disabled.\"\n        )\n        stacked = False\n\n    if is_nuclei_image:\n        boundary_pmaps = normalize_01(image.get_data())\n        boundary_pmaps = 1.0 - boundary_pmaps\n        mask = boundary_pmaps &lt; threshold\n    else:\n        boundary_pmaps = image.get_data()\n        mask = None\n\n    dt_seg = dt_watershed(\n        boundary_pmaps=boundary_pmaps,\n        threshold=threshold,\n        sigma_seeds=sigma_seeds,\n        stacked=stacked,\n        sigma_weights=sigma_weights,\n        min_size=min_size,\n        alpha=alpha,\n        pixel_pitch=pixel_pitch,\n        apply_nonmax_suppression=apply_nonmax_suppression,\n        n_threads=n_threads,\n        mask=mask,\n    )\n\n    dt_seg_image = image.derive_new(\n        dt_seg,\n        name=f\"{image.name}_dt_watershed\",\n        semantic_type=SemanticType.SEGMENTATION,\n    )\n    return dt_seg_image\n</code></pre>"},{"location":"chapters/python_api/tasks/segmentation_tasks/#cluster-segmentation-task","title":"Cluster segmentation task","text":""},{"location":"chapters/python_api/tasks/segmentation_tasks/#plantseg.tasks.segmentation_tasks.clustering_segmentation_task","title":"<code>plantseg.tasks.segmentation_tasks.clustering_segmentation_task(image: PlantSegImage, over_segmentation: PlantSegImage | None = None, mode='gasp', beta: float = 0.5, post_min_size: int = 100) -&gt; PlantSegImage</code>","text":"<p>Agglomerative segmentation task.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>PlantSegImage</code>)           \u2013            <p>input image object</p> </li> <li> <code>over_segmentation</code>               (<code>PlantSegImage</code>, default:                   <code>None</code> )           \u2013            <p>over-segmentation image object</p> </li> <li> <code>mode</code>               (<code>str</code>, default:                   <code>'gasp'</code> )           \u2013            <p>mode for the agglomerative segmentation</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter</p> </li> <li> <code>post_min_size</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>minimum size for the segments</p> </li> </ul> Source code in <code>plantseg/tasks/segmentation_tasks.py</code> <pre><code>@task_tracker\ndef clustering_segmentation_task(\n    image: PlantSegImage,\n    over_segmentation: PlantSegImage | None = None,\n    mode=\"gasp\",\n    beta: float = 0.5,\n    post_min_size: int = 100,\n) -&gt; PlantSegImage:\n    \"\"\"Agglomerative segmentation task.\n\n    Args:\n        image (PlantSegImage): input image object\n        over_segmentation (PlantSegImage): over-segmentation image object\n        mode (str): mode for the agglomerative segmentation\n        beta (float): beta parameter\n        post_min_size (int): minimum size for the segments\n    \"\"\"\n    if image.is_multichannel:\n        raise ValueError(\"Multichannel images are not supported for this task.\")\n\n    if image.semantic_type != SemanticType.PREDICTION:\n        logger.warning(\n            \"The input image is not a boundary probability map. The task will still attempt to run, but the results may not be as expected.\"\n        )\n\n    boundary_pmaps = image.get_data()\n\n    if over_segmentation is None:\n        superpixels = None\n    else:\n        if over_segmentation.semantic_type != SemanticType.SEGMENTATION:\n            raise ValueError(\"The input over_segmentation is not a segmentation map.\")\n        superpixels = over_segmentation.get_data()\n\n        if boundary_pmaps.shape != superpixels.shape:\n            raise ValueError(\n                \"The boundary probability map and the over-segmentation map should have the same shape.\"\n            )\n\n    if mode == \"gasp\":\n        seg = gasp(\n            boundary_pmaps,\n            superpixels=superpixels,\n            beta=beta,\n            post_minsize=post_min_size,\n        )\n    elif mode == \"multicut\":\n        if superpixels is None:\n            raise ValueError(\"The superpixels are required for the multicut mode.\")\n        seg = multicut(\n            boundary_pmaps,\n            superpixels=superpixels,\n            beta=beta,\n            post_minsize=post_min_size,\n        )\n    elif mode == \"mutex_ws\":\n        seg = mutex_ws(\n            boundary_pmaps,\n            superpixels=superpixels,\n            beta=beta,\n            post_minsize=post_min_size,\n        )\n    else:\n        raise ValueError(\n            f\"Unknown mode: {mode}, select one of ['gasp', 'multicut', 'mutex_ws']\"\n        )\n\n    seg_image = image.derive_new(\n        seg, name=f\"{image.name}_{mode}\", semantic_type=SemanticType.SEGMENTATION\n    )\n    return seg_image\n</code></pre>"},{"location":"chapters/python_api/tasks/segmentation_tasks/#lifted-multicut-task","title":"Lifted Multicut task","text":""},{"location":"chapters/python_api/tasks/segmentation_tasks/#plantseg.tasks.segmentation_tasks.lmc_segmentation_task","title":"<code>plantseg.tasks.segmentation_tasks.lmc_segmentation_task(boundary_pmap: PlantSegImage, superpixels: PlantSegImage, nuclei: PlantSegImage, beta: float = 0.5, post_min_size: int = 100) -&gt; PlantSegImage</code>","text":"<p>Lifted multicut segmentation task.</p> <p>Parameters:</p> <ul> <li> <code>boundary_pmap</code>               (<code>PlantSegImage</code>)           \u2013            <p>cell boundary prediction, PlantSegImage of shape (Z, Y, X) with values between 0 and 1.</p> </li> <li> <code>superpixels</code>               (<code>PlantSegImage</code>)           \u2013            <p>superpixels/over-segmentation. Must have the same shape as boundary_pmap.</p> </li> <li> <code>nuclei</code>               (<code>PlantSegImage</code>)           \u2013            <p>a nuclear segmentation or prediction map. Must have the same shape as boundary_pmap.</p> </li> <li> <code>beta</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>beta parameter for the Multicut. A small value will steer the segmentation towards under-segmentation, while a high-value bias the segmentation towards the over-segmentation. (default: 0.5)</p> </li> <li> <code>post_min_size</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>minimal size of the segments after Multicut. (default: 100)</p> </li> </ul> Source code in <code>plantseg/tasks/segmentation_tasks.py</code> <pre><code>@task_tracker\ndef lmc_segmentation_task(\n    boundary_pmap: PlantSegImage,\n    superpixels: PlantSegImage,\n    nuclei: PlantSegImage,\n    beta: float = 0.5,\n    post_min_size: int = 100,\n) -&gt; PlantSegImage:\n    \"\"\"Lifted multicut segmentation task.\n\n    Args:\n        boundary_pmap (PlantSegImage): cell boundary prediction, PlantSegImage of shape (Z, Y, X) with values between 0 and 1.\n        superpixels (PlantSegImage): superpixels/over-segmentation. Must have the same shape as boundary_pmap.\n        nuclei (PlantSegImage): a nuclear segmentation or prediction map. Must have the same shape as boundary_pmap.\n        beta (float): beta parameter for the Multicut.\n            A small value will steer the segmentation towards under-segmentation, while\n            a high-value bias the segmentation towards the over-segmentation. (default: 0.5)\n        post_min_size (int): minimal size of the segments after Multicut. (default: 100)\n    \"\"\"\n    if (\n        nuclei.semantic_type is SemanticType.PREDICTION\n        or nuclei.semantic_type is SemanticType.RAW\n    ):\n        lmc = lifted_multicut_from_nuclei_pmaps\n        extra_key = \"nuclei_pmaps\"\n    else:\n        lmc = lifted_multicut_from_nuclei_segmentation\n        extra_key = \"nuclei_seg\"\n\n    segmentation = lmc(\n        boundary_pmaps=boundary_pmap.get_data(),\n        superpixels=superpixels.get_data(),\n        **{extra_key: nuclei.get_data()},\n        beta=beta,\n        post_minsize=post_min_size,\n    )\n\n    ps_seg = superpixels.derive_new(\n        segmentation,\n        name=f\"{superpixels.name}_lmc\",\n        semantic_type=SemanticType.SEGMENTATION,\n    )\n    return ps_seg\n</code></pre>"},{"location":"snippets/napari/dataprocessing/rescale/","title":"Rescale","text":"From factorTo layer voxel sizeTo layer shapeTo model voxel sizeTo voxel sizeTo shapeSet voxel size <p>Using the <code>From factor</code> mode, the user can rescale the image by a multiplicate factor. For example, if the image has a shape <code>(10, 10, 10)</code> and the user wants to rescale it by a factor of <code>(2, 2, 2)</code>, the new size will be <code>(20, 20, 20)</code>.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Rescaling factor: Define the scaling factor to use for resizing the input image.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To layer voxel size</code> mode, the user can rescale the image to the voxel size of a specific layer. For example, if two images are loaded in the viewer, one with a voxel size of <code>(0.1, 0.1, 0.1)um</code> and the other with a voxel size of <code>(0.1, 0.05, 0.05)um</code>, the user can rescale the first image to the voxel size of the second image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Reference layer: Rescale to same voxel size as selected layer.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To layer shape</code> mode, the user can rescale the image to the shape of a specific layer. For example, if two images are loaded in the viewer, one with a shape <code>(10, 10, 10)</code> and the other with a shape <code>(20, 20, 20)</code>, the user can rescale the first image to the shape of the second image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Reference layer: Rescale to same voxel size as selected layer.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To model voxel size</code> mode, the user can rescale the image to the voxel size of the model. For example, if the model has been trained with data at voxel size of <code>(0.1, 0.1, 0.1)um</code>, the user can rescale the image to this voxel size.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Reference model: Rescale to same voxel size as selected model.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To voxel size</code> mode, the user can rescale the image to a specific voxel size.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Out voxel size: Define the output voxel size. Units are same as imported, (if units are missing default is \"um\").</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>To shape</code> mode, the user can rescale the image to a specific shape.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Out shape: Rescale to a manually selected shape.</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p> <p>Using the <code>Set voxel size</code> mode, the user can set the voxel size of the image to a specific value. This only changes the metadata of the image and does not rescale the image.</p> <p> <p></p> <p></p>                      Rescale an image or label layer.             <ul><li>Select layer: Layer to apply the rescaling.</li> <li>Rescale mode: None</li> <li>Out voxel size: Define the output voxel size. Units are same as imported, (if units are missing default is \"um\").</li> <li>Interpolation order: 0 for nearest neighbours (default for labels), 1 for linear, 2 for bilinear.</li> </ul> </p>"},{"location":"snippets/napari/main/widget_unet_prediction/","title":"Widget unet prediction","text":"PlantSeg ZooBioImage.IO Model Zoo <p> <p></p> <p></p>                      None             <ul><li>Mode: Select the mode to run the prediction.</li> <li>Image: Raw image to be processed with a neural network.</li> <li>PlantSeg model: Select a pretrained PlantSeg model. Current model description: Unet trained on confocal images of Arabidopsis Ovules on 1/2-resolution in XY with BCEDiceLoss.</li> </ul> </p> <p> <p></p> <p></p>                      None             <ul><li>Mode: Select the mode to run the prediction.</li> <li>Image: Raw image to be processed with a neural network.</li> <li>Model filter: Choose to only show models tagged with `plantseg`.</li> <li>BioImage.IO model: Select a model from BioImage.IO model zoo.</li> </ul> </p>"}]}